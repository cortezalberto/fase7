# ============================================================================
# AI-Native MVP - Environment Variables Template
# ============================================================================
# Copy this file to .env and fill in your values
# NEVER commit .env to git (it's in .gitignore)
#
# REQUIRED variables (docker-compose will fail without these):
#   - POSTGRES_PASSWORD
#   - REDIS_PASSWORD
#   - JWT_SECRET_KEY
#   - SECRET_KEY
#
# Generate secrets with: python -c 'import secrets; print(secrets.token_urlsafe(32))'
# ============================================================================

# ============================================================================
# DATABASE (REQUIRED)
# ============================================================================
POSTGRES_DB=ai_native
POSTGRES_USER=ai_native
POSTGRES_PASSWORD=CHANGE_THIS_GENERATE_SECURE_PASSWORD

# Full connection URL (auto-constructed in docker-compose, but needed for local dev)
DATABASE_URL=postgresql://ai_native:${POSTGRES_PASSWORD}@localhost:5432/ai_native
DB_POOL_SIZE=80
DB_MAX_OVERFLOW=80
DB_POOL_TIMEOUT=5
DB_POOL_RECYCLE=3600

# ============================================================================
# REDIS CACHE (REQUIRED)
# ============================================================================
REDIS_PASSWORD=CHANGE_THIS_GENERATE_SECURE_PASSWORD
REDIS_URL=redis://:${REDIS_PASSWORD}@localhost:6379/0
LLM_CACHE_ENABLED=true
LLM_CACHE_TTL=3600
LLM_CACHE_MAX_ENTRIES=1000
# FIX 4.8: Add CACHE_SALT for cache key generation
CACHE_SALT=CHANGE_THIS_GENERATE_WITH_PYTHON_SECRETS

# ============================================================================
# LLM PROVIDER - Choose ONE: gemini, ollama, or openai
# ============================================================================
# Recommended: gemini (Google Gemini - Fast, cost-effective, powerful)
LLM_PROVIDER=gemini

# ============================================================================
# GEMINI CONFIGURATION (Google Gemini API - RECOMMENDED)
# ============================================================================
# Get your API key from: https://makersuite.google.com/app/apikey
GEMINI_API_KEY=YOUR_GEMINI_API_KEY_HERE

# Default model (gemini-1.5-flash for conversations, automatically switches to Pro for code analysis)
# Options: gemini-1.5-flash, gemini-1.5-pro
# Note: The system automatically uses Pro model for code analysis tasks
GEMINI_MODEL=gemini-1.5-flash

# Generation temperature (0.0 = deterministic, 1.0 = creative)
GEMINI_TEMPERATURE=0.7

# Request timeout in seconds
GEMINI_TIMEOUT=60

# Maximum retry attempts for failed requests
GEMINI_MAX_RETRIES=3

# ============================================================================
# OLLAMA CONFIGURATION (Local, Free, Privacy-First - ALTERNATIVE)
# ============================================================================
# Uncomment to use Ollama instead of Gemini:
# LLM_PROVIDER=ollama

# Ollama server URL (default: localhost)
# For Docker: http://ollama:11434
# For local: http://localhost:11434
OLLAMA_BASE_URL=http://localhost:11434

# Model to use (install with: ollama pull <model>)
# Options: phi3, llama2, mistral, codellama, gemma:7b, etc.
# Recommended: phi3 (Microsoft Phi-3: 3.8B params, fast, efficient)
OLLAMA_MODEL=phi3

# Generation temperature (0.0 = deterministic, 1.0 = creative)
OLLAMA_TEMPERATURE=0.7

# FIX 4.8: Add OLLAMA_TIMEOUT (was missing)
# Request timeout in seconds
OLLAMA_TIMEOUT=120

# Keep model loaded to avoid cold-start between requests.
# IMPORTANT: Ollama expects a Go duration string (e.g., "10m", "1h", "24h", "0s").
# Using a bare number like "-1" can cause Ollama to fail with: time: missing unit in duration "-1".
OLLAMA_KEEP_ALIVE=24h

# Server-side keep-alive (applies inside the Ollama container). Keep this a valid duration.
OLLAMA_SERVER_KEEP_ALIVE=24h

# Optional performance tuning (leave blank to use Ollama defaults)
OLLAMA_NUM_CTX=
OLLAMA_NUM_THREAD=
OLLAMA_NUM_GPU=

# ============================================================================
# ALTERNATIVE LLM PROVIDERS (Optional - choose ONE provider)
# ============================================================================
# If using OpenAI instead of Gemini/Ollama:
# LLM_PROVIDER=openai
# OPENAI_API_KEY=sk-your-openai-api-key-here

# If using Google Gemini:
# LLM_PROVIDER=gemini
# GEMINI_API_KEY=your-gemini-api-key-here

# If using Anthropic Claude:
# LLM_PROVIDER=anthropic
# ANTHROPIC_API_KEY=sk-ant-your-anthropic-api-key-here

# ============================================================================
# SECURITY (REQUIRED)
# ============================================================================
# JWT Secret Key - REQUIRED, no default value
# Generate with: python -c 'import secrets; print(secrets.token_urlsafe(32))'
JWT_SECRET_KEY=CHANGE_THIS_GENERATE_WITH_PYTHON_SECRETS
SECRET_KEY=CHANGE_THIS_GENERATE_WITH_PYTHON_SECRETS

JWT_ALGORITHM=HS256
JWT_ACCESS_TOKEN_EXPIRE_MINUTES=30
JWT_REFRESH_TOKEN_EXPIRE_DAYS=7

# ============================================================================
# APPLICATION
# ============================================================================
ENVIRONMENT=development
DEBUG=false
LOG_LEVEL=INFO

# ============================================================================
# CORTEZ50: Training Integration Feature Flags
# ============================================================================
# Enable T-IA-Cog contextual hints in training mode
# When true: LLM generates pedagogically contextualized hints
# When false: Falls back to static hints from exercise definitions
TRAINING_USE_TUTOR_HINTS=false

# Enable N4 cognitive traceability in training mode
# When true: Records cognitive traces for code attempts, hints, reflections
# When false: Training mode operates without traceability (lighter, faster)
TRAINING_N4_TRACING=false

# Enable real-time risk monitoring during training
# When true: Detects copy-paste, frustration, hint dependency patterns
# When false: No risk analysis during training
TRAINING_RISK_MONITOR=false

# ============================================================================
# CORS (Frontend origins)
# ============================================================================
# Include all frontend development ports (3000, 3001, 5173, 8080)
ALLOWED_ORIGINS=http://localhost:3000,http://localhost:3001,http://localhost:5173,http://localhost:8080

# ============================================================================
# MONITORING PROFILE (docker-compose --profile monitoring)
# FIX 1.2: Required when using monitoring profile
# ============================================================================
GRAFANA_USER=admin
GRAFANA_PASSWORD=CHANGE_THIS_GENERATE_SECURE_PASSWORD

# Prometheus doesn't require auth by default, but consider nginx proxy in production

# ============================================================================
# DEBUG PROFILE (docker-compose --profile debug)
# FIX 1.2, 1.3: Required when using debug profile
# ============================================================================
# pgAdmin credentials
PGADMIN_EMAIL=admin@ai-native.local
PGADMIN_PASSWORD=CHANGE_THIS_GENERATE_SECURE_PASSWORD

# Redis Commander credentials (FIX 1.3: HTTP basic auth)
REDIS_COMMANDER_USER=admin
REDIS_COMMANDER_PASSWORD=CHANGE_THIS_GENERATE_SECURE_PASSWORD

# ============================================================================
# QUICK START
# ============================================================================
# 1. Install Ollama: https://ollama.ai
# 2. Download a model: ollama pull llama2
# 3. Start Ollama: ollama serve
# 4. Copy this file: cp .env.example .env
# 5. Run Phoenix: python scripts/run_api.py
# 6. Access API docs: http://localhost:8000/docs
#
# For Docker deployment:
#   docker compose up -d
#
# For Docker + NVIDIA GPU:
#   docker compose -f docker-compose.yml -f docker-compose.gpu.yml up -d
#
# Documentation:
#   - Quick Start: OLLAMA_QUICKSTART.md
#   - Full Guide: OLLAMA_INTEGRATION_GUIDE.md
