"""
Submodelo 1: Tutor IA Disciplinar Cognitivo (T-IA-Cog)

Agente de andamiaje cognitivo y metacognitivo que amplifica capacidades
del estudiante sin sustituirlas, operando bajo reglas pedag√≥gicas y √©ticas expl√≠citas.

VERSI√ìN 2.0 - TUTOR SOCR√ÅTICO PERSONALIZADO
Incorpora:
1. Reglas inquebrantables (Anti-Soluci√≥n, Socr√°tico, Explicitaci√≥n, Refuerzo Conceptual)
2. Sistema de gobernanza con sem√°foros (Verde/Amarillo/Rojo)
3. Procesamiento IPC -> GSR -> Andamiaje
4. Metadata completa para an√°lisis N4
"""
from typing import Optional, Dict, Any, List
from enum import Enum
from datetime import datetime
import uuid
import logging

logger = logging.getLogger(__name__)

from ...models.trace import CognitiveTrace, TraceLevel, InteractionType
from .rules import (
    TutorRulesEngine,
    TutorRule,
    InterventionType as TutorInterventionType,
    CognitiveScaffoldingLevel
)
from .governance import (
    TutorGovernanceEngine,
    SemaforoState,
    PromptIntent
)
from .metadata import (
    TutorMetadataTracker,
    TutorInterventionMetadata,
    StudentCognitiveEvent
)
from .prompts import TutorSystemPrompts


class TutorMode(str, Enum):
    """Modos de tutor√≠a"""
    SOCRATICO = "socratico"  # Preguntas socr√°ticas
    EXPLICATIVO = "explicativo"  # Explicaciones conceptuales
    GUIADO = "guiado"  # Pistas graduadas
    METACOGNITIVO = "metacognitivo"  # Reflexi√≥n sobre el proceso


class HelpLevel(str, Enum):
    """Niveles de ayuda"""
    MINIMO = "minimo"  # Solo preguntas orientadoras
    BAJO = "bajo"  # Pistas muy generales
    MEDIO = "medio"  # Pistas con algo de detalle
    ALTO = "alto"  # Explicaciones detalladas (sin c√≥digo completo)


class TutorCognitivoAgent:
    """
    T-IA-Cog: Tutor IA Disciplinar Cognitivo (VERSI√ìN 2.0 - Socr√°tico)

    Funciones principales:
    1. Guiar el razonamiento (NO proveer soluciones - REGLA INQUEBRANTABLE)
    2. Promover la explicitaci√≥n del pensamiento (forzar justificaci√≥n)
    3. Prevenir delegaci√≥n acr√≠tica (sistema de sem√°foros)
    4. Reforzar fundamentos conceptuales (no parches sint√°cticos)
    5. Escalar dificultad cognitiva adaptativamente (por nivel)

    NUEVOS COMPONENTES V2.0:
    - TutorRulesEngine: Aplica las 4 reglas inquebrantables
    - TutorGovernanceEngine: Procesa IPC -> GSR -> Andamiaje
    - TutorMetadataTracker: Registra metadata para an√°lisis N4
    - TutorSystemPrompts: System prompts personalizados por contexto

    Basado en:
    - Cognici√≥n distribuida (Hutchins, 1995)
    - Cognici√≥n extendida (Clark & Chalmers, 1998)
    - Teor√≠a de carga cognitiva (Sweller, 1988)
    - Autorregulaci√≥n (Zimmerman, 2002)
    """

    def __init__(self, llm_provider=None, config: Optional[Dict[str, Any]] = None):
        self.llm_provider = llm_provider
        self.config = config or {}
        
        # Log LLM provider status
        if llm_provider:
            # FIX Cortez36: Use lazy logging formatting
            logger.info("TutorCognitivoAgent initialized with LLM provider: %s", type(llm_provider).__name__)
        else:
            logger.warning("TutorCognitivoAgent initialized WITHOUT LLM provider - will use templates")
        
        # Componentes V2.0
        self.rules_engine = TutorRulesEngine(config)
        self.governance_engine = TutorGovernanceEngine(self.rules_engine)
        self.metadata_tracker = TutorMetadataTracker()
        self.prompts = TutorSystemPrompts()
        
        # Pol√≠ticas pedag√≥gicas (legacy - mantenidas por compatibilidad)
        self.policies = {
            "prioritize_questions": True,
            "require_justification": True,
            "adaptive_difficulty": True,
            "max_help_level": HelpLevel.MEDIO,
            "block_complete_solutions": True,
        }

        # Actualizar con config
        if config:
            self.policies.update(config.get("policies", {}))
    
    async def process_student_request(
        self,
        session_id: str,
        student_prompt: str,
        student_profile: Dict[str, Any],
        conversation_history: Optional[List[Dict[str, Any]]] = None
    ) -> Dict[str, Any]:
        """
        M√âTODO PRINCIPAL V2.0: Procesa request del estudiante con pipeline completo
        
        Pipeline:
        1. IPC (Ingesta y Comprensi√≥n de Prompt)
        2. GSR (Gobernanza y Sem√°foro de Riesgo)
        3. Selecci√≥n de Estrategia de Andamiaje
        4. Chequeo de Reglas Pedag√≥gicas
        5. Generaci√≥n de Respuesta
        6. Registro de Metadata para N4
        
        Args:
            session_id: ID de la sesi√≥n
            student_prompt: Pregunta/solicitud del estudiante
            student_profile: Perfil del estudiante con m√©tricas
            conversation_history: Historial de la conversaci√≥n
        
        Returns:
            Dict con:
            - message: Respuesta del tutor
            - intervention_type: Tipo de intervenci√≥n
            - metadata: Metadata completa para N4
            - semaforo: Estado del sem√°foro
        """
        try:
            # ============================================================
            # VALIDACI√ìN DE ENTRADA ROBUSTA
            # ============================================================
            if not session_id or not isinstance(session_id, str):
                # FIX Cortez36: Use lazy logging formatting
                logger.error("Invalid session_id: %s", session_id)
                raise ValueError("session_id debe ser un string no vac√≠o")
            
            if not student_prompt or not isinstance(student_prompt, str):
                # FIX Cortez36: Use lazy logging formatting
                logger.error("Invalid student_prompt: %s", student_prompt)
                raise ValueError("student_prompt debe ser un string no vac√≠o")
            
            if student_prompt.strip() == "":
                # FIX Cortez36: Use lazy logging formatting
                logger.warning("Empty student_prompt for session %s", session_id)
                return self._generate_error_response(
                    "Por favor, ingres√° una pregunta o consulta para que pueda ayudarte.",
                    session_id,
                    "empty_prompt"
                )
            
            if not isinstance(student_profile, dict):
                # FIX Cortez36: Use lazy logging formatting
                logger.error("Invalid student_profile type: %s", type(student_profile))
                student_profile = {}  # Usar perfil vac√≠o como fallback
            
            conversation_history = conversation_history or []
            if not isinstance(conversation_history, list):
                # FIX Cortez36: Use lazy logging formatting
                logger.error("Invalid conversation_history type: %s", type(conversation_history))
                conversation_history = []

            # FIX Cortez68 (HIGH-002): Prompt injection detection
            if self._detect_prompt_injection(student_prompt):
                logger.warning(
                    "Prompt injection attempt detected - Session: %s",
                    session_id
                )
                return self._generate_error_response(
                    "Tu mensaje contiene patrones que no puedo procesar. "
                    "Por favor, reformula tu pregunta enfoc√°ndote en el tema que est√°s estudiando.",
                    session_id,
                    "prompt_injection_detected"
                )

            interaction_id = str(uuid.uuid4())
            # FIX Cortez36: Use lazy logging formatting
            logger.info("Processing student request - Session: %s, Interaction: %s", session_id, interaction_id)
            
            # ============================================================
            # FASE 1-3: Procesamiento por Gobernanza (IPC -> GSR -> Andamiaje)
            # ============================================================
            try:
                governance_result = self.governance_engine.process_student_request(
                    student_prompt=student_prompt,
                    student_profile=student_profile,
                    conversation_history=conversation_history
                )
            except Exception as e:
                # FIX Cortez36: Use lazy logging formatting
                # FIX Cortez36: Added exc_info for stack trace
                logger.error("Error en gobernanza para session %s: %s: %s", session_id, type(e).__name__, e, exc_info=True)
                return self._generate_error_response(
                    "Tuve un problema procesando tu solicitud. Por favor, intent√° reformular tu pregunta.",
                    session_id,
                    "governance_error",
                    {"error_type": type(e).__name__, "error_message": str(e)}
                )
            
            # ============================================================
            # VALIDAR RESULTADO DE GOBERNANZA
            # ============================================================
            try:
                analysis = governance_result["analysis"]
                semaforo = governance_result["semaforo"]
                strategy = governance_result["strategy"]
                risk_details = governance_result["risk_details"]
            except KeyError as e:
                # FIX Cortez36: Use lazy logging formatting
                # FIX Cortez36: Added exc_info for stack trace
                logger.error("Missing key in governance_result: %s", e, exc_info=True)
                return self._generate_error_response(
                    "Hubo un problema al analizar tu solicitud. Por favor, intent√° nuevamente.",
                    session_id,
                    "governance_result_invalid",
                    {"missing_key": str(e)}
                )
            except Exception as e:
                # FIX Cortez36: Use lazy logging formatting
                # FIX Cortez36: Added exc_info for stack trace
                logger.error("Error extracting governance_result fields: %s: %s", type(e).__name__, e, exc_info=True)
                return self._generate_error_response(
                    "Ocurri√≥ un error inesperado. Por favor, intent√° reformular tu pregunta.",
                    session_id,
                    "governance_extraction_error",
                    {"error_type": type(e).__name__, "error_message": str(e)}
                )
            
            # ============================================================
            # FASE 4: Chequeo de Reglas Pedag√≥gicas
            # ============================================================
            try:
                rules_violations = self._check_pedagogical_rules(
                    student_prompt=student_prompt,
                    student_level=analysis.student_level,
                    conversation_history=conversation_history,
                    strategy=strategy
                )
            except Exception as e:
                # FIX Cortez36: Use lazy logging formatting
                # FIX Cortez36: Added exc_info for stack trace
                logger.error("Error checking pedagogical rules: %s: %s", type(e).__name__, e, exc_info=True)
                # Continuar sin violaciones de reglas
                rules_violations = {"critical_violation": False, "rules_applied": []}
            
            # ============================================================
            # APLICAR REGLAS O GENERAR RESPUESTA
            # ============================================================
            try:
                # Si hay violaciones cr√≠ticas (ej: solicitud de c√≥digo), aplicar regla
                if rules_violations.get("critical_violation"):
                    # Aplicar intervenci√≥n de rechazo pedag√≥gico
                    intervention_type = TutorInterventionType.RECHAZO_PEDAGOGICO
                    message = rules_violations.get("rejection_message", "No puedo proporcionar una soluci√≥n directa.")
                    counter_question = rules_violations.get("counter_question", "¬øPod√©s explicarme qu√© intentaste hasta ahora?")
                    
                    response = {
                        "message": f"{message}\n\n{counter_question}",
                        "intervention_type": intervention_type.value,
                        "semaforo": semaforo.value if hasattr(semaforo, 'value') else str(semaforo),
                        "requires_student_response": True,
                        "metadata": self._build_metadata(
                            interaction_id,
                            session_id,
                            intervention_type,
                            analysis,
                            semaforo,
                            strategy,
                            rules_violations.get("rules_applied", [])
                        )
                    }
                else:
                    # FASE 5: Generaci√≥n de Respuesta Normal
                    response = await self._generate_tutor_response(
                        interaction_id=interaction_id,
                        session_id=session_id,
                        student_prompt=student_prompt,
                        strategy=strategy,
                        analysis=analysis,
                        semaforo=semaforo,
                        risk_details=risk_details
                    )
            except Exception as e:
                # FIX Cortez36: Use lazy logging formatting
                # FIX Cortez36: Added exc_info for stack trace
                logger.error("Error generating tutor response: %s: %s", type(e).__name__, e, exc_info=True)
                return self._generate_error_response(
                    "Tuve dificultades para generar una respuesta. Por favor, intent√° reformular tu pregunta de otra manera.",
                    session_id,
                    "response_generation_error",
                    {"error_type": type(e).__name__, "error_message": str(e)}
                )
            
            # ============================================================
            # FASE 6: Registrar Metadata para N4
            # ============================================================
            try:
                self.metadata_tracker.record_intervention(
                    session_id=session_id,
                    interaction_id=interaction_id,
                    intervention_type=response.get("intervention_type"),
                    student_level=analysis.student_level,
                    help_level=strategy.get("help_level", "medio"),
                    semaforo_state=semaforo,
                    cognitive_state=analysis.cognitive_state,
                    student_intent=analysis.intent.value if hasattr(analysis.intent, 'value') else str(analysis.intent),
                    autonomy_level=analysis.autonomy_level,
                    rules_applied=rules_violations.get("rules_applied", []),
                    restrictions_applied=risk_details.get("restrictions", []),
                    additional_metadata=response.get("metadata", {})
                )
            except Exception as e:
                # FIX Cortez36: Use lazy logging formatting
                logger.warning("Error recording metadata (non-critical): %s: %s", type(e).__name__, e)
                # No bloqueamos la respuesta por errores de metadata
            
            # FIX Cortez36: Use lazy logging formatting
            logger.info("Successfully processed student request - Session: %s, Interaction: %s", session_id, interaction_id)
            return response
            
        except ValueError as ve:
            # Errores de validaci√≥n ya fueron manejados arriba
            raise ve
        except Exception as e:
            # FIX Cortez36: Use lazy logging formatting
            # FIX Cortez36: Added exc_info for stack trace
            logger.error("Unexpected critical error in process_student_request: %s: %s", type(e).__name__, e, exc_info=True)
            return self._generate_error_response(
                "Ocurri√≥ un error inesperado. Por favor, intent√° nuevamente m√°s tarde o contact√° al soporte t√©cnico.",
                session_id if 'session_id' in locals() else "unknown",
                "critical_error",
                {"error_type": type(e).__name__, "error_message": str(e)}
            )
    
    def _check_pedagogical_rules(
        self,
        student_prompt: str,
        student_level: CognitiveScaffoldingLevel,
        conversation_history: List[Dict[str, Any]],
        strategy: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        Chequea las reglas pedag√≥gicas inquebrantables
        
        Returns:
            Dict con:
            - critical_violation: bool
            - rules_applied: List[str]
            - rejection_message: str (si aplica)
            - counter_question: str (si aplica)
        """
        violations = {
            "critical_violation": False,
            "rules_applied": [],
        }
        
        # Regla #1: Anti-Soluci√≥n Directa
        anti_solution_result = self.rules_engine.check_anti_solution_rule(
            student_request=student_prompt,
            student_level=student_level
        )
        
        if anti_solution_result.get("violated"):
            violations["critical_violation"] = True
            violations["rules_applied"].append(TutorRule.ANTI_SOLUCION.value)
            violations["rejection_message"] = anti_solution_result.get("message")
            violations["counter_question"] = anti_solution_result.get("counter_question")
            return violations  # Retornar inmediatamente
        
        # Regla #2: Modo Socr√°tico Prioritario
        socratic_result = self.rules_engine.check_socratic_priority_rule(
            conversation_context=conversation_history
        )
        
        if socratic_result.get("should_question_first"):
            violations["rules_applied"].append(TutorRule.MODO_SOCRATICO.value)
            strategy["force_socratic"] = True
        
        # Regla #3: Exigencia de Explicitaci√≥n
        explicitacion_result = self.rules_engine.check_explicitacion_rule(
            student_message=student_prompt,
            conversation_context=conversation_history
        )
        
        if explicitacion_result.get("needs_explicitacion"):
            violations["rules_applied"].append(TutorRule.EXIGIR_EXPLICITACION.value)
            strategy["require_justification"] = True
            strategy["explicitacion_message"] = explicitacion_result.get("message")
        
        return violations
    
    async def _generate_tutor_response(
        self,
        interaction_id: str,
        session_id: str,
        student_prompt: str,
        strategy: Dict[str, Any],
        analysis: Any,  # StudentContextAnalysis
        semaforo: SemaforoState,
        risk_details: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        Genera respuesta del tutor seg√∫n estrategia y contexto
        
        Usa system prompts personalizados para garantizar cumplimiento de reglas.
        """
        intervention_type = strategy.get("intervention_type", TutorInterventionType.PREGUNTA_SOCRATICA)
        
        # Generar system prompt personalizado
        system_prompt = self.prompts.get_intervention_prompt(
            intervention_type=intervention_type,
            student_level=analysis.student_level,
            semaforo_state=semaforo,
            context={
                "risk_type": risk_details.get("risk_type"),
                "restrictions": risk_details.get("restrictions", []),
                "student_intent": analysis.intent.value,
                "cognitive_state": analysis.cognitive_state
            }
        )
        
        # Si hay mensaje de advertencia (sem√°foro rojo), incluirlo
        warning_message = risk_details.get("warning_message")
        
        # ‚úÖ NUEVO: Si hay LLM provider, usar respuesta din√°mica con memoria de conversaci√≥n
        if self.llm_provider and hasattr(self.llm_provider, 'generate'):
            try:
                # Recuperar historial de conversaci√≥n (ya cargado en guide())
                conversation_history = strategy.get("conversation_history", [])
                
                # Agregar el prompt actual al historial
                messages = conversation_history + [
                    {"role": "user", "content": student_prompt}
                ]
                
                # Generar respuesta con contexto completo
                llm_response = await self.llm_provider.generate(
                    messages=messages,
                    system_prompt=system_prompt,
                    temperature=0.7,
                    max_tokens=500
                )
                
                # Extraer el contenido de la respuesta del LLM
                response_content = llm_response.content if hasattr(llm_response, 'content') else str(llm_response)
                
                legacy_response = {
                    "message": response_content,
                    "requires_student_response": True
                }
                
            except Exception as e:
                # FIX Cortez36: Use lazy logging formatting
                logger.warning("LLM generation failed, using fallback: %s", e)
                # Fallback a m√©todo legacy
                response_type = strategy.get("response_type", "socratic_questioning")
                if response_type == "socratic_questioning":
                    legacy_response = await self._generate_socratic_response(
                        student_prompt, analysis.cognitive_state, strategy
                    )
                elif response_type == "conceptual_explanation":
                    legacy_response = await self._generate_conceptual_explanation(
                        student_prompt, analysis.cognitive_state, strategy
                    )
                elif response_type == "guided_hints":
                    legacy_response = await self._generate_guided_hints(
                        student_prompt, analysis.cognitive_state, strategy, None
                    )
                else:
                    legacy_response = self._generate_clarification_request(
                        student_prompt, analysis.cognitive_state
                    )
        else:
            # Sin LLM provider, usar m√©todo legacy
            response_type = strategy.get("response_type", "socratic_questioning")

            if response_type == "socratic_questioning":
                legacy_response = await self._generate_socratic_response(
                    student_prompt, analysis.cognitive_state, strategy
                )
            elif response_type == "conceptual_explanation":
                legacy_response = await self._generate_conceptual_explanation(
                    student_prompt, analysis.cognitive_state, strategy
                )
            elif response_type == "guided_hints":
                legacy_response = await self._generate_guided_hints(
                    student_prompt, analysis.cognitive_state, strategy, None
                )
            else:
                # Manejar tipos de respuesta desconocidos
                # FIX Cortez36: Use lazy logging formatting
                logger.warning("Unknown response_type '%s', using conceptual_explanation as fallback", response_type)
                legacy_response = await self._generate_conceptual_explanation(
                    student_prompt, analysis.cognitive_state, strategy
                )
        
        # Combinar warning si existe
        final_message = legacy_response["message"]
        if warning_message:
            final_message = f"{warning_message}\n\n---\n\n{final_message}"
        
        response_dict = {
            "message": final_message,
            "intervention_type": intervention_type.value if hasattr(intervention_type, 'value') else intervention_type,
            "semaforo": semaforo.value,
            "help_level": strategy.get("help_level"),
            "requires_student_response": legacy_response.get("requires_student_response", True),
            "system_prompt_used": system_prompt,  # Para logging/debugging
            "metadata": self._build_metadata(
                interaction_id,
                session_id,
                intervention_type,
                analysis,
                semaforo,
                strategy,
                []
            )
        }
        
        # Guardar la respuesta del tutor en metadata para historial
        response_dict["metadata"]["tutor_response"] = final_message
        
        return response_dict
    
    def _generate_error_response(
        self,
        error_message: str,
        session_id: str,
        error_type: str,
        details: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        Genera una respuesta de error consistente y amigable para el usuario
        
        Args:
            error_message: Mensaje de error amigable para el estudiante
            session_id: ID de la sesi√≥n
            error_type: Tipo de error para logging
            details: Detalles adicionales del error
        
        Returns:
            Dict con respuesta de error estructurada
        """
        # FIX Cortez36: Use lazy logging formatting
        logger.warning("Generating error response - Session: %s, Type: %s", session_id, error_type)
        
        return {
            "message": error_message,
            "intervention_type": "error_recovery",
            "semaforo": SemaforoState.AMARILLO.value,
            "requires_student_response": True,
            "metadata": {
                "error_type": error_type,
                "error_details": details or {},
                "timestamp": datetime.now().isoformat(),
                "session_id": session_id
            }
        }
    
    def _build_metadata(
        self,
        interaction_id: str,
        session_id: str,
        intervention_type: Any,
        analysis: Any,
        semaforo: SemaforoState,
        strategy: Dict[str, Any],
        rules_applied: List[str]
    ) -> Dict[str, Any]:
        """Construye metadata completa para N4"""
        return {
            "interaction_id": interaction_id,
            "session_id": session_id,
            "intervention_type": intervention_type.value if hasattr(intervention_type, 'value') else intervention_type,
            "student_level": analysis.student_level.value if hasattr(analysis, 'student_level') else "intermedio",
            "student_intent": analysis.intent.value if hasattr(analysis, 'intent') else "exploracion",
            "cognitive_state": analysis.cognitive_state if hasattr(analysis, 'cognitive_state') else "exploracion",
            "autonomy_level": analysis.autonomy_level if hasattr(analysis, 'autonomy_level') else 0.5,
            "semaforo": semaforo.value,
            "help_level": strategy.get("help_level", "medio"),
            "rules_applied": rules_applied,
            "restrictions": strategy.get("restrictions", []),
            "allow_code": strategy.get("allow_code", False),
            "allow_pseudocode": strategy.get("allow_pseudocode", True),
            "require_justification": strategy.get("require_justification", True)
        }
    
    def evaluate_student_response_v2(
        self,
        session_id: str,
        interaction_id: str,
        student_response: str,
        time_to_response_minutes: float
    ) -> Dict[str, Any]:
        """
        NUEVO V2.0: Eval√∫a respuesta del estudiante y actualiza metadata
        
        Args:
            session_id: ID de la sesi√≥n
            interaction_id: ID de la intervenci√≥n a evaluar
            student_response: Respuesta del estudiante
            time_to_response_minutes: Tiempo en minutos
        
        Returns:
            Dict con an√°lisis de la respuesta
        """
        # Buscar la intervenci√≥n en el historial
        intervention = None
        for i in self.metadata_tracker.intervention_history:
            if i.interaction_id == interaction_id:
                intervention = i
                break
        
        if not intervention:
            return {"error": "Intervention not found"}
        
        # Detectar eventos cognitivos
        cognitive_events = self.metadata_tracker.detect_cognitive_events(
            student_response=student_response,
            previous_intervention=intervention
        )
        
        # Evaluar efectividad
        effectiveness = self.metadata_tracker.evaluate_intervention_effectiveness(
            intervention=intervention,
            student_response=student_response,
            cognitive_events=cognitive_events,
            time_to_response_minutes=time_to_response_minutes
        )
        
        # Actualizar metadata
        self.metadata_tracker.update_intervention_effectiveness(
            interaction_id=interaction_id,
            effectiveness=effectiveness,
            cognitive_events=cognitive_events
        )
        
        return {
            "cognitive_events": [e.value for e in cognitive_events],
            "effectiveness": effectiveness.value,
            "analysis": self.evaluate_student_response(student_response, {}),  # Legacy
            "should_adjust_strategy": self._should_adjust_strategy(
                effectiveness, cognitive_events
            )
        }
    
    def _should_adjust_strategy(
        self,
        effectiveness: Any,
        cognitive_events: List[StudentCognitiveEvent]
    ) -> Dict[str, Any]:
        """Determina si se debe ajustar la estrategia pedag√≥gica"""
        from .tutor_metadata import InterventionEffectiveness
        
        # Si la intervenci√≥n fue muy efectiva, mantener nivel
        if effectiveness == InterventionEffectiveness.MUY_EFECTIVA:
            return {"adjust": False, "reason": "highly_effective"}
        
        # Si fue contraproducente, reducir complejidad
        if effectiveness == InterventionEffectiveness.CONTRAPRODUCENTE:
            return {
                "adjust": True,
                "direction": "simplify",
                "reason": "counterproductive_intervention"
            }
        
        # Si no hay eventos cognitivos positivos, incrementar gu√≠a
        positive_events = [
            StudentCognitiveEvent.AUTOCORRECCION,
            StudentCognitiveEvent.REFLEXION_METACOGNITIVA,
            StudentCognitiveEvent.JUSTIFICACION_DECISION
        ]
        
        if not any(event in cognitive_events for event in positive_events):
            return {
                "adjust": True,
                "direction": "increase_guidance",
                "reason": "lack_of_cognitive_events"
            }
        
        return {"adjust": False, "reason": "adequate_effectiveness"}
    
    def get_session_analytics_n4(self, session_id: str) -> Dict[str, Any]:
        """
        NUEVO V2.0: Obtiene analytics N4 de una sesi√≥n

        Args:
            session_id: ID de la sesi√≥n

        Returns:
            M√©tricas agregadas para an√°lisis N4
        """
        return self.metadata_tracker.generate_n4_analytics(session_id)

    def _detect_prompt_injection(self, prompt: str) -> bool:
        """
        FIX Cortez68 (HIGH-002): Detect common prompt injection patterns

        Checks for patterns that attempt to:
        - Override system instructions
        - Change assistant persona
        - Bypass content filters
        - Access system prompts

        Args:
            prompt: Student's input text

        Returns:
            True if injection pattern detected, False otherwise
        """
        # Normalize for case-insensitive matching
        lower_prompt = prompt.lower()

        # Common injection patterns
        injection_patterns = [
            # System instruction override attempts
            "ignore previous instructions",
            "ignore all previous",
            "disregard previous",
            "forget your instructions",
            "override your programming",
            "bypass your restrictions",
            "ignore your training",
            "ignore your rules",
            # Persona manipulation
            "you are now",
            "pretend you are",
            "act as if you",
            "roleplay as",
            "from now on you",
            "you are no longer",
            # Prompt leaking
            "repeat your instructions",
            "show me your prompt",
            "reveal your system prompt",
            "what are your instructions",
            "show your configuration",
            # Jailbreak attempts
            "dan mode",
            "developer mode",
            "jailbreak",
            "do anything now",
        ]

        for pattern in injection_patterns:
            if pattern in lower_prompt:
                return True

        return False

        # Pol√≠ticas pedag√≥gicas
        self.policies = {
            "prioritize_questions": True,
            "require_justification": True,
            "adaptive_difficulty": True,
            "max_help_level": HelpLevel.MEDIO,
            "block_complete_solutions": True,
        }

        # Actualizar con config
        if config:
            self.policies.update(config.get("policies", {}))

    def generate_response(
        self,
        student_prompt: str,
        cognitive_state: str,
        strategy: Dict[str, Any],
        student_history: Optional[List[CognitiveTrace]] = None
    ) -> Dict[str, Any]:
        """
        Genera respuesta tutorial basada en principios pedag√≥gicos

        Args:
            student_prompt: Pregunta/solicitud del estudiante
            cognitive_state: Estado cognitivo actual
            strategy: Estrategia pedag√≥gica definida por CRPE
            student_history: Historial de interacciones

        Returns:
            Diccionario con respuesta y metadata pedag√≥gica
        """
        response_type = strategy.get("response_type", "socratic_questioning")

        if response_type == "socratic_questioning":
            return self._generate_socratic_response(
                student_prompt, cognitive_state, strategy
            )
        elif response_type == "conceptual_explanation":
            return self._generate_conceptual_explanation(
                student_prompt, cognitive_state, strategy
            )
        elif response_type == "guided_hints":
            return self._generate_guided_hints(
                student_prompt, cognitive_state, strategy, student_history
            )
        else:
            return self._generate_clarification_request(
                student_prompt, cognitive_state
            )

    async def _generate_socratic_response(
        self,
        prompt: str,
        cognitive_state: str,
        strategy: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        Genera respuesta socr√°tica con preguntas que gu√≠an el razonamiento

        Usa LLM si est√° disponible, sino usa plantillas predefinidas.
        """
        # Si hay LLM provider, generar respuesta con IA
        if self.llm_provider:
            try:
                # FIX Cortez36: Use lazy logging formatting
                logger.info("Attempting to generate Socratic response with LLM for prompt: %s...", prompt[:50])
                result = await self._generate_socratic_with_llm(prompt, cognitive_state, strategy)
                # FIX Cortez36: Use lazy logging formatting
                logger.info("Successfully generated Socratic response with LLM")
                return result
            except AttributeError as e:
                # FIX Cortez36: Use lazy logging formatting
                # FIX Cortez36: Added exc_info for stack trace
                logger.error("LLM attribute error (method not found): %s: %s", type(e).__name__, str(e), exc_info=True)
                # Continue to fallback
            except ValueError as e:
                # FIX Cortez36: Use lazy logging formatting
                # FIX Cortez36: Added exc_info for stack trace
                logger.error("LLM value error (invalid parameters): %s: %s", type(e).__name__, str(e), exc_info=True)
                # Continue to fallback
            except Exception as e:
                # Fallback a plantillas si falla el LLM
                # FIX Cortez36: Use lazy logging formatting
                # FIX Cortez36: Added exc_info for stack trace
                logger.error("LLM generation failed unexpectedly, falling back to template. Error: %s: %s", type(e).__name__, str(e), exc_info=True)
                # Continue to fallback
        else:
            logger.warning("No LLM provider available, using template")
        
        # Fallback: Plantillas predefinidas
        questions = self._formulate_socratic_questions(prompt, cognitive_state)

        message = f"""
## An√°lisis del Problema

Para guiarte efectivamente, necesito comprender tu proceso de pensamiento.
Por favor, respond√© las siguientes preguntas:

{self._format_questions(questions)}

üìù **Importante**: No estoy evitando ayudarte. Estas preguntas son fundamentales
para que desarrolles tu capacidad de descomposici√≥n y an√°lisis de problemas,
que es m√°s valiosa que cualquier soluci√≥n espec√≠fica.

Una vez que compartas tu razonamiento, podr√© orientarte de manera precisa.
"""

        return {
            "message": message.strip(),
            "mode": TutorMode.SOCRATICO,
            "pedagogical_intent": "promote_decomposition_and_planning",
            "questions": questions,
            "requires_student_response": True,
            "metadata": {
                "cognitive_state": cognitive_state,
                "help_level": HelpLevel.MINIMO,
            }
        }

    async def _generate_socratic_with_llm(
        self,
        prompt: str,
        cognitive_state: str,
        strategy: Dict[str, Any]
    ) -> Dict[str, Any]:
        """Genera respuesta socr√°tica usando LLM (async)"""
        from ..llm.base import LLMMessage, LLMRole

        # Preparar par√°metros para el prompt
        student_level = strategy.get("student_level", CognitiveScaffoldingLevel.INTERMEDIO)
        semaforo_state_str = strategy.get("semaforo_state", "verde")

        # Convertir string de semaforo a enum si es necesario
        if isinstance(semaforo_state_str, str):
            semaforo_map = {"verde": SemaforoState.VERDE, "amarillo": SemaforoState.AMARILLO, "rojo": SemaforoState.ROJO}
            semaforo_state = semaforo_map.get(semaforo_state_str, SemaforoState.VERDE)
        else:
            semaforo_state = semaforo_state_str

        system_prompt = self.prompts.get_intervention_prompt(
            intervention_type=TutorInterventionType.PREGUNTA_SOCRATICA,
            student_level=student_level,
            semaforo_state=semaforo_state,
            context={
                "cognitive_state": cognitive_state,
                "help_level": strategy.get("help_level", "bajo"),
                "prompt": prompt
            }
        )

        messages = [
            LLMMessage(role=LLMRole.SYSTEM, content=system_prompt),
            LLMMessage(role=LLMRole.USER, content=f"Estudiante pregunta: {prompt}")
        ]

        # Ejecutar generate de forma as√≠ncrona
        llm_response = await self.llm_provider.generate(messages, temperature=0.7, max_tokens=500)

        # Extraer el texto de la respuesta
        response_text = llm_response.content if hasattr(llm_response, 'content') else str(llm_response)

        return {
            "message": response_text,
            "mode": TutorMode.SOCRATICO,
            "pedagogical_intent": "socratic_questioning",
            "requires_student_response": True,
            "metadata": {
                "cognitive_state": cognitive_state,
                "help_level": strategy.get("help_level", HelpLevel.BAJO),
                "generated_with_llm": True
            }
        }

    async def _generate_conceptual_with_llm(
        self,
        prompt: str,
        cognitive_state: str,
        strategy: Dict[str, Any]
    ) -> Dict[str, Any]:
        """Genera correcci√≥n conceptual usando LLM (async)"""
        from ..llm.base import LLMMessage, LLMRole

        student_level = strategy.get("student_level", CognitiveScaffoldingLevel.INTERMEDIO)
        semaforo_state_str = strategy.get("semaforo_state", "verde")

        if isinstance(semaforo_state_str, str):
            semaforo_map = {"verde": SemaforoState.VERDE, "amarillo": SemaforoState.AMARILLO, "rojo": SemaforoState.ROJO}
            semaforo_state = semaforo_map.get(semaforo_state_str, SemaforoState.VERDE)
        else:
            semaforo_state = semaforo_state_str

        system_prompt = self.prompts.get_intervention_prompt(
            intervention_type=TutorInterventionType.CORRECCION_CONCEPTUAL,
            student_level=student_level,
            semaforo_state=semaforo_state,
            context={
                "cognitive_state": cognitive_state,
                "help_level": strategy.get("help_level", "medio"),
                "prompt": prompt
            }
        )

        messages = [
            LLMMessage(role=LLMRole.SYSTEM, content=system_prompt),
            LLMMessage(role=LLMRole.USER, content=f"Pregunta del estudiante: {prompt}")
        ]

        # Ejecutar generate de forma as√≠ncrona
        llm_response = await self.llm_provider.generate(messages, temperature=0.7, max_tokens=600)

        response_text = llm_response.content if hasattr(llm_response, 'content') else str(llm_response)

        return {
            "message": response_text,
            "mode": TutorMode.EXPLICATIVO,
            "pedagogical_intent": "conceptual_understanding",
            "help_level": HelpLevel.MEDIO,
            "metadata": {
                "cognitive_state": cognitive_state,
                "provides_code": False,
                "generated_with_llm": True
            }
        }

    async def _generate_hints_with_llm(
        self,
        prompt: str,
        cognitive_state: str,
        strategy: Dict[str, Any],
        student_history: Optional[List[CognitiveTrace]] = None
    ) -> Dict[str, Any]:
        """Genera pistas guiadas usando LLM (async)"""
        from ..llm.base import LLMMessage, LLMRole

        student_level = strategy.get("student_level", CognitiveScaffoldingLevel.INTERMEDIO)
        semaforo_state_str = strategy.get("semaforo_state", "verde")
        help_level = self._determine_adaptive_help_level(student_history, strategy)

        if isinstance(semaforo_state_str, str):
            semaforo_map = {"verde": SemaforoState.VERDE, "amarillo": SemaforoState.AMARILLO, "rojo": SemaforoState.ROJO}
            semaforo_state = semaforo_map.get(semaforo_state_str, SemaforoState.VERDE)
        else:
            semaforo_state = semaforo_state_str

        system_prompt = self.prompts.get_intervention_prompt(
            intervention_type=TutorInterventionType.PISTAS_GUIADAS,
            student_level=student_level,
            semaforo_state=semaforo_state,
            context={
                "cognitive_state": cognitive_state,
                "help_level": help_level.value if hasattr(help_level, 'value') else str(help_level),
                "prompt": prompt,
                "previous_hints": self._count_previous_hints(student_history) if student_history else 0
            }
        )

        messages = [
            LLMMessage(role=LLMRole.SYSTEM, content=system_prompt),
            LLMMessage(role=LLMRole.USER, content=f"Estudiante pregunta: {prompt}")
        ]

        # Ejecutar generate de forma as√≠ncrona
        llm_response = await self.llm_provider.generate(messages, temperature=0.7, max_tokens=700)

        response_text = llm_response.content if hasattr(llm_response, 'content') else str(llm_response)

        return {
            "message": response_text,
            "mode": TutorMode.GUIADO,
            "pedagogical_intent": "scaffolding",
            "help_level": help_level,
            "requires_justification": True,
            "metadata": {
                "cognitive_state": cognitive_state,
                "generated_with_llm": True
            }
        }

    def _formulate_socratic_questions(
        self,
        prompt: str,
        cognitive_state: str
    ) -> List[str]:
        """Formula preguntas socr√°ticas adaptadas al contexto"""
        base_questions = [
            "¬øQu√© entend√©s que te est√°n pidiendo resolver en este problema?",
            "¬øQu√© conceptos o estructuras de datos consider√°s relevantes?",
            "¬øPod√©s describir con tus palabras c√≥mo funcionar√≠a una soluci√≥n?",
            "¬øQu√© intentaste hasta ahora? ¬øQu√© resultado obtuviste?",
        ]

        # Adaptar seg√∫n estado cognitivo
        if cognitive_state == "exploracion":
            base_questions.insert(
                1,
                "¬øQu√© partes del enunciado te resultan claras y cu√°les confusas?"
            )
        elif cognitive_state == "depuracion":
            base_questions = [
                "¬øQu√© comportamiento esperabas y qu√© obtuviste?",
                "¬øEn qu√© punto espec√≠fico falla tu c√≥digo?",
                "¬øQu√© hip√≥tesis ten√©s sobre la causa del error?",
                "¬øQu√© pruebas hiciste para verificar tu hip√≥tesis?",
            ]

        return base_questions

    def _format_questions(self, questions: List[str]) -> str:
        """Formatea lista de preguntas"""
        return "\n".join([f"{i+1}. {q}" for i, q in enumerate(questions)])

    async def _generate_conceptual_explanation(
        self,
        prompt: str,
        cognitive_state: str,
        strategy: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        Genera explicaci√≥n conceptual sin dar implementaci√≥n espec√≠fica

        Reduce carga extr√≠nseca, favorece carga germinal (Sweller, 1988)
        """
        # Usar LLM si est√° disponible
        if self.llm_provider:
            try:
                # FIX Cortez36: Use lazy logging formatting
                logger.info("Generating conceptual explanation with LLM for: %s...", prompt[:50])
                return await self._generate_conceptual_with_llm(prompt, cognitive_state, strategy)
            except Exception as e:
                # FIX Cortez36: Use lazy logging formatting
                # FIX Cortez36: Added exc_info for stack trace
                logger.error("LLM generation failed for conceptual explanation: %s: %s", type(e).__name__, str(e), exc_info=True)
                pass
        
        # Fallback: template gen√©rico
        message = """
## Conceptos Fundamentales

Vamos a abordar esto desde los conceptos fundamentales, sin adelantarnos a la implementaci√≥n.

### Concepto Clave

[El concepto principal que necesit√°s comprender para resolver este problema]

### Principios Importantes

1. **Principio 1**: [Explicaci√≥n del principio]
2. **Principio 2**: [Explicaci√≥n del principio]

### Ejemplo Simple

[Analog√≠a o ejemplo simple que ilustra el concepto]

### Conexi√≥n con tu Problema

Para tu caso espec√≠fico, estos conceptos significan que...

---

üí° **Pr√≥ximo paso**: Ahora que ten√©s m√°s claros estos conceptos, ¬øc√≥mo
pens√°s aplicarlos a tu problema? ¬øQu√© parte quer√©s que profundice?
"""

        return {
            "message": message.strip(),
            "mode": TutorMode.EXPLICATIVO,
            "pedagogical_intent": "conceptual_understanding",
            "help_level": HelpLevel.MEDIO,
            "metadata": {
                "cognitive_state": cognitive_state,
                "provides_code": False,
            }
        }

    async def _generate_guided_hints(
        self,
        prompt: str,
        cognitive_state: str,
        strategy: Dict[str, Any],
        student_history: Optional[List[CognitiveTrace]] = None
    ) -> Dict[str, Any]:
        """
        Genera pistas graduadas sin revelar la soluci√≥n completa

        Implementa andamiaje cognitivo (scaffolding) con niveles adaptativos
        seg√∫n el historial del estudiante.

        Niveles de pistas:
        - Nivel 1 (MINIMO): Preguntas socr√°ticas orientadoras
        - Nivel 2 (BAJO): Pistas conceptuales generales
        - Nivel 3 (MEDIO): Pistas con algo de detalle + pseudoc√≥digo alto nivel
        - Nivel 4 (ALTO): Fragmentos conceptuales + estrategia detallada
        """
        # Usar LLM si est√° disponible
        if self.llm_provider:
            try:
                # FIX Cortez36: Use lazy logging formatting
                logger.info("Generating guided hints with LLM for: %s...", prompt[:50])
                return await self._generate_hints_with_llm(prompt, cognitive_state, strategy, student_history)
            except Exception as e:
                # FIX Cortez36: Use lazy logging formatting
                # FIX Cortez36: Added exc_info for stack trace
                logger.error("LLM generation failed for hints: %s: %s", type(e).__name__, str(e), exc_info=True)
                pass
        
        # Fallback: l√≥gica de templates
        # Determinar nivel de ayuda basado en historial
        help_level = self._determine_adaptive_help_level(student_history, strategy)

        # Analizar cu√°ntas pistas ha recibido ya
        previous_hints_count = self._count_previous_hints(student_history) if student_history else 0

        # Generar pistas seg√∫n nivel
        if help_level == HelpLevel.MINIMO:
            hints = self._generate_level1_hints(prompt, cognitive_state)
        elif help_level == HelpLevel.BAJO:
            hints = self._generate_level2_hints(prompt, cognitive_state)
        elif help_level == HelpLevel.MEDIO:
            hints = self._generate_level3_hints(prompt, cognitive_state)
        else:  # ALTO
            hints = self._generate_level4_hints(prompt, cognitive_state)

        # Construir mensaje
        message = f"""
## Pistas Graduadas - Nivel {help_level.value.upper()}

{self._format_hints_message(hints, help_level)}

---

{self._generate_followup_question(help_level, previous_hints_count)}
"""

        return {
            "message": message.strip(),
            "mode": TutorMode.GUIADO,
            "pedagogical_intent": "scaffolding",
            "help_level": help_level,
            "hints_provided": hints,
            "hints_count": len(hints),
            "previous_hints_count": previous_hints_count,
            "requires_justification": True,
            "metadata": {
                "cognitive_state": cognitive_state,
                "provides_code": False,
                "provides_pseudocode": help_level in [HelpLevel.MEDIO, HelpLevel.ALTO],
                "adaptive_level": help_level.value,
            }
        }

    def _determine_adaptive_help_level(
        self,
        student_history: Optional[List[CognitiveTrace]],
        strategy: Dict[str, Any]
    ) -> HelpLevel:
        """
        Determina el nivel de ayuda adaptativamente seg√∫n:
        1. Estrategia sugerida por CRPE
        2. Historial de pistas recibidas (si recibi√≥ muchas, reducir detalle)
        3. Nivel de AI involvement acumulado
        """
        # Nivel base desde estrategia
        strategy_level = strategy.get("help_level", HelpLevel.MEDIO)

        if not student_history:
            return strategy_level

        # Contar pistas previas
        hints_received = self._count_previous_hints(student_history)

        # Si ya recibi√≥ muchas pistas (>5), reducir nivel para fomentar autonom√≠a
        if hints_received > 5:
            if strategy_level == HelpLevel.ALTO:
                return HelpLevel.MEDIO
            elif strategy_level == HelpLevel.MEDIO:
                return HelpLevel.BAJO

        # Calcular AI involvement promedio
        avg_ai_involvement = sum(t.ai_involvement for t in student_history) / len(student_history)

        # Si dependency alta (>0.6), reducir nivel de ayuda
        if avg_ai_involvement > 0.6:
            if strategy_level == HelpLevel.ALTO:
                return HelpLevel.MEDIO
            elif strategy_level == HelpLevel.MEDIO:
                return HelpLevel.BAJO

        return strategy_level

    def _count_previous_hints(self, student_history: List[CognitiveTrace]) -> int:
        """Cuenta cu√°ntas pistas ha recibido el estudiante"""
        return sum(
            1 for t in student_history
            if "hints_provided" in t.metadata.get("response_metadata", {})
        )

    def _generate_level1_hints(self, prompt: str, cognitive_state: str) -> List[Dict[str, str]]:
        """Nivel 1 - MINIMO: Solo preguntas socr√°ticas orientadoras"""
        return [
            {
                "level": 1,
                "type": "question",
                "content": "¬øQu√© pasos cre√©s que son necesarios para resolver este problema?"
            },
            {
                "level": 1,
                "type": "question",
                "content": "¬øQu√© conceptos o estructuras de datos podr√≠an ser relevantes aqu√≠?"
            },
            {
                "level": 1,
                "type": "question",
                "content": "¬øPod√©s describir con tus palabras c√≥mo funcionar√≠a una soluci√≥n ideal?"
            }
        ]

    def _generate_level2_hints(self, prompt: str, cognitive_state: str) -> List[Dict[str, str]]:
        """Nivel 2 - BAJO: Pistas conceptuales generales"""
        return [
            {
                "level": 2,
                "type": "conceptual",
                "content": "Pens√° en descomponer el problema en partes m√°s peque√±as. ¬øCu√°les ser√≠an esas partes?"
            },
            {
                "level": 2,
                "type": "conceptual",
                "content": "Consider√° qu√© estructura de datos se adapta mejor a las operaciones que necesit√°s realizar."
            },
            {
                "level": 2,
                "type": "reflection",
                "content": "¬øQu√© casos especiales o de borde deber√≠as tener en cuenta?"
            }
        ]

    def _generate_level3_hints(self, prompt: str, cognitive_state: str) -> List[Dict[str, str]]:
        """Nivel 3 - MEDIO: Pistas con detalle + pseudoc√≥digo alto nivel"""
        return [
            {
                "level": 3,
                "type": "decomposition",
                "content": "Divid√≠ el problema en estas etapas: 1) Inicializaci√≥n, 2) Operaci√≥n principal, 3) Validaci√≥n"
            },
            {
                "level": 3,
                "type": "strategy",
                "content": "Una estrategia com√∫n es usar [concepto general] para gestionar [aspecto del problema]"
            },
            {
                "level": 3,
                "type": "pseudocode",
                "content": """```
// Estructura general (alto nivel)
funci√≥n resolver():
    // Paso 1: Preparar datos
    // Paso 2: Procesar elemento por elemento
    // Paso 3: Retornar resultado
```"""
            }
        ]

    def _generate_level4_hints(self, prompt: str, cognitive_state: str) -> List[Dict[str, str]]:
        """Nivel 4 - ALTO: Fragmentos conceptuales + estrategia detallada"""
        return [
            {
                "level": 4,
                "type": "detailed_strategy",
                "content": "Consider√° este enfoque: [descripci√≥n detallada de estrategia sin c√≥digo espec√≠fico]"
            },
            {
                "level": 4,
                "type": "pattern",
                "content": "Un patr√≥n √∫til aqu√≠ es [nombre del patr√≥n], que consiste en [explicaci√≥n conceptual]"
            },
            {
                "level": 4,
                "type": "conceptual_fragment",
                "content": """Para gestionar [aspecto espec√≠fico]:
- Opci√≥n A: [ventajas y desventajas]
- Opci√≥n B: [ventajas y desventajas]
¬øCu√°l elegir√≠as y por qu√©?"""
            }
        ]

    def _format_hints_message(self, hints: List[Dict[str, str]], level: HelpLevel) -> str:
        """Formatea las pistas para el mensaje"""
        icons = {
            "question": "‚ùì",
            "conceptual": "üí°",
            "reflection": "ü§î",
            "decomposition": "üîç",
            "strategy": "üéØ",
            "pseudocode": "üìù",
            "detailed_strategy": "üó∫Ô∏è",
            "pattern": "üß©",
            "conceptual_fragment": "üí≠"
        }

        formatted = []
        for i, hint in enumerate(hints, 1):
            icon = icons.get(hint["type"], "‚Ä¢")
            hint_type = hint["type"].replace("_", " ").title()
            formatted.append(f"### {icon} Pista {i}: {hint_type}\n{hint['content']}")

        return "\n\n".join(formatted)

    def _generate_followup_question(self, level: HelpLevel, hints_count: int) -> str:
        """Genera pregunta de seguimiento seg√∫n contexto"""
        if hints_count > 5:
            return """‚ö†Ô∏è **Nota**: Has recibido varias pistas ya. Es momento de que intentes
avanzar de forma m√°s aut√≥noma. ¬øQu√© vas a hacer con la informaci√≥n que ten√©s?"""
        elif level == HelpLevel.MINIMO:
            return """‚ùì **Pregunta para vos**: Respond√© primero estas preguntas antes de
solicitar m√°s ayuda. La clave est√° en tu razonamiento, no en la respuesta de la IA."""
        elif level in [HelpLevel.MEDIO, HelpLevel.ALTO]:
            return """‚ùì **Pregunta para vos**: Bas√°ndote en estas pistas, ¬øcu√°l ser√≠a tu
pr√≥ximo paso concreto? ¬øQu√© decisi√≥n de dise√±o tomar√≠as y **por qu√©**?"""
        else:
            return """‚ùì **Pr√≥ximo paso**: Intent√° formular un plan bas√°ndote en estas pistas.
¬øQu√© har√≠as primero?"""

    def _generate_clarification_request(
        self,
        prompt: str,
        cognitive_state: str
    ) -> Dict[str, Any]:
        """Solicita clarificaci√≥n cuando el prompt es ambiguo"""
        message = """
## Necesito M√°s Informaci√≥n

Para poder ayudarte de manera efectiva, necesito que seas m√°s espec√≠fico:

### üìå Contexto del Problema
- ¬øQu√© parte exacta te genera dificultad?
- ¬øQu√© entend√©s que ten√©s que lograr?

### üìå Lo que Intentaste
- ¬øQu√© enfoque probaste?
- ¬øQu√© c√≥digo escribiste hasta ahora?
- ¬øQu√© resultado obtuviste vs. qu√© esperabas?

### üìå Tu Hip√≥tesis
- ¬øQu√© cre√©s que podr√≠a estar causando el problema?
- ¬øQu√© soluciones consideraste?

Por favor, reformul√° tu consulta incluyendo esta informaci√≥n.
"""

        return {
            "message": message.strip(),
            "mode": TutorMode.SOCRATICO,
            "pedagogical_intent": "promote_specificity",
            "requires_student_response": True,
            "metadata": {
                "cognitive_state": cognitive_state,
            }
        }

    def evaluate_student_response(
        self,
        student_response: str,
        previous_interaction: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        Eval√∫a la respuesta del estudiante a preguntas/pistas previas

        Detecta:
        - Nivel de elaboraci√≥n
        - Explicitaci√≥n del razonamiento
        - Justificaci√≥n de decisiones
        - Autocorrecci√≥n
        """
        analysis = {
            "has_justification": self._detect_justification(student_response),
            "shows_decomposition": self._detect_decomposition(student_response),
            "shows_planning": self._detect_planning(student_response),
            "shows_self_reflection": self._detect_self_reflection(student_response),
            "quality_score": 0.0,  # 0-1
        }

        # Calcular score de calidad
        score = 0.0
        if analysis["has_justification"]:
            score += 0.3
        if analysis["shows_decomposition"]:
            score += 0.3
        if analysis["shows_planning"]:
            score += 0.2
        if analysis["shows_self_reflection"]:
            score += 0.2

        analysis["quality_score"] = score

        return analysis

    def _detect_justification(self, text: str) -> bool:
        """Detecta si hay justificaci√≥n en la respuesta"""
        justification_signals = [
            "porque", "ya que", "debido a", "considerando que",
            "mi raz√≥n es", "pens√© que", "decid√≠", "eleg√≠"
        ]
        return any(signal in text.lower() for signal in justification_signals)

    def _detect_decomposition(self, text: str) -> bool:
        """Detecta si hay descomposici√≥n del problema"""
        decomposition_signals = [
            "primero", "luego", "despu√©s", "paso", "parte",
            "dividir", "separar", "componente", "subproblema"
        ]
        return any(signal in text.lower() for signal in decomposition_signals)

    def _detect_planning(self, text: str) -> bool:
        """Detecta si hay evidencia de planificaci√≥n"""
        planning_signals = [
            "voy a", "planeo", "mi estrategia", "mi plan",
            "primero har√©", "mi enfoque", "mi idea es"
        ]
        return any(signal in text.lower() for signal in planning_signals)

    def _detect_self_reflection(self, text: str) -> bool:
        """Detecta si hay reflexi√≥n metacognitiva"""
        reflection_signals = [
            "me doy cuenta", "entiendo que", "ahora veo",
            "me confund√≠", "comet√≠ el error", "deber√≠a"
        ]
        return any(signal in text.lower() for signal in reflection_signals)