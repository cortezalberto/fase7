"""
Submodelo 1: Tutor IA Disciplinar Cognitivo (T-IA-Cog)

Agente de andamiaje cognitivo y metacognitivo que amplifica capacidades
del estudiante sin sustituirlas, operando bajo reglas pedagÃ³gicas y Ã©ticas explÃ­citas.

VERSIÃ“N 2.0 - TUTOR SOCRÃTICO PERSONALIZADO
Incorpora:
1. Reglas inquebrantables (Anti-SoluciÃ³n, SocrÃ¡tico, ExplicitaciÃ³n, Refuerzo Conceptual)
2. Sistema de gobernanza con semÃ¡foros (Verde/Amarillo/Rojo)
3. Procesamiento IPC -> GSR -> Andamiaje
4. Metadata completa para anÃ¡lisis N4
"""
from typing import Optional, Dict, Any, List
from enum import Enum
from datetime import datetime
import uuid
import logging

logger = logging.getLogger(__name__)

from ..models.trace import CognitiveTrace, TraceLevel, InteractionType
from .tutor_rules import (
    TutorRulesEngine,
    TutorRule,
    InterventionType as TutorInterventionType,
    CognitiveScaffoldingLevel
)
from .tutor_governance import (
    TutorGovernanceEngine,
    SemaforoState,
    PromptIntent
)
from .tutor_metadata import (
    TutorMetadataTracker,
    TutorInterventionMetadata,
    StudentCognitiveEvent
)
from .tutor_prompts import TutorSystemPrompts


class TutorMode(str, Enum):
    """Modos de tutorÃ­a"""
    SOCRATICO = "socratico"  # Preguntas socrÃ¡ticas
    EXPLICATIVO = "explicativo"  # Explicaciones conceptuales
    GUIADO = "guiado"  # Pistas graduadas
    METACOGNITIVO = "metacognitivo"  # ReflexiÃ³n sobre el proceso


class HelpLevel(str, Enum):
    """Niveles de ayuda"""
    MINIMO = "minimo"  # Solo preguntas orientadoras
    BAJO = "bajo"  # Pistas muy generales
    MEDIO = "medio"  # Pistas con algo de detalle
    ALTO = "alto"  # Explicaciones detalladas (sin cÃ³digo completo)


class TutorCognitivoAgent:
    """
    T-IA-Cog: Tutor IA Disciplinar Cognitivo (VERSIÃ“N 2.0 - SocrÃ¡tico)

    Funciones principales:
    1. Guiar el razonamiento (NO proveer soluciones - REGLA INQUEBRANTABLE)
    2. Promover la explicitaciÃ³n del pensamiento (forzar justificaciÃ³n)
    3. Prevenir delegaciÃ³n acrÃ­tica (sistema de semÃ¡foros)
    4. Reforzar fundamentos conceptuales (no parches sintÃ¡cticos)
    5. Escalar dificultad cognitiva adaptativamente (por nivel)

    NUEVOS COMPONENTES V2.0:
    - TutorRulesEngine: Aplica las 4 reglas inquebrantables
    - TutorGovernanceEngine: Procesa IPC -> GSR -> Andamiaje
    - TutorMetadataTracker: Registra metadata para anÃ¡lisis N4
    - TutorSystemPrompts: System prompts personalizados por contexto

    Basado en:
    - CogniciÃ³n distribuida (Hutchins, 1995)
    - CogniciÃ³n extendida (Clark & Chalmers, 1998)
    - TeorÃ­a de carga cognitiva (Sweller, 1988)
    - AutorregulaciÃ³n (Zimmerman, 2002)
    """

    def __init__(self, llm_provider=None, config: Optional[Dict[str, Any]] = None):
        self.llm_provider = llm_provider
        self.config = config or {}
        
        # Log LLM provider status
        if llm_provider:
            # FIX Cortez36: Use lazy logging formatting
            logger.info("TutorCognitivoAgent initialized with LLM provider: %s", type(llm_provider).__name__)
        else:
            logger.warning("TutorCognitivoAgent initialized WITHOUT LLM provider - will use templates")
        
        # Componentes V2.0
        self.rules_engine = TutorRulesEngine(config)
        self.governance_engine = TutorGovernanceEngine(self.rules_engine)
        self.metadata_tracker = TutorMetadataTracker()
        self.prompts = TutorSystemPrompts()
        
        # PolÃ­ticas pedagÃ³gicas (legacy - mantenidas por compatibilidad)
        self.policies = {
            "prioritize_questions": True,
            "require_justification": True,
            "adaptive_difficulty": True,
            "max_help_level": HelpLevel.MEDIO,
            "block_complete_solutions": True,
        }

        # Actualizar con config
        if config:
            self.policies.update(config.get("policies", {}))
    
    async def process_student_request(
        self,
        session_id: str,
        student_prompt: str,
        student_profile: Dict[str, Any],
        conversation_history: Optional[List[Dict[str, Any]]] = None
    ) -> Dict[str, Any]:
        """
        MÃ‰TODO PRINCIPAL V2.0: Procesa request del estudiante con pipeline completo
        
        Pipeline:
        1. IPC (Ingesta y ComprensiÃ³n de Prompt)
        2. GSR (Gobernanza y SemÃ¡foro de Riesgo)
        3. SelecciÃ³n de Estrategia de Andamiaje
        4. Chequeo de Reglas PedagÃ³gicas
        5. GeneraciÃ³n de Respuesta
        6. Registro de Metadata para N4
        
        Args:
            session_id: ID de la sesiÃ³n
            student_prompt: Pregunta/solicitud del estudiante
            student_profile: Perfil del estudiante con mÃ©tricas
            conversation_history: Historial de la conversaciÃ³n
        
        Returns:
            Dict con:
            - message: Respuesta del tutor
            - intervention_type: Tipo de intervenciÃ³n
            - metadata: Metadata completa para N4
            - semaforo: Estado del semÃ¡foro
        """
        try:
            # ============================================================
            # VALIDACIÃ“N DE ENTRADA ROBUSTA
            # ============================================================
            if not session_id or not isinstance(session_id, str):
                # FIX Cortez36: Use lazy logging formatting
                logger.error("Invalid session_id: %s", session_id)
                raise ValueError("session_id debe ser un string no vacÃ­o")
            
            if not student_prompt or not isinstance(student_prompt, str):
                # FIX Cortez36: Use lazy logging formatting
                logger.error("Invalid student_prompt: %s", student_prompt)
                raise ValueError("student_prompt debe ser un string no vacÃ­o")
            
            if student_prompt.strip() == "":
                # FIX Cortez36: Use lazy logging formatting
                logger.warning("Empty student_prompt for session %s", session_id)
                return self._generate_error_response(
                    "Por favor, ingresÃ¡ una pregunta o consulta para que pueda ayudarte.",
                    session_id,
                    "empty_prompt"
                )
            
            if not isinstance(student_profile, dict):
                # FIX Cortez36: Use lazy logging formatting
                logger.error("Invalid student_profile type: %s", type(student_profile))
                student_profile = {}  # Usar perfil vacÃ­o como fallback
            
            conversation_history = conversation_history or []
            if not isinstance(conversation_history, list):
                # FIX Cortez36: Use lazy logging formatting
                logger.error("Invalid conversation_history type: %s", type(conversation_history))
                conversation_history = []
            
            interaction_id = str(uuid.uuid4())
            # FIX Cortez36: Use lazy logging formatting
            logger.info("Processing student request - Session: %s, Interaction: %s", session_id, interaction_id)
            
            # ============================================================
            # FASE 1-3: Procesamiento por Gobernanza (IPC -> GSR -> Andamiaje)
            # ============================================================
            try:
                governance_result = self.governance_engine.process_student_request(
                    student_prompt=student_prompt,
                    student_profile=student_profile,
                    conversation_history=conversation_history
                )
            except Exception as e:
                # FIX Cortez36: Use lazy logging formatting
                # FIX Cortez36: Added exc_info for stack trace
                logger.error("Error en gobernanza para session %s: %s: %s", session_id, type(e).__name__, e, exc_info=True)
                return self._generate_error_response(
                    "Tuve un problema procesando tu solicitud. Por favor, intentÃ¡ reformular tu pregunta.",
                    session_id,
                    "governance_error",
                    {"error_type": type(e).__name__, "error_message": str(e)}
                )
            
            # ============================================================
            # VALIDAR RESULTADO DE GOBERNANZA
            # ============================================================
            try:
                analysis = governance_result["analysis"]
                semaforo = governance_result["semaforo"]
                strategy = governance_result["strategy"]
                risk_details = governance_result["risk_details"]
            except KeyError as e:
                # FIX Cortez36: Use lazy logging formatting
                # FIX Cortez36: Added exc_info for stack trace
                logger.error("Missing key in governance_result: %s", e, exc_info=True)
                return self._generate_error_response(
                    "Hubo un problema al analizar tu solicitud. Por favor, intentÃ¡ nuevamente.",
                    session_id,
                    "governance_result_invalid",
                    {"missing_key": str(e)}
                )
            except Exception as e:
                # FIX Cortez36: Use lazy logging formatting
                # FIX Cortez36: Added exc_info for stack trace
                logger.error("Error extracting governance_result fields: %s: %s", type(e).__name__, e, exc_info=True)
                return self._generate_error_response(
                    "OcurriÃ³ un error inesperado. Por favor, intentÃ¡ reformular tu pregunta.",
                    session_id,
                    "governance_extraction_error",
                    {"error_type": type(e).__name__, "error_message": str(e)}
                )
            
            # ============================================================
            # FASE 4: Chequeo de Reglas PedagÃ³gicas
            # ============================================================
            try:
                rules_violations = self._check_pedagogical_rules(
                    student_prompt=student_prompt,
                    student_level=analysis.student_level,
                    conversation_history=conversation_history,
                    strategy=strategy
                )
            except Exception as e:
                # FIX Cortez36: Use lazy logging formatting
                # FIX Cortez36: Added exc_info for stack trace
                logger.error("Error checking pedagogical rules: %s: %s", type(e).__name__, e, exc_info=True)
                # Continuar sin violaciones de reglas
                rules_violations = {"critical_violation": False, "rules_applied": []}
            
            # ============================================================
            # APLICAR REGLAS O GENERAR RESPUESTA
            # ============================================================
            try:
                # Si hay violaciones crÃ­ticas (ej: solicitud de cÃ³digo), aplicar regla
                if rules_violations.get("critical_violation"):
                    # Aplicar intervenciÃ³n de rechazo pedagÃ³gico
                    intervention_type = TutorInterventionType.RECHAZO_PEDAGOGICO
                    message = rules_violations.get("rejection_message", "No puedo proporcionar una soluciÃ³n directa.")
                    counter_question = rules_violations.get("counter_question", "Â¿PodÃ©s explicarme quÃ© intentaste hasta ahora?")
                    
                    response = {
                        "message": f"{message}\n\n{counter_question}",
                        "intervention_type": intervention_type.value,
                        "semaforo": semaforo.value if hasattr(semaforo, 'value') else str(semaforo),
                        "requires_student_response": True,
                        "metadata": self._build_metadata(
                            interaction_id,
                            session_id,
                            intervention_type,
                            analysis,
                            semaforo,
                            strategy,
                            rules_violations.get("rules_applied", [])
                        )
                    }
                else:
                    # FASE 5: GeneraciÃ³n de Respuesta Normal
                    response = await self._generate_tutor_response(
                        interaction_id=interaction_id,
                        session_id=session_id,
                        student_prompt=student_prompt,
                        strategy=strategy,
                        analysis=analysis,
                        semaforo=semaforo,
                        risk_details=risk_details
                    )
            except Exception as e:
                # FIX Cortez36: Use lazy logging formatting
                # FIX Cortez36: Added exc_info for stack trace
                logger.error("Error generating tutor response: %s: %s", type(e).__name__, e, exc_info=True)
                return self._generate_error_response(
                    "Tuve dificultades para generar una respuesta. Por favor, intentÃ¡ reformular tu pregunta de otra manera.",
                    session_id,
                    "response_generation_error",
                    {"error_type": type(e).__name__, "error_message": str(e)}
                )
            
            # ============================================================
            # FASE 6: Registrar Metadata para N4
            # ============================================================
            try:
                self.metadata_tracker.record_intervention(
                    session_id=session_id,
                    interaction_id=interaction_id,
                    intervention_type=response.get("intervention_type"),
                    student_level=analysis.student_level,
                    help_level=strategy.get("help_level", "medio"),
                    semaforo_state=semaforo,
                    cognitive_state=analysis.cognitive_state,
                    student_intent=analysis.intent.value if hasattr(analysis.intent, 'value') else str(analysis.intent),
                    autonomy_level=analysis.autonomy_level,
                    rules_applied=rules_violations.get("rules_applied", []),
                    restrictions_applied=risk_details.get("restrictions", []),
                    additional_metadata=response.get("metadata", {})
                )
            except Exception as e:
                # FIX Cortez36: Use lazy logging formatting
                logger.warning("Error recording metadata (non-critical): %s: %s", type(e).__name__, e)
                # No bloqueamos la respuesta por errores de metadata
            
            # FIX Cortez36: Use lazy logging formatting
            logger.info("Successfully processed student request - Session: %s, Interaction: %s", session_id, interaction_id)
            return response
            
        except ValueError as ve:
            # Errores de validaciÃ³n ya fueron manejados arriba
            raise ve
        except Exception as e:
            # FIX Cortez36: Use lazy logging formatting
            # FIX Cortez36: Added exc_info for stack trace
            logger.error("Unexpected critical error in process_student_request: %s: %s", type(e).__name__, e, exc_info=True)
            return self._generate_error_response(
                "OcurriÃ³ un error inesperado. Por favor, intentÃ¡ nuevamente mÃ¡s tarde o contactÃ¡ al soporte tÃ©cnico.",
                session_id if 'session_id' in locals() else "unknown",
                "critical_error",
                {"error_type": type(e).__name__, "error_message": str(e)}
            )
    
    def _check_pedagogical_rules(
        self,
        student_prompt: str,
        student_level: CognitiveScaffoldingLevel,
        conversation_history: List[Dict[str, Any]],
        strategy: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        Chequea las reglas pedagÃ³gicas inquebrantables
        
        Returns:
            Dict con:
            - critical_violation: bool
            - rules_applied: List[str]
            - rejection_message: str (si aplica)
            - counter_question: str (si aplica)
        """
        violations = {
            "critical_violation": False,
            "rules_applied": [],
        }
        
        # Regla #1: Anti-SoluciÃ³n Directa
        anti_solution_result = self.rules_engine.check_anti_solution_rule(
            student_request=student_prompt,
            student_level=student_level
        )
        
        if anti_solution_result.get("violated"):
            violations["critical_violation"] = True
            violations["rules_applied"].append(TutorRule.ANTI_SOLUCION.value)
            violations["rejection_message"] = anti_solution_result.get("message")
            violations["counter_question"] = anti_solution_result.get("counter_question")
            return violations  # Retornar inmediatamente
        
        # Regla #2: Modo SocrÃ¡tico Prioritario
        socratic_result = self.rules_engine.check_socratic_priority_rule(
            conversation_context=conversation_history
        )
        
        if socratic_result.get("should_question_first"):
            violations["rules_applied"].append(TutorRule.MODO_SOCRATICO.value)
            strategy["force_socratic"] = True
        
        # Regla #3: Exigencia de ExplicitaciÃ³n
        explicitacion_result = self.rules_engine.check_explicitacion_rule(
            student_message=student_prompt,
            conversation_context=conversation_history
        )
        
        if explicitacion_result.get("needs_explicitacion"):
            violations["rules_applied"].append(TutorRule.EXIGIR_EXPLICITACION.value)
            strategy["require_justification"] = True
            strategy["explicitacion_message"] = explicitacion_result.get("message")
        
        return violations
    
    async def _generate_tutor_response(
        self,
        interaction_id: str,
        session_id: str,
        student_prompt: str,
        strategy: Dict[str, Any],
        analysis: Any,  # StudentContextAnalysis
        semaforo: SemaforoState,
        risk_details: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        Genera respuesta del tutor segÃºn estrategia y contexto
        
        Usa system prompts personalizados para garantizar cumplimiento de reglas.
        """
        intervention_type = strategy.get("intervention_type", TutorInterventionType.PREGUNTA_SOCRATICA)
        
        # Generar system prompt personalizado
        system_prompt = self.prompts.get_intervention_prompt(
            intervention_type=intervention_type,
            student_level=analysis.student_level,
            semaforo_state=semaforo,
            context={
                "risk_type": risk_details.get("risk_type"),
                "restrictions": risk_details.get("restrictions", []),
                "student_intent": analysis.intent.value,
                "cognitive_state": analysis.cognitive_state
            }
        )
        
        # Si hay mensaje de advertencia (semÃ¡foro rojo), incluirlo
        warning_message = risk_details.get("warning_message")
        
        # âœ… NUEVO: Si hay LLM provider, usar respuesta dinÃ¡mica con memoria de conversaciÃ³n
        if self.llm_provider and hasattr(self.llm_provider, 'generate'):
            try:
                # Recuperar historial de conversaciÃ³n (ya cargado en guide())
                conversation_history = strategy.get("conversation_history", [])
                
                # Agregar el prompt actual al historial
                messages = conversation_history + [
                    {"role": "user", "content": student_prompt}
                ]
                
                # Generar respuesta con contexto completo
                llm_response = await self.llm_provider.generate(
                    messages=messages,
                    system_prompt=system_prompt,
                    temperature=0.7,
                    max_tokens=500
                )
                
                # Extraer el contenido de la respuesta del LLM
                response_content = llm_response.content if hasattr(llm_response, 'content') else str(llm_response)
                
                legacy_response = {
                    "message": response_content,
                    "requires_student_response": True
                }
                
            except Exception as e:
                # FIX Cortez36: Use lazy logging formatting
                logger.warning("LLM generation failed, using fallback: %s", e)
                # Fallback a mÃ©todo legacy
                response_type = strategy.get("response_type", "socratic_questioning")
                if response_type == "socratic_questioning":
                    legacy_response = await self._generate_socratic_response(
                        student_prompt, analysis.cognitive_state, strategy
                    )
                elif response_type == "conceptual_explanation":
                    legacy_response = await self._generate_conceptual_explanation(
                        student_prompt, analysis.cognitive_state, strategy
                    )
                elif response_type == "guided_hints":
                    legacy_response = await self._generate_guided_hints(
                        student_prompt, analysis.cognitive_state, strategy, None
                    )
                else:
                    legacy_response = self._generate_clarification_request(
                        student_prompt, analysis.cognitive_state
                    )
        else:
            # Sin LLM provider, usar mÃ©todo legacy
            response_type = strategy.get("response_type", "socratic_questioning")

            if response_type == "socratic_questioning":
                legacy_response = await self._generate_socratic_response(
                    student_prompt, analysis.cognitive_state, strategy
                )
            elif response_type == "conceptual_explanation":
                legacy_response = await self._generate_conceptual_explanation(
                    student_prompt, analysis.cognitive_state, strategy
                )
            elif response_type == "guided_hints":
                legacy_response = await self._generate_guided_hints(
                    student_prompt, analysis.cognitive_state, strategy, None
                )
            else:
                # Manejar tipos de respuesta desconocidos
                # FIX Cortez36: Use lazy logging formatting
                logger.warning("Unknown response_type '%s', using conceptual_explanation as fallback", response_type)
                legacy_response = await self._generate_conceptual_explanation(
                    student_prompt, analysis.cognitive_state, strategy
                )
        
        # Combinar warning si existe
        final_message = legacy_response["message"]
        if warning_message:
            final_message = f"{warning_message}\n\n---\n\n{final_message}"
        
        response_dict = {
            "message": final_message,
            "intervention_type": intervention_type.value if hasattr(intervention_type, 'value') else intervention_type,
            "semaforo": semaforo.value,
            "help_level": strategy.get("help_level"),
            "requires_student_response": legacy_response.get("requires_student_response", True),
            "system_prompt_used": system_prompt,  # Para logging/debugging
            "metadata": self._build_metadata(
                interaction_id,
                session_id,
                intervention_type,
                analysis,
                semaforo,
                strategy,
                []
            )
        }
        
        # Guardar la respuesta del tutor en metadata para historial
        response_dict["metadata"]["tutor_response"] = final_message
        
        return response_dict
    
    def _generate_error_response(
        self,
        error_message: str,
        session_id: str,
        error_type: str,
        details: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        Genera una respuesta de error consistente y amigable para el usuario
        
        Args:
            error_message: Mensaje de error amigable para el estudiante
            session_id: ID de la sesiÃ³n
            error_type: Tipo de error para logging
            details: Detalles adicionales del error
        
        Returns:
            Dict con respuesta de error estructurada
        """
        # FIX Cortez36: Use lazy logging formatting
        logger.warning("Generating error response - Session: %s, Type: %s", session_id, error_type)
        
        return {
            "message": error_message,
            "intervention_type": "error_recovery",
            "semaforo": SemaforoState.AMARILLO.value,
            "requires_student_response": True,
            "metadata": {
                "error_type": error_type,
                "error_details": details or {},
                "timestamp": datetime.now().isoformat(),
                "session_id": session_id
            }
        }
    
    def _build_metadata(
        self,
        interaction_id: str,
        session_id: str,
        intervention_type: Any,
        analysis: Any,
        semaforo: SemaforoState,
        strategy: Dict[str, Any],
        rules_applied: List[str]
    ) -> Dict[str, Any]:
        """Construye metadata completa para N4"""
        return {
            "interaction_id": interaction_id,
            "session_id": session_id,
            "intervention_type": intervention_type.value if hasattr(intervention_type, 'value') else intervention_type,
            "student_level": analysis.student_level.value if hasattr(analysis, 'student_level') else "intermedio",
            "student_intent": analysis.intent.value if hasattr(analysis, 'intent') else "exploracion",
            "cognitive_state": analysis.cognitive_state if hasattr(analysis, 'cognitive_state') else "exploracion",
            "autonomy_level": analysis.autonomy_level if hasattr(analysis, 'autonomy_level') else 0.5,
            "semaforo": semaforo.value,
            "help_level": strategy.get("help_level", "medio"),
            "rules_applied": rules_applied,
            "restrictions": strategy.get("restrictions", []),
            "allow_code": strategy.get("allow_code", False),
            "allow_pseudocode": strategy.get("allow_pseudocode", True),
            "require_justification": strategy.get("require_justification", True)
        }
    
    def evaluate_student_response_v2(
        self,
        session_id: str,
        interaction_id: str,
        student_response: str,
        time_to_response_minutes: float
    ) -> Dict[str, Any]:
        """
        NUEVO V2.0: EvalÃºa respuesta del estudiante y actualiza metadata
        
        Args:
            session_id: ID de la sesiÃ³n
            interaction_id: ID de la intervenciÃ³n a evaluar
            student_response: Respuesta del estudiante
            time_to_response_minutes: Tiempo en minutos
        
        Returns:
            Dict con anÃ¡lisis de la respuesta
        """
        # Buscar la intervenciÃ³n en el historial
        intervention = None
        for i in self.metadata_tracker.intervention_history:
            if i.interaction_id == interaction_id:
                intervention = i
                break
        
        if not intervention:
            return {"error": "Intervention not found"}
        
        # Detectar eventos cognitivos
        cognitive_events = self.metadata_tracker.detect_cognitive_events(
            student_response=student_response,
            previous_intervention=intervention
        )
        
        # Evaluar efectividad
        effectiveness = self.metadata_tracker.evaluate_intervention_effectiveness(
            intervention=intervention,
            student_response=student_response,
            cognitive_events=cognitive_events,
            time_to_response_minutes=time_to_response_minutes
        )
        
        # Actualizar metadata
        self.metadata_tracker.update_intervention_effectiveness(
            interaction_id=interaction_id,
            effectiveness=effectiveness,
            cognitive_events=cognitive_events
        )
        
        return {
            "cognitive_events": [e.value for e in cognitive_events],
            "effectiveness": effectiveness.value,
            "analysis": self.evaluate_student_response(student_response, {}),  # Legacy
            "should_adjust_strategy": self._should_adjust_strategy(
                effectiveness, cognitive_events
            )
        }
    
    def _should_adjust_strategy(
        self,
        effectiveness: Any,
        cognitive_events: List[StudentCognitiveEvent]
    ) -> Dict[str, Any]:
        """Determina si se debe ajustar la estrategia pedagÃ³gica"""
        from .tutor_metadata import InterventionEffectiveness
        
        # Si la intervenciÃ³n fue muy efectiva, mantener nivel
        if effectiveness == InterventionEffectiveness.MUY_EFECTIVA:
            return {"adjust": False, "reason": "highly_effective"}
        
        # Si fue contraproducente, reducir complejidad
        if effectiveness == InterventionEffectiveness.CONTRAPRODUCENTE:
            return {
                "adjust": True,
                "direction": "simplify",
                "reason": "counterproductive_intervention"
            }
        
        # Si no hay eventos cognitivos positivos, incrementar guÃ­a
        positive_events = [
            StudentCognitiveEvent.AUTOCORRECCION,
            StudentCognitiveEvent.REFLEXION_METACOGNITIVA,
            StudentCognitiveEvent.JUSTIFICACION_DECISION
        ]
        
        if not any(event in cognitive_events for event in positive_events):
            return {
                "adjust": True,
                "direction": "increase_guidance",
                "reason": "lack_of_cognitive_events"
            }
        
        return {"adjust": False, "reason": "adequate_effectiveness"}
    
    def get_session_analytics_n4(self, session_id: str) -> Dict[str, Any]:
        """
        NUEVO V2.0: Obtiene analytics N4 de una sesiÃ³n
        
        Args:
            session_id: ID de la sesiÃ³n
        
        Returns:
            MÃ©tricas agregadas para anÃ¡lisis N4
        """
        return self.metadata_tracker.generate_n4_analytics(session_id)

        # PolÃ­ticas pedagÃ³gicas
        self.policies = {
            "prioritize_questions": True,
            "require_justification": True,
            "adaptive_difficulty": True,
            "max_help_level": HelpLevel.MEDIO,
            "block_complete_solutions": True,
        }

        # Actualizar con config
        if config:
            self.policies.update(config.get("policies", {}))

    def generate_response(
        self,
        student_prompt: str,
        cognitive_state: str,
        strategy: Dict[str, Any],
        student_history: Optional[List[CognitiveTrace]] = None
    ) -> Dict[str, Any]:
        """
        Genera respuesta tutorial basada en principios pedagÃ³gicos

        Args:
            student_prompt: Pregunta/solicitud del estudiante
            cognitive_state: Estado cognitivo actual
            strategy: Estrategia pedagÃ³gica definida por CRPE
            student_history: Historial de interacciones

        Returns:
            Diccionario con respuesta y metadata pedagÃ³gica
        """
        response_type = strategy.get("response_type", "socratic_questioning")

        if response_type == "socratic_questioning":
            return self._generate_socratic_response(
                student_prompt, cognitive_state, strategy
            )
        elif response_type == "conceptual_explanation":
            return self._generate_conceptual_explanation(
                student_prompt, cognitive_state, strategy
            )
        elif response_type == "guided_hints":
            return self._generate_guided_hints(
                student_prompt, cognitive_state, strategy, student_history
            )
        else:
            return self._generate_clarification_request(
                student_prompt, cognitive_state
            )

    async def _generate_socratic_response(
        self,
        prompt: str,
        cognitive_state: str,
        strategy: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        Genera respuesta socrÃ¡tica con preguntas que guÃ­an el razonamiento

        Usa LLM si estÃ¡ disponible, sino usa plantillas predefinidas.
        """
        # Si hay LLM provider, generar respuesta con IA
        if self.llm_provider:
            try:
                # FIX Cortez36: Use lazy logging formatting
                logger.info("Attempting to generate Socratic response with LLM for prompt: %s...", prompt[:50])
                result = await self._generate_socratic_with_llm(prompt, cognitive_state, strategy)
                # FIX Cortez36: Use lazy logging formatting
                logger.info("Successfully generated Socratic response with LLM")
                return result
            except AttributeError as e:
                # FIX Cortez36: Use lazy logging formatting
                # FIX Cortez36: Added exc_info for stack trace
                logger.error("LLM attribute error (method not found): %s: %s", type(e).__name__, str(e), exc_info=True)
                # Continue to fallback
            except ValueError as e:
                # FIX Cortez36: Use lazy logging formatting
                # FIX Cortez36: Added exc_info for stack trace
                logger.error("LLM value error (invalid parameters): %s: %s", type(e).__name__, str(e), exc_info=True)
                # Continue to fallback
            except Exception as e:
                # Fallback a plantillas si falla el LLM
                # FIX Cortez36: Use lazy logging formatting
                # FIX Cortez36: Added exc_info for stack trace
                logger.error("LLM generation failed unexpectedly, falling back to template. Error: %s: %s", type(e).__name__, str(e), exc_info=True)
                # Continue to fallback
        else:
            logger.warning("No LLM provider available, using template")
        
        # Fallback: Plantillas predefinidas
        questions = self._formulate_socratic_questions(prompt, cognitive_state)

        message = f"""
## AnÃ¡lisis del Problema

Para guiarte efectivamente, necesito comprender tu proceso de pensamiento.
Por favor, respondÃ© las siguientes preguntas:

{self._format_questions(questions)}

ðŸ“ **Importante**: No estoy evitando ayudarte. Estas preguntas son fundamentales
para que desarrolles tu capacidad de descomposiciÃ³n y anÃ¡lisis de problemas,
que es mÃ¡s valiosa que cualquier soluciÃ³n especÃ­fica.

Una vez que compartas tu razonamiento, podrÃ© orientarte de manera precisa.
"""

        return {
            "message": message.strip(),
            "mode": TutorMode.SOCRATICO,
            "pedagogical_intent": "promote_decomposition_and_planning",
            "questions": questions,
            "requires_student_response": True,
            "metadata": {
                "cognitive_state": cognitive_state,
                "help_level": HelpLevel.MINIMO,
            }
        }

    async def _generate_socratic_with_llm(
        self,
        prompt: str,
        cognitive_state: str,
        strategy: Dict[str, Any]
    ) -> Dict[str, Any]:
        """Genera respuesta socrÃ¡tica usando LLM (async)"""
        from ..llm.base import LLMMessage, LLMRole

        # Preparar parÃ¡metros para el prompt
        student_level = strategy.get("student_level", CognitiveScaffoldingLevel.INTERMEDIO)
        semaforo_state_str = strategy.get("semaforo_state", "verde")

        # Convertir string de semaforo a enum si es necesario
        if isinstance(semaforo_state_str, str):
            semaforo_map = {"verde": SemaforoState.VERDE, "amarillo": SemaforoState.AMARILLO, "rojo": SemaforoState.ROJO}
            semaforo_state = semaforo_map.get(semaforo_state_str, SemaforoState.VERDE)
        else:
            semaforo_state = semaforo_state_str

        system_prompt = self.prompts.get_intervention_prompt(
            intervention_type=TutorInterventionType.PREGUNTA_SOCRATICA,
            student_level=student_level,
            semaforo_state=semaforo_state,
            context={
                "cognitive_state": cognitive_state,
                "help_level": strategy.get("help_level", "bajo"),
                "prompt": prompt
            }
        )

        messages = [
            LLMMessage(role=LLMRole.SYSTEM, content=system_prompt),
            LLMMessage(role=LLMRole.USER, content=f"Estudiante pregunta: {prompt}")
        ]

        # Ejecutar generate de forma asÃ­ncrona
        llm_response = await self.llm_provider.generate(messages, temperature=0.7, max_tokens=500)

        # Extraer el texto de la respuesta
        response_text = llm_response.content if hasattr(llm_response, 'content') else str(llm_response)

        return {
            "message": response_text,
            "mode": TutorMode.SOCRATICO,
            "pedagogical_intent": "socratic_questioning",
            "requires_student_response": True,
            "metadata": {
                "cognitive_state": cognitive_state,
                "help_level": strategy.get("help_level", HelpLevel.BAJO),
                "generated_with_llm": True
            }
        }

    async def _generate_conceptual_with_llm(
        self,
        prompt: str,
        cognitive_state: str,
        strategy: Dict[str, Any]
    ) -> Dict[str, Any]:
        """Genera correcciÃ³n conceptual usando LLM (async)"""
        from ..llm.base import LLMMessage, LLMRole

        student_level = strategy.get("student_level", CognitiveScaffoldingLevel.INTERMEDIO)
        semaforo_state_str = strategy.get("semaforo_state", "verde")

        if isinstance(semaforo_state_str, str):
            semaforo_map = {"verde": SemaforoState.VERDE, "amarillo": SemaforoState.AMARILLO, "rojo": SemaforoState.ROJO}
            semaforo_state = semaforo_map.get(semaforo_state_str, SemaforoState.VERDE)
        else:
            semaforo_state = semaforo_state_str

        system_prompt = self.prompts.get_intervention_prompt(
            intervention_type=TutorInterventionType.CORRECCION_CONCEPTUAL,
            student_level=student_level,
            semaforo_state=semaforo_state,
            context={
                "cognitive_state": cognitive_state,
                "help_level": strategy.get("help_level", "medio"),
                "prompt": prompt
            }
        )

        messages = [
            LLMMessage(role=LLMRole.SYSTEM, content=system_prompt),
            LLMMessage(role=LLMRole.USER, content=f"Pregunta del estudiante: {prompt}")
        ]

        # Ejecutar generate de forma asÃ­ncrona
        llm_response = await self.llm_provider.generate(messages, temperature=0.7, max_tokens=600)

        response_text = llm_response.content if hasattr(llm_response, 'content') else str(llm_response)

        return {
            "message": response_text,
            "mode": TutorMode.EXPLICATIVO,
            "pedagogical_intent": "conceptual_understanding",
            "help_level": HelpLevel.MEDIO,
            "metadata": {
                "cognitive_state": cognitive_state,
                "provides_code": False,
                "generated_with_llm": True
            }
        }

    async def _generate_hints_with_llm(
        self,
        prompt: str,
        cognitive_state: str,
        strategy: Dict[str, Any],
        student_history: Optional[List[CognitiveTrace]] = None
    ) -> Dict[str, Any]:
        """Genera pistas guiadas usando LLM (async)"""
        from ..llm.base import LLMMessage, LLMRole

        student_level = strategy.get("student_level", CognitiveScaffoldingLevel.INTERMEDIO)
        semaforo_state_str = strategy.get("semaforo_state", "verde")
        help_level = self._determine_adaptive_help_level(student_history, strategy)

        if isinstance(semaforo_state_str, str):
            semaforo_map = {"verde": SemaforoState.VERDE, "amarillo": SemaforoState.AMARILLO, "rojo": SemaforoState.ROJO}
            semaforo_state = semaforo_map.get(semaforo_state_str, SemaforoState.VERDE)
        else:
            semaforo_state = semaforo_state_str

        system_prompt = self.prompts.get_intervention_prompt(
            intervention_type=TutorInterventionType.PISTAS_GUIADAS,
            student_level=student_level,
            semaforo_state=semaforo_state,
            context={
                "cognitive_state": cognitive_state,
                "help_level": help_level.value if hasattr(help_level, 'value') else str(help_level),
                "prompt": prompt,
                "previous_hints": self._count_previous_hints(student_history) if student_history else 0
            }
        )

        messages = [
            LLMMessage(role=LLMRole.SYSTEM, content=system_prompt),
            LLMMessage(role=LLMRole.USER, content=f"Estudiante pregunta: {prompt}")
        ]

        # Ejecutar generate de forma asÃ­ncrona
        llm_response = await self.llm_provider.generate(messages, temperature=0.7, max_tokens=700)

        response_text = llm_response.content if hasattr(llm_response, 'content') else str(llm_response)

        return {
            "message": response_text,
            "mode": TutorMode.GUIADO,
            "pedagogical_intent": "scaffolding",
            "help_level": help_level,
            "requires_justification": True,
            "metadata": {
                "cognitive_state": cognitive_state,
                "generated_with_llm": True
            }
        }

    def _formulate_socratic_questions(
        self,
        prompt: str,
        cognitive_state: str
    ) -> List[str]:
        """Formula preguntas socrÃ¡ticas adaptadas al contexto"""
        base_questions = [
            "Â¿QuÃ© entendÃ©s que te estÃ¡n pidiendo resolver en este problema?",
            "Â¿QuÃ© conceptos o estructuras de datos considerÃ¡s relevantes?",
            "Â¿PodÃ©s describir con tus palabras cÃ³mo funcionarÃ­a una soluciÃ³n?",
            "Â¿QuÃ© intentaste hasta ahora? Â¿QuÃ© resultado obtuviste?",
        ]

        # Adaptar segÃºn estado cognitivo
        if cognitive_state == "exploracion":
            base_questions.insert(
                1,
                "Â¿QuÃ© partes del enunciado te resultan claras y cuÃ¡les confusas?"
            )
        elif cognitive_state == "depuracion":
            base_questions = [
                "Â¿QuÃ© comportamiento esperabas y quÃ© obtuviste?",
                "Â¿En quÃ© punto especÃ­fico falla tu cÃ³digo?",
                "Â¿QuÃ© hipÃ³tesis tenÃ©s sobre la causa del error?",
                "Â¿QuÃ© pruebas hiciste para verificar tu hipÃ³tesis?",
            ]

        return base_questions

    def _format_questions(self, questions: List[str]) -> str:
        """Formatea lista de preguntas"""
        return "\n".join([f"{i+1}. {q}" for i, q in enumerate(questions)])

    async def _generate_conceptual_explanation(
        self,
        prompt: str,
        cognitive_state: str,
        strategy: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        Genera explicaciÃ³n conceptual sin dar implementaciÃ³n especÃ­fica

        Reduce carga extrÃ­nseca, favorece carga germinal (Sweller, 1988)
        """
        # Usar LLM si estÃ¡ disponible
        if self.llm_provider:
            try:
                # FIX Cortez36: Use lazy logging formatting
                logger.info("Generating conceptual explanation with LLM for: %s...", prompt[:50])
                return await self._generate_conceptual_with_llm(prompt, cognitive_state, strategy)
            except Exception as e:
                # FIX Cortez36: Use lazy logging formatting
                # FIX Cortez36: Added exc_info for stack trace
                logger.error("LLM generation failed for conceptual explanation: %s: %s", type(e).__name__, str(e), exc_info=True)
                pass
        
        # Fallback: template genÃ©rico
        message = """
## Conceptos Fundamentales

Vamos a abordar esto desde los conceptos fundamentales, sin adelantarnos a la implementaciÃ³n.

### Concepto Clave

[El concepto principal que necesitÃ¡s comprender para resolver este problema]

### Principios Importantes

1. **Principio 1**: [ExplicaciÃ³n del principio]
2. **Principio 2**: [ExplicaciÃ³n del principio]

### Ejemplo Simple

[AnalogÃ­a o ejemplo simple que ilustra el concepto]

### ConexiÃ³n con tu Problema

Para tu caso especÃ­fico, estos conceptos significan que...

---

ðŸ’¡ **PrÃ³ximo paso**: Ahora que tenÃ©s mÃ¡s claros estos conceptos, Â¿cÃ³mo
pensÃ¡s aplicarlos a tu problema? Â¿QuÃ© parte querÃ©s que profundice?
"""

        return {
            "message": message.strip(),
            "mode": TutorMode.EXPLICATIVO,
            "pedagogical_intent": "conceptual_understanding",
            "help_level": HelpLevel.MEDIO,
            "metadata": {
                "cognitive_state": cognitive_state,
                "provides_code": False,
            }
        }

    async def _generate_guided_hints(
        self,
        prompt: str,
        cognitive_state: str,
        strategy: Dict[str, Any],
        student_history: Optional[List[CognitiveTrace]] = None
    ) -> Dict[str, Any]:
        """
        Genera pistas graduadas sin revelar la soluciÃ³n completa

        Implementa andamiaje cognitivo (scaffolding) con niveles adaptativos
        segÃºn el historial del estudiante.

        Niveles de pistas:
        - Nivel 1 (MINIMO): Preguntas socrÃ¡ticas orientadoras
        - Nivel 2 (BAJO): Pistas conceptuales generales
        - Nivel 3 (MEDIO): Pistas con algo de detalle + pseudocÃ³digo alto nivel
        - Nivel 4 (ALTO): Fragmentos conceptuales + estrategia detallada
        """
        # Usar LLM si estÃ¡ disponible
        if self.llm_provider:
            try:
                # FIX Cortez36: Use lazy logging formatting
                logger.info("Generating guided hints with LLM for: %s...", prompt[:50])
                return await self._generate_hints_with_llm(prompt, cognitive_state, strategy, student_history)
            except Exception as e:
                # FIX Cortez36: Use lazy logging formatting
                # FIX Cortez36: Added exc_info for stack trace
                logger.error("LLM generation failed for hints: %s: %s", type(e).__name__, str(e), exc_info=True)
                pass
        
        # Fallback: lÃ³gica de templates
        # Determinar nivel de ayuda basado en historial
        help_level = self._determine_adaptive_help_level(student_history, strategy)

        # Analizar cuÃ¡ntas pistas ha recibido ya
        previous_hints_count = self._count_previous_hints(student_history) if student_history else 0

        # Generar pistas segÃºn nivel
        if help_level == HelpLevel.MINIMO:
            hints = self._generate_level1_hints(prompt, cognitive_state)
        elif help_level == HelpLevel.BAJO:
            hints = self._generate_level2_hints(prompt, cognitive_state)
        elif help_level == HelpLevel.MEDIO:
            hints = self._generate_level3_hints(prompt, cognitive_state)
        else:  # ALTO
            hints = self._generate_level4_hints(prompt, cognitive_state)

        # Construir mensaje
        message = f"""
## Pistas Graduadas - Nivel {help_level.value.upper()}

{self._format_hints_message(hints, help_level)}

---

{self._generate_followup_question(help_level, previous_hints_count)}
"""

        return {
            "message": message.strip(),
            "mode": TutorMode.GUIADO,
            "pedagogical_intent": "scaffolding",
            "help_level": help_level,
            "hints_provided": hints,
            "hints_count": len(hints),
            "previous_hints_count": previous_hints_count,
            "requires_justification": True,
            "metadata": {
                "cognitive_state": cognitive_state,
                "provides_code": False,
                "provides_pseudocode": help_level in [HelpLevel.MEDIO, HelpLevel.ALTO],
                "adaptive_level": help_level.value,
            }
        }

    def _determine_adaptive_help_level(
        self,
        student_history: Optional[List[CognitiveTrace]],
        strategy: Dict[str, Any]
    ) -> HelpLevel:
        """
        Determina el nivel de ayuda adaptativamente segÃºn:
        1. Estrategia sugerida por CRPE
        2. Historial de pistas recibidas (si recibiÃ³ muchas, reducir detalle)
        3. Nivel de AI involvement acumulado
        """
        # Nivel base desde estrategia
        strategy_level = strategy.get("help_level", HelpLevel.MEDIO)

        if not student_history:
            return strategy_level

        # Contar pistas previas
        hints_received = self._count_previous_hints(student_history)

        # Si ya recibiÃ³ muchas pistas (>5), reducir nivel para fomentar autonomÃ­a
        if hints_received > 5:
            if strategy_level == HelpLevel.ALTO:
                return HelpLevel.MEDIO
            elif strategy_level == HelpLevel.MEDIO:
                return HelpLevel.BAJO

        # Calcular AI involvement promedio
        avg_ai_involvement = sum(t.ai_involvement for t in student_history) / len(student_history)

        # Si dependency alta (>0.6), reducir nivel de ayuda
        if avg_ai_involvement > 0.6:
            if strategy_level == HelpLevel.ALTO:
                return HelpLevel.MEDIO
            elif strategy_level == HelpLevel.MEDIO:
                return HelpLevel.BAJO

        return strategy_level

    def _count_previous_hints(self, student_history: List[CognitiveTrace]) -> int:
        """Cuenta cuÃ¡ntas pistas ha recibido el estudiante"""
        return sum(
            1 for t in student_history
            if "hints_provided" in t.metadata.get("response_metadata", {})
        )

    def _generate_level1_hints(self, prompt: str, cognitive_state: str) -> List[Dict[str, str]]:
        """Nivel 1 - MINIMO: Solo preguntas socrÃ¡ticas orientadoras"""
        return [
            {
                "level": 1,
                "type": "question",
                "content": "Â¿QuÃ© pasos creÃ©s que son necesarios para resolver este problema?"
            },
            {
                "level": 1,
                "type": "question",
                "content": "Â¿QuÃ© conceptos o estructuras de datos podrÃ­an ser relevantes aquÃ­?"
            },
            {
                "level": 1,
                "type": "question",
                "content": "Â¿PodÃ©s describir con tus palabras cÃ³mo funcionarÃ­a una soluciÃ³n ideal?"
            }
        ]

    def _generate_level2_hints(self, prompt: str, cognitive_state: str) -> List[Dict[str, str]]:
        """Nivel 2 - BAJO: Pistas conceptuales generales"""
        return [
            {
                "level": 2,
                "type": "conceptual",
                "content": "PensÃ¡ en descomponer el problema en partes mÃ¡s pequeÃ±as. Â¿CuÃ¡les serÃ­an esas partes?"
            },
            {
                "level": 2,
                "type": "conceptual",
                "content": "ConsiderÃ¡ quÃ© estructura de datos se adapta mejor a las operaciones que necesitÃ¡s realizar."
            },
            {
                "level": 2,
                "type": "reflection",
                "content": "Â¿QuÃ© casos especiales o de borde deberÃ­as tener en cuenta?"
            }
        ]

    def _generate_level3_hints(self, prompt: str, cognitive_state: str) -> List[Dict[str, str]]:
        """Nivel 3 - MEDIO: Pistas con detalle + pseudocÃ³digo alto nivel"""
        return [
            {
                "level": 3,
                "type": "decomposition",
                "content": "DividÃ­ el problema en estas etapas: 1) InicializaciÃ³n, 2) OperaciÃ³n principal, 3) ValidaciÃ³n"
            },
            {
                "level": 3,
                "type": "strategy",
                "content": "Una estrategia comÃºn es usar [concepto general] para gestionar [aspecto del problema]"
            },
            {
                "level": 3,
                "type": "pseudocode",
                "content": """```
// Estructura general (alto nivel)
funciÃ³n resolver():
    // Paso 1: Preparar datos
    // Paso 2: Procesar elemento por elemento
    // Paso 3: Retornar resultado
```"""
            }
        ]

    def _generate_level4_hints(self, prompt: str, cognitive_state: str) -> List[Dict[str, str]]:
        """Nivel 4 - ALTO: Fragmentos conceptuales + estrategia detallada"""
        return [
            {
                "level": 4,
                "type": "detailed_strategy",
                "content": "ConsiderÃ¡ este enfoque: [descripciÃ³n detallada de estrategia sin cÃ³digo especÃ­fico]"
            },
            {
                "level": 4,
                "type": "pattern",
                "content": "Un patrÃ³n Ãºtil aquÃ­ es [nombre del patrÃ³n], que consiste en [explicaciÃ³n conceptual]"
            },
            {
                "level": 4,
                "type": "conceptual_fragment",
                "content": """Para gestionar [aspecto especÃ­fico]:
- OpciÃ³n A: [ventajas y desventajas]
- OpciÃ³n B: [ventajas y desventajas]
Â¿CuÃ¡l elegirÃ­as y por quÃ©?"""
            }
        ]

    def _format_hints_message(self, hints: List[Dict[str, str]], level: HelpLevel) -> str:
        """Formatea las pistas para el mensaje"""
        icons = {
            "question": "â“",
            "conceptual": "ðŸ’¡",
            "reflection": "ðŸ¤”",
            "decomposition": "ðŸ”",
            "strategy": "ðŸŽ¯",
            "pseudocode": "ðŸ“",
            "detailed_strategy": "ðŸ—ºï¸",
            "pattern": "ðŸ§©",
            "conceptual_fragment": "ðŸ’­"
        }

        formatted = []
        for i, hint in enumerate(hints, 1):
            icon = icons.get(hint["type"], "â€¢")
            hint_type = hint["type"].replace("_", " ").title()
            formatted.append(f"### {icon} Pista {i}: {hint_type}\n{hint['content']}")

        return "\n\n".join(formatted)

    def _generate_followup_question(self, level: HelpLevel, hints_count: int) -> str:
        """Genera pregunta de seguimiento segÃºn contexto"""
        if hints_count > 5:
            return """âš ï¸ **Nota**: Has recibido varias pistas ya. Es momento de que intentes
avanzar de forma mÃ¡s autÃ³noma. Â¿QuÃ© vas a hacer con la informaciÃ³n que tenÃ©s?"""
        elif level == HelpLevel.MINIMO:
            return """â“ **Pregunta para vos**: RespondÃ© primero estas preguntas antes de
solicitar mÃ¡s ayuda. La clave estÃ¡ en tu razonamiento, no en la respuesta de la IA."""
        elif level in [HelpLevel.MEDIO, HelpLevel.ALTO]:
            return """â“ **Pregunta para vos**: BasÃ¡ndote en estas pistas, Â¿cuÃ¡l serÃ­a tu
prÃ³ximo paso concreto? Â¿QuÃ© decisiÃ³n de diseÃ±o tomarÃ­as y **por quÃ©**?"""
        else:
            return """â“ **PrÃ³ximo paso**: IntentÃ¡ formular un plan basÃ¡ndote en estas pistas.
Â¿QuÃ© harÃ­as primero?"""

    def _generate_clarification_request(
        self,
        prompt: str,
        cognitive_state: str
    ) -> Dict[str, Any]:
        """Solicita clarificaciÃ³n cuando el prompt es ambiguo"""
        message = """
## Necesito MÃ¡s InformaciÃ³n

Para poder ayudarte de manera efectiva, necesito que seas mÃ¡s especÃ­fico:

### ðŸ“Œ Contexto del Problema
- Â¿QuÃ© parte exacta te genera dificultad?
- Â¿QuÃ© entendÃ©s que tenÃ©s que lograr?

### ðŸ“Œ Lo que Intentaste
- Â¿QuÃ© enfoque probaste?
- Â¿QuÃ© cÃ³digo escribiste hasta ahora?
- Â¿QuÃ© resultado obtuviste vs. quÃ© esperabas?

### ðŸ“Œ Tu HipÃ³tesis
- Â¿QuÃ© creÃ©s que podrÃ­a estar causando el problema?
- Â¿QuÃ© soluciones consideraste?

Por favor, reformulÃ¡ tu consulta incluyendo esta informaciÃ³n.
"""

        return {
            "message": message.strip(),
            "mode": TutorMode.SOCRATICO,
            "pedagogical_intent": "promote_specificity",
            "requires_student_response": True,
            "metadata": {
                "cognitive_state": cognitive_state,
            }
        }

    def evaluate_student_response(
        self,
        student_response: str,
        previous_interaction: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        EvalÃºa la respuesta del estudiante a preguntas/pistas previas

        Detecta:
        - Nivel de elaboraciÃ³n
        - ExplicitaciÃ³n del razonamiento
        - JustificaciÃ³n de decisiones
        - AutocorrecciÃ³n
        """
        analysis = {
            "has_justification": self._detect_justification(student_response),
            "shows_decomposition": self._detect_decomposition(student_response),
            "shows_planning": self._detect_planning(student_response),
            "shows_self_reflection": self._detect_self_reflection(student_response),
            "quality_score": 0.0,  # 0-1
        }

        # Calcular score de calidad
        score = 0.0
        if analysis["has_justification"]:
            score += 0.3
        if analysis["shows_decomposition"]:
            score += 0.3
        if analysis["shows_planning"]:
            score += 0.2
        if analysis["shows_self_reflection"]:
            score += 0.2

        analysis["quality_score"] = score

        return analysis

    def _detect_justification(self, text: str) -> bool:
        """Detecta si hay justificaciÃ³n en la respuesta"""
        justification_signals = [
            "porque", "ya que", "debido a", "considerando que",
            "mi razÃ³n es", "pensÃ© que", "decidÃ­", "elegÃ­"
        ]
        return any(signal in text.lower() for signal in justification_signals)

    def _detect_decomposition(self, text: str) -> bool:
        """Detecta si hay descomposiciÃ³n del problema"""
        decomposition_signals = [
            "primero", "luego", "despuÃ©s", "paso", "parte",
            "dividir", "separar", "componente", "subproblema"
        ]
        return any(signal in text.lower() for signal in decomposition_signals)

    def _detect_planning(self, text: str) -> bool:
        """Detecta si hay evidencia de planificaciÃ³n"""
        planning_signals = [
            "voy a", "planeo", "mi estrategia", "mi plan",
            "primero harÃ©", "mi enfoque", "mi idea es"
        ]
        return any(signal in text.lower() for signal in planning_signals)

    def _detect_self_reflection(self, text: str) -> bool:
        """Detecta si hay reflexiÃ³n metacognitiva"""
        reflection_signals = [
            "me doy cuenta", "entiendo que", "ahora veo",
            "me confundÃ­", "cometÃ­ el error", "deberÃ­a"
        ]
        return any(signal in text.lower() for signal in reflection_signals)