"""
Submodelo 2: Evaluador IA de Procesos Cognitivos (E-IA-Proc)

Analiza, reconstruye y eval√∫a el proceso cognitivo h√≠brido humano-IA
que condujo a una soluci√≥n t√©cnica.

Cortez93: Refactored to use LLMGenerationMixin for DRY LLM handling.
"""
from typing import Optional, Dict, Any, List
from datetime import datetime
import logging

# Asumo que estos imports existen en tu estructura de proyecto
from ..models.trace import CognitiveTrace, TraceSequence
from ..models.evaluation import (
    EvaluationReport,
    EvaluationDimension,
    ReasoningAnalysis,
    GitAnalysis,
    CompetencyLevel,
    CognitivePhase,
)
from .base_agent import LLMGenerationMixin, AgentResponseBuilder, AgentConfig
from ..llm.base import LLMMessage, LLMRole

logger = logging.getLogger(__name__)


class EvaluadorProcesosAgent(LLMGenerationMixin):
    """
    E-IA-Proc: Evaluador de Procesos Cognitivos

    Funciones:
    1. An√°lisis de razonamiento (camino cognitivo)
    2. Detecci√≥n de errores conceptuales y epistemol√≥gicos
    3. Evaluaci√≥n de autorregulaci√≥n (Zimmerman, 2002)
    4. Comparaci√≥n y coherencia evolutiva v√≠a Git
    5. Generaci√≥n del Informe de Evaluaci√≥n Cognitiva (IEC)

    NO califica ni aprueba - solo analiza y genera evidencia
    """

    def __init__(self, llm_provider=None, config: Optional[AgentConfig] = None):
        """
        Initialize the process evaluator agent.

        Args:
            llm_provider: LLM provider for deep analysis
            config: Agent configuration (AgentConfig or dict for backward compatibility)

        Cortez93: Now uses AgentConfig typed dataclass for configuration.
        """
        self.llm_provider = llm_provider
        # Support both AgentConfig and dict for backward compatibility
        if isinstance(config, AgentConfig):
            self._config = config
        else:
            self._config = AgentConfig.from_dict(config)

    async def evaluate_process_async(
        self,
        trace_sequence: TraceSequence,
        code_evolution: Optional[List[Dict[str, Any]]] = None
    ) -> EvaluationReport:
        """
        Eval√∫a el proceso cognitivo completo de una actividad (versi√≥n async con LLM)

        Args:
            trace_sequence: Secuencia de trazas N4
            code_evolution: Evoluci√≥n del c√≥digo (commits Git)

        Returns:
            EvaluationReport con an√°lisis completo (usa Gemini Pro si est√° disponible)
        """
        # 1. An√°lisis del razonamiento (con Gemini Pro si est√° disponible)
        if self.llm_provider:
            reasoning = await self._analyze_reasoning_deep(trace_sequence)
        else:
            reasoning = self._analyze_reasoning(trace_sequence)

        # 2. An√°lisis Git (si disponible)
        git_analysis = None
        if code_evolution:
            git_analysis = self._analyze_git_evolution(code_evolution, trace_sequence)

        # 3. Calcular dependencia de IA
        ai_dependency = self._calculate_ai_dependency(trace_sequence)

        # 4. Identificar riesgos cognitivos
        cognitive_risks = self._identify_cognitive_risks(trace_sequence, reasoning)

        # 5. Evaluar dimensiones
        dimensions = self._evaluate_dimensions(trace_sequence, reasoning)

        # 6. Evaluaci√≥n general
        overall_level, overall_score = self._compute_overall_evaluation(dimensions)

        # 7. Generar recomendaciones
        strengths, improvements = self._identify_strengths_and_improvements(
            dimensions, reasoning
        )
        rec_student, rec_teacher = self._generate_recommendations(
            dimensions, reasoning, cognitive_risks
        )

        # Crear reporte
        report = EvaluationReport(
            id=f"eval_{trace_sequence.id}",
            session_id=trace_sequence.session_id,
            timestamp=datetime.now(),
            student_id=trace_sequence.student_id,
            activity_id=trace_sequence.activity_id,
            reasoning_analysis=reasoning,
            git_analysis=git_analysis,
            dimensions=dimensions,
            ai_dependency_score=ai_dependency,
            ai_usage_patterns=self._analyze_ai_usage_patterns(trace_sequence),
            reasoning_map=self._build_reasoning_map(trace_sequence),
            cognitive_risks=cognitive_risks,
            overall_competency_level=overall_level,
            overall_score=overall_score,
            key_strengths=strengths,
            improvement_areas=improvements,
            recommendations_student=rec_student,
            recommendations_teacher=rec_teacher,
            trace_sequences_analyzed=1
        )

        return report

    def evaluate_process(
        self,
        trace_sequence: TraceSequence,
        code_evolution: Optional[List[Dict[str, Any]]] = None
    ) -> EvaluationReport:
        """
        Eval√∫a el proceso cognitivo completo de una actividad (versi√≥n s√≠ncrona)

        Args:
            trace_sequence: Secuencia de trazas N4
            code_evolution: Evoluci√≥n del c√≥digo (commits Git)

        Returns:
            EvaluationReport con an√°lisis completo
            
        Note:
            Si hay llm_provider, considera usar evaluate_process_async para an√°lisis profundo
        """
        # 1. An√°lisis del razonamiento (heur√≠stico)
        reasoning = self._analyze_reasoning(trace_sequence)

        # 2. An√°lisis Git (si disponible)
        git_analysis = None
        if code_evolution:
            git_analysis = self._analyze_git_evolution(code_evolution, trace_sequence)

        # 3. Calcular dependencia de IA
        ai_dependency = self._calculate_ai_dependency(trace_sequence)

        # 4. Identificar riesgos cognitivos
        cognitive_risks = self._identify_cognitive_risks(trace_sequence, reasoning)

        # 5. Evaluar dimensiones
        dimensions = self._evaluate_dimensions(trace_sequence, reasoning)

        # 6. Evaluaci√≥n general
        overall_level, overall_score = self._compute_overall_evaluation(dimensions)

        # 7. Generar recomendaciones
        strengths, improvements = self._identify_strengths_and_improvements(
            dimensions, reasoning
        )
        rec_student, rec_teacher = self._generate_recommendations(
            dimensions, reasoning, cognitive_risks
        )

        # Crear reporte
        report = EvaluationReport(
            id=f"eval_{trace_sequence.id}",
            session_id=trace_sequence.session_id,
            timestamp=datetime.now(),
            student_id=trace_sequence.student_id,
            activity_id=trace_sequence.activity_id,
            reasoning_analysis=reasoning,
            git_analysis=git_analysis,
            dimensions=dimensions,
            ai_dependency_score=ai_dependency,
            ai_usage_patterns=self._analyze_ai_usage_patterns(trace_sequence),
            reasoning_map=self._build_reasoning_map(trace_sequence),
            cognitive_risks=cognitive_risks,
            overall_competency_level=overall_level,
            overall_score=overall_score,
            key_strengths=strengths,
            improvement_areas=improvements,
            recommendations_student=rec_student,
            recommendations_teacher=rec_teacher,
            trace_sequences_analyzed=1
        )

        return report

    def _analyze_reasoning(self, trace_sequence: TraceSequence) -> ReasoningAnalysis:
        """Analiza el camino de razonamiento"""
        traces = trace_sequence.traces

        # Reconstruir camino cognitivo
        cognitive_path = trace_sequence.get_cognitive_path()

        # Identificar fases completadas
        phases_completed = self._identify_phases(traces)

        # Contar eventos clave
        strategy_changes = sum(
            1 for t in traces
            if t.interaction_type.value == "strategy_change"
        )
        self_corrections = sum(
            1 for t in traces
            if t.interaction_type.value == "self_correction"
        )
        ai_critiques = sum(
            1 for t in traces
            if t.interaction_type.value == "ai_critique"
        )

        # Analizar coherencia (con LLM si est√° disponible)
        coherence_score = self._calculate_coherence(traces)

        # Detectar errores (con an√°lisis profundo si hay LLM)
        conceptual_errors = self._detect_conceptual_errors(traces)
        logical_fallacies = self._detect_logical_fallacies(traces)

        # Analizar autorregulaci√≥n
        planning_quality = self._assess_planning_quality(traces)
        monitoring_evidence = self._extract_monitoring_evidence(traces)
        self_explanation_quality = self._assess_self_explanation(traces)

        return ReasoningAnalysis(
            cognitive_path=cognitive_path,
            phases_completed=phases_completed,
            strategy_changes=strategy_changes,
            self_corrections=self_corrections,
            ai_critiques=ai_critiques,
            coherence_score=coherence_score,
            conceptual_errors=conceptual_errors,
            logical_fallacies=logical_fallacies,
            planning_quality=planning_quality,
            monitoring_evidence=monitoring_evidence,
            self_explanation_quality=self_explanation_quality
        )

    async def _analyze_reasoning_deep(self, trace_sequence: TraceSequence) -> ReasoningAnalysis:
        """
        An√°lisis profundo de razonamiento usando Gemini Pro

        Cortez93: Refactored to use LLMGenerationMixin._generate_with_timeout()
        for consistent timeout handling and logging.

        Este m√©todo usa LLM para an√°lisis cognitivo m√°s sofisticado.
        Si no hay LLM disponible, usa _analyze_reasoning (heur√≠stico).
        """
        if not self.llm_provider:
            # Fallback a an√°lisis heur√≠stico
            return self._analyze_reasoning(trace_sequence)

        try:
            # Construir resumen de las trazas
            traces_summary = self._build_traces_summary(trace_sequence.traces)

            system_prompt = """Eres un experto en an√°lisis cognitivo y evaluaci√≥n de procesos de aprendizaje.

Analiza el proceso de razonamiento del estudiante bas√°ndote en sus trazas cognitivas.

Eval√∫a:
1. Coherencia del razonamiento
2. Errores conceptuales (malentendidos fundamentales)
3. Falacias l√≥gicas (errores en el razonamiento)
4. Calidad de planificaci√≥n (0.0-1.0)
5. Evidencia de monitoreo/autorregulaci√≥n
6. Calidad de auto-explicaci√≥n (0.0-1.0)

Responde en formato JSON:
{
  "coherence_score": 0.0-1.0,
  "conceptual_errors": ["error1", "error2", ...],
  "logical_fallacies": ["falacia1", "falacia2", ...],
  "planning_quality": 0.0-1.0,
  "monitoring_evidence": ["evidencia1", "evidencia2", ...],
  "self_explanation_quality": 0.0-1.0,
  "summary": "Resumen del an√°lisis en 2-3 oraciones"
}"""

            # Cortez93: Use _build_conversation_messages from LLMGenerationMixin
            messages = self._build_conversation_messages(
                system_prompt=system_prompt,
                user_input=f"""Proceso cognitivo del estudiante:

{traces_summary}

Analiza este proceso de razonamiento."""
            )

            # Cortez93: Use _generate_with_timeout from LLMGenerationMixin
            response = await self._generate_with_timeout(
                messages,
                temperature=0.3,  # Baja temperatura para an√°lisis consistente
                max_tokens=800,
                is_code_analysis=True,  # FORZAR Pro model para an√°lisis profundo
                context_label="evaluator_deep_analysis"
            )

            if response is None:
                # Timeout or error - fallback to heuristic
                return self._analyze_reasoning(trace_sequence)

            # Cortez88: Usar extraccion JSON robusta en lugar de regex fragil
            from ..utils.json_extraction import extract_json_from_text
            analysis_data = extract_json_from_text(response.content)
            if analysis_data:
                
                # Reconstruir ReasoningAnalysis con datos del LLM
                traces = trace_sequence.traces
                cognitive_path = trace_sequence.get_cognitive_path()
                phases_completed = self._identify_phases(traces)
                
                strategy_changes = sum(
                    1 for t in traces
                    if t.interaction_type.value == "strategy_change"
                )
                self_corrections = sum(
                    1 for t in traces
                    if t.interaction_type.value == "self_correction"
                )
                ai_critiques = sum(
                    1 for t in traces
                    if t.interaction_type.value == "ai_critique"
                )
                
                return ReasoningAnalysis(
                    cognitive_path=cognitive_path,
                    phases_completed=phases_completed,
                    strategy_changes=strategy_changes,
                    self_corrections=self_corrections,
                    ai_critiques=ai_critiques,
                    coherence_score=analysis_data.get("coherence_score", 0.5),
                    conceptual_errors=analysis_data.get("conceptual_errors", []),
                    logical_fallacies=analysis_data.get("logical_fallacies", []),
                    planning_quality=analysis_data.get("planning_quality", 0.5),
                    monitoring_evidence=analysis_data.get("monitoring_evidence", []),
                    self_explanation_quality=analysis_data.get("self_explanation_quality", 0.5)
                )
            else:
                # Si no puede parsear, usar heur√≠stico
                return self._analyze_reasoning(trace_sequence)
                
        except Exception as e:
            # FIX Cortez36: Use lazy logging formatting
            logger.warning("Deep reasoning analysis failed, using heuristic: %s", e)
            return self._analyze_reasoning(trace_sequence)

    def _build_traces_summary(self, traces: List[CognitiveTrace]) -> str:
        """Construye un resumen de las trazas para an√°lisis LLM"""
        summary_parts = []
        for i, trace in enumerate(traces[:20], 1):  # Limitar a 20 trazas
            summary_parts.append(
                f"{i}. [{trace.interaction_type.value}] {trace.content[:200]}"
            )
        
        if len(traces) > 20:
            summary_parts.append(f"... y {len(traces) - 20} trazas m√°s")
        
        return "\n".join(summary_parts)

    def _identify_phases(self, traces: List[CognitiveTrace]) -> List[CognitivePhase]:
        """Identifica qu√© fases cognitivas se completaron"""
        phases_keywords = {
            CognitivePhase.PLANIFICACION: ["plan", "estrategia", "voy a", "primero"],
            CognitivePhase.EXPLORACION: ["entiendo", "qu√© es", "c√≥mo funciona"],
            CognitivePhase.IMPLEMENTACION: ["implemento", "c√≥digo", "funci√≥n"],
            CognitivePhase.DEPURACION: ["error", "bug", "falla", "no funciona"],
            CognitivePhase.VALIDACION: ["prueba", "test", "verifica", "funciona"],
            CognitivePhase.REFLEXION: ["me doy cuenta", "entiendo que", "aprend√≠"]
        }

        phases_found = []
        for phase, keywords in phases_keywords.items():
            for trace in traces:
                if any(kw in trace.content.lower() for kw in keywords):
                    if phase not in phases_found:
                        phases_found.append(phase)
                    break

        return phases_found

    def _calculate_coherence(self, traces: List[CognitiveTrace]) -> float:
        """Calcula coherencia entre decisiones y justificaciones"""
        # Simplificado para MVP
        decisions = [t for t in traces if t.decision_justification]
        if not decisions:
            return 0.5  # Neutro si no hay decisiones justificadas

        # Si hay justificaciones, es buena se√±al
        justification_ratio = len(decisions) / len(traces) if traces else 0
        return min(1.0, justification_ratio * 2)

    def _detect_conceptual_errors(self, traces: List[CognitiveTrace]) -> List[str]:
        """Detecta errores conceptuales en el razonamiento"""
        # Placeholder - en producci√≥n usar√≠a an√°lisis sem√°ntico
        errors = []
        for trace in traces:
            content_lower = trace.content.lower()
            if "confundo" in content_lower or "cre√≠a que" in content_lower:
                errors.append(f"Posible confusi√≥n conceptual en: {trace.content[:100]}...")
        return errors

    def _detect_logical_fallacies(self, traces: List[CognitiveTrace]) -> List[str]:
        """Detecta falacias l√≥gicas"""
        # Placeholder
        return []

    def _assess_planning_quality(self, traces: List[CognitiveTrace]) -> float:
        """Eval√∫a calidad de planificaci√≥n"""
        planning_traces = [
            t for t in traces
            if any(kw in t.content.lower() for kw in ["plan", "voy a", "primero", "estrategia"])
        ]
        # M√°s trazas de planificaci√≥n = mejor planificaci√≥n
        return min(1.0, len(planning_traces) / max(len(traces) * 0.2, 1))

    def _extract_monitoring_evidence(self, traces: List[CognitiveTrace]) -> List[str]:
        """Extrae evidencias de monitoreo del proceso"""
        monitoring_keywords = ["reviso", "verifico", "chequeo", "me doy cuenta"]
        evidence = []
        for trace in traces:
            if any(kw in trace.content.lower() for kw in monitoring_keywords):
                evidence.append(trace.content[:100])
        return evidence

    def _assess_self_explanation(self, traces: List[CognitiveTrace]) -> float:
        """Eval√∫a calidad de autoexplicaci√≥n"""
        explanations = [
            t for t in traces
            if any(kw in t.content.lower() for kw in ["porque", "ya que", "entiendo que"])
        ]
        return min(1.0, len(explanations) / max(len(traces) * 0.3, 1))

    def _analyze_git_evolution(
        self,
        code_evolution: List[Dict[str, Any]],
        trace_sequence: TraceSequence
    ) -> GitAnalysis:
        """Analiza la evoluci√≥n del c√≥digo v√≠a Git"""
        # Placeholder para MVP
        return GitAnalysis(
            total_commits=len(code_evolution),
            commit_messages_quality=0.7,
            suspicious_jumps=[],
            evolution_coherence=0.8,
            traces_linked=0
        )

    def _calculate_ai_dependency(self, trace_sequence: TraceSequence) -> float:
        """Calcula nivel de dependencia de IA"""
        return trace_sequence.ai_dependency_score

    def _identify_cognitive_risks(
        self,
        trace_sequence: TraceSequence,
        reasoning: ReasoningAnalysis
    ) -> List[str]:
        """Identifica riesgos cognitivos"""
        risks = []

        if reasoning.strategy_changes == 0:
            risks.append("No se observan cambios de estrategia (posible inflexibilidad)")

        if reasoning.self_corrections == 0:
            risks.append("No se observan autocorrecciones (falta de revisi√≥n cr√≠tica)")

        if reasoning.planning_quality < 0.3:
            risks.append("Planificaci√≥n insuficiente")

        if reasoning.coherence_score < 0.5:
            risks.append("Baja coherencia entre decisiones y justificaciones")

        return risks

    def _evaluate_dimensions(
        self,
        trace_sequence: TraceSequence,
        reasoning: ReasoningAnalysis
    ) -> List[EvaluationDimension]:
        """Eval√∫a dimensiones espec√≠ficas de competencia"""
        dimensions = []

        # Dimensi√≥n: Descomposici√≥n de problemas
        dimensions.append(EvaluationDimension(
            name="Descomposici√≥n de Problemas",
            description="Capacidad de dividir problemas complejos en subproblemas manejables",
            level=self._score_to_level(reasoning.planning_quality * 10),
            score=reasoning.planning_quality * 10,
            evidence=[],
            strengths=[] if reasoning.planning_quality < 0.5 else ["Buena planificaci√≥n inicial"],
            weaknesses=["Planificaci√≥n insuficiente"] if reasoning.planning_quality < 0.5 else [],
            recommendations=[]
        ))

        # Dimensi√≥n: Autorregulaci√≥n
        dimensions.append(EvaluationDimension(
            name="Autorregulaci√≥n y Metacognici√≥n",
            description="Capacidad de monitorear y ajustar el propio proceso de aprendizaje",
            level=self._score_to_level(reasoning.self_explanation_quality * 10),
            score=reasoning.self_explanation_quality * 10,
            evidence=reasoning.monitoring_evidence,
            strengths=[],
            weaknesses=[],
            recommendations=[]
        ))

        # Dimensi√≥n: Coherencia l√≥gica
        dimensions.append(EvaluationDimension(
            name="Coherencia L√≥gica",
            description="Coherencia entre razonamiento, decisiones y justificaciones",
            level=self._score_to_level(reasoning.coherence_score * 10),
            score=reasoning.coherence_score * 10,
            evidence=[],
            strengths=[],
            weaknesses=[],
            recommendations=[]
        ))

        return dimensions

    def _score_to_level(self, score: float) -> CompetencyLevel:
        """Convierte score num√©rico a nivel de competencia"""
        if score >= 8.5:
            return CompetencyLevel.EXPERTO
        elif score >= 7.0:
            return CompetencyLevel.AUTONOMO
        elif score >= 5.0:
            return CompetencyLevel.EN_DESARROLLO
        else:
            return CompetencyLevel.INICIAL

    def _compute_overall_evaluation(
        self,
        dimensions: List[EvaluationDimension]
    ) -> tuple[CompetencyLevel, float]:
        """Calcula evaluaci√≥n general"""
        if not dimensions:
            return CompetencyLevel.INICIAL, 0.0

        avg_score = sum(d.score for d in dimensions) / len(dimensions)
        overall_level = self._score_to_level(avg_score)

        return overall_level, avg_score

    def _identify_strengths_and_improvements(
        self,
        dimensions: List[EvaluationDimension],
        reasoning: ReasoningAnalysis
    ) -> tuple[List[str], List[str]]:
        """Identifica fortalezas y √°reas de mejora"""
        strengths = []
        improvements = []

        for dim in dimensions:
            if dim.score >= 7.0:
                strengths.extend(dim.strengths)
            elif dim.score < 6.0:
                improvements.append(f"Mejorar: {dim.name}")

        if reasoning.self_corrections > 0:
            strengths.append("Capacidad de autocorrecci√≥n")

        if reasoning.strategy_changes > 2:
            strengths.append("Flexibilidad cognitiva (cambios de estrategia)")

        return strengths, improvements

    def _generate_recommendations(
        self,
        dimensions: List[EvaluationDimension],
        reasoning: ReasoningAnalysis,
        risks: List[str]
    ) -> tuple[List[str], List[str]]:
        """Genera recomendaciones para estudiante y docente"""
        rec_student = []
        rec_teacher = []

        # Recomendaciones seg√∫n riesgos
        if "Planificaci√≥n insuficiente" in risks:
            rec_student.append(
                "Dedic√° m√°s tiempo a planificar antes de implementar. "
                "Escrib√≠ en papel o comentarios tu estrategia."
            )
            rec_teacher.append(
                "Solicitar planificaci√≥n expl√≠cita antes de codificar"
            )

        if reasoning.self_corrections == 0:
            rec_student.append(
                "Practic√° la revisi√≥n cr√≠tica: cuestion√° tus propias soluciones"
            )

        # Recomendaciones generales
        rec_teacher.append("Revisar proceso completo documentado en trazabilidad N4")

        return rec_student, rec_teacher

    def _analyze_ai_usage_patterns(self, trace_sequence: TraceSequence) -> Dict[str, Any]:
        """Analiza patrones de uso de IA"""
        traces = trace_sequence.traces
        ai_interactions = [t for t in traces if t.interaction_type.value == "ai_response"]

        return {
            "total_ai_interactions": len(ai_interactions),
            "ai_interaction_rate": len(ai_interactions) / len(traces) if traces else 0,
            "delegation_attempts": 0,  # Placeholder
        }

    def _build_reasoning_map(self, trace_sequence: TraceSequence) -> Dict[str, Any]:
        """Construye mapa visual del razonamiento h√≠brido"""
        return {
            "nodes": [
                {
                    "id": t.id,
                    "type": t.interaction_type.value,
                    "content": t.content[:50],
                    "timestamp": str(t.timestamp)
                }
                for t in trace_sequence.traces
            ],
            "cognitive_path": trace_sequence.get_cognitive_path()
        }

    def generate_formative_feedback(
        self,
        evaluation_report: EvaluationReport,
        student_friendly: bool = True
    ) -> str:
        """
        Genera retroalimentaci√≥n formativa al final de sesi√≥n (HU-EST-007)

        Args:
            evaluation_report: Reporte de evaluaci√≥n completo
            student_friendly: Si es True, usa lenguaje amigable para estudiantes

        Returns:
            String con retroalimentaci√≥n formativa en markdown
        """
        if student_friendly:
            return self._generate_student_feedback(evaluation_report)
        else:
            return self._generate_teacher_feedback(evaluation_report)

    def _generate_student_feedback(self, report: EvaluationReport) -> str:
        """
        Genera retroalimentaci√≥n formativa para el estudiante

        ‚úÖ REFACTORED (2025-11-22): Uso de list.join() en lugar de += (H1)
        Mejora performance 3-5x en strings largos (>1KB)
        """

        # ‚úÖ REFACTORED: Construcci√≥n con lista + join (H1)
        parts = []

        # Header
        parts.append(f"""
# üìä Retroalimentaci√≥n de tu Proceso de Aprendizaje

**Actividad**: {report.activity_id}
**Fecha**: {report.timestamp.strftime("%d/%m/%Y %H:%M")}

---

## üéØ Evaluaci√≥n General

**Nivel de Competencia**: {report.overall_competency_level.value.upper()}
**Puntaje**: {report.overall_score:.1f}/10

""")

        # Interpretaci√≥n del nivel
        level_explanations = {
            "INICIAL": "Est√°s comenzando a desarrollar estas competencias. ¬°Segu√≠ practicando!",
            "EN_DESARROLLO": "Vas por buen camino. Con m√°s pr√°ctica alcanzar√°s mayor autonom√≠a.",
            "AUTONOMO": "Ten√©s un buen dominio de estas competencias. Pod√©s trabajar de forma independiente.",
            "EXPERTO": "¬°Excelente! Demostr√°s dominio experto de estas competencias."
        }

        parts.append(f"**¬øQu√© significa esto?** {level_explanations.get(report.overall_competency_level.value.upper(), '')}\n\n")
        parts.append("---\n\n")

        # Fortalezas
        if report.key_strengths:
            parts.append("## ‚úÖ Tus Fortalezas\n\n")
            for strength in report.key_strengths:
                parts.append(f"- {strength}\n")
            parts.append("\n---\n\n")

        # √Åreas de mejora
        if report.improvement_areas:
            parts.append("## üéì √Åreas para Mejorar\n\n")
            for area in report.improvement_areas:
                parts.append(f"- {area}\n")
            parts.append("\n---\n\n")

        # An√°lisis del proceso
        parts.append("## üîç An√°lisis de tu Proceso\n\n")

        reasoning = report.reasoning_analysis

        # Fases completadas
        if reasoning.phases_completed:
            parts.append("### Fases que Completaste\n\n")
            phase_names = {
                "PLANIFICACION": "üìã Planificaci√≥n",
                "EXPLORACION": "üîç Exploraci√≥n Conceptual",
                "IMPLEMENTACION": "üíª Implementaci√≥n",
                "DEPURACION": "üêõ Depuraci√≥n",
                "VALIDACION": "‚úÖ Validaci√≥n",
                "REFLEXION": "üí≠ Reflexi√≥n"
            }
            for phase in reasoning.phases_completed:
                phase_str = phase.value if hasattr(phase, 'value') else str(phase)
                parts.append(f"- {phase_names.get(phase_str.upper(), phase_str)}\n")
            parts.append("\n")

        # Eventos clave
        parts.append("### Eventos Clave\n\n")
        parts.append(f"- **Cambios de estrategia**: {reasoning.strategy_changes} {'‚úÖ' if reasoning.strategy_changes > 0 else '‚ö†Ô∏è'}\n")
        parts.append(f"- **Autocorrecciones**: {reasoning.self_corrections} {'‚úÖ' if reasoning.self_corrections > 0 else '‚ö†Ô∏è'}\n")
        parts.append(f"- **Revisi√≥n cr√≠tica de IA**: {reasoning.ai_critiques} {'‚úÖ' if reasoning.ai_critiques > 0 else '‚ö†Ô∏è'}\n\n")

        # Interpretaci√≥n
        if reasoning.strategy_changes == 0:
            parts.append("üí° **Tip**: No observamos cambios de estrategia. Si algo no funciona, prob√° con otro enfoque.\n\n")

        if reasoning.self_corrections == 0:
            parts.append("üí° **Tip**: No encontramos evidencia de revisi√≥n de tu propio c√≥digo. ¬°Es fundamental revisar cr√≠ticamente tu trabajo!\n\n")

        parts.append("---\n\n")

        # Uso de IA
        parts.append(f"## ü§ñ Tu Colaboraci√≥n con IA\n\n")
        parts.append(f"**Nivel de asistencia de IA**: {report.ai_dependency_score:.0%}\n\n")

        if report.ai_dependency_score < 0.3:
            parts.append("‚úÖ Trabajaste de forma muy aut√≥noma. ¬°Excelente!\n\n")
        elif report.ai_dependency_score < 0.6:
            parts.append("‚úÖ Balance saludable entre tu trabajo y la asistencia de IA.\n\n")
        elif report.ai_dependency_score < 0.8:
            parts.append("‚ö†Ô∏è Dependencia moderada-alta de IA. Intent√° resolver m√°s por tu cuenta antes de pedir ayuda.\n\n")
        else:
            parts.append("‚ö†Ô∏è **Atenci√≥n**: Dependencia muy alta de IA. La IA deber√≠a asistirte, no resolver por vos.\n\n")

        parts.append("---\n\n")

        # Dimensiones espec√≠ficas
        if report.dimensions:
            parts.append("## üìà Evaluaci√≥n Detallada por Dimensi√≥n\n\n")
            for dim in report.dimensions:
                parts.append(f"### {dim.name}\n")
                parts.append(f"**Nivel**: {dim.level.value} | **Puntaje**: {dim.score:.1f}/10\n\n")
                parts.append(f"{dim.description}\n\n")

                if dim.strengths:
                    parts.append("**Fortalezas**:\n")
                    for s in dim.strengths:
                        parts.append(f"- {s}\n")
                    parts.append("\n")

                if dim.weaknesses:
                    parts.append("**A trabajar**:\n")
                    for w in dim.weaknesses:
                        parts.append(f"- {w}\n")
                    parts.append("\n")

                parts.append("---\n\n")

        # Riesgos cognitivos (si hay)
        if report.cognitive_risks:
            parts.append("## ‚ö†Ô∏è Aspectos a Considerar\n\n")
            for risk in report.cognitive_risks:
                parts.append(f"- {risk}\n")
            parts.append("\n---\n\n")

        # Recomendaciones accionables
        if report.recommendations_student:
            parts.append("## üéØ Recomendaciones para Seguir Mejorando\n\n")
            for i, rec in enumerate(report.recommendations_student, 1):
                parts.append(f"{i}. {rec}\n\n")
            parts.append("---\n\n")

        # Cierre motivacional
        parts.append("## üí™ Reflexi√≥n Final\n\n")
        if report.overall_score >= 7.0:
            parts.append("¬°Muy buen trabajo! Tu proceso demuestra comprensi√≥n s√≥lida y buen uso de estrategias de aprendizaje.\n\n")
        elif report.overall_score >= 5.0:
            parts.append("Vas por buen camino. Segu√≠ practicando estas competencias y prest√° atenci√≥n a las recomendaciones.\n\n")
        else:
            parts.append("Est√°s en la etapa inicial de desarrollo. No te desanimes: el aprendizaje requiere pr√°ctica constante.\n\n")

        parts.append("""
**Record√°**: Esta evaluaci√≥n analiza tu **proceso**, no solo el producto final.
El objetivo es que desarrolles habilidades de pensamiento computacional y autonom√≠a cognitiva.

---

ü§ñ *Retroalimentaci√≥n generada autom√°ticamente por E-IA-Proc basada en trazabilidad N4*
""")

        # ‚úÖ REFACTORED: Join √∫nico en lugar de m√∫ltiples concatenaciones (H1)
        return "".join(parts).strip()

    def _generate_teacher_feedback(self, report: EvaluationReport) -> str:
        """
        Genera retroalimentaci√≥n t√©cnica para el docente

        ‚úÖ REFACTORED (2025-11-22): Uso de list.join() en lugar de += (H1)
        Mejora performance 3-5x en strings largos (>1KB)
        """

        # ‚úÖ REFACTORED: Construcci√≥n con lista + join (H1)
        parts = []

        parts.append(f"""
# üìä Reporte de Evaluaci√≥n de Proceso - Docente

**Estudiante**: {report.student_id}
**Actividad**: {report.activity_id}
**Sesi√≥n**: {report.session_id}
**Fecha**: {report.timestamp.strftime("%d/%m/%Y %H:%M")}

---

## Resumen Ejecutivo

- **Nivel de Competencia**: {report.overall_competency_level.value.upper()}
- **Puntaje General**: {report.overall_score:.1f}/10
- **Dependencia de IA**: {report.ai_dependency_score:.0%}
- **Trazas Analizadas**: {report.trace_sequences_analyzed}

---

## An√°lisis del Razonamiento

### Camino Cognitivo

{self._format_cognitive_path(report.reasoning_analysis.cognitive_path)}

### Fases Completadas

{", ".join([str(p) for p in report.reasoning_analysis.phases_completed])}

### M√©tricas Clave

| M√©trica | Valor | An√°lisis |
|---------|-------|----------|
| Cambios de estrategia | {report.reasoning_analysis.strategy_changes} | {'‚úÖ Flexibilidad cognitiva' if report.reasoning_analysis.strategy_changes > 1 else '‚ö†Ô∏è Baja adaptaci√≥n'} |
| Autocorrecciones | {report.reasoning_analysis.self_corrections} | {'‚úÖ Revisi√≥n cr√≠tica activa' if report.reasoning_analysis.self_corrections > 0 else '‚ö†Ô∏è Falta revisi√≥n'} |
| Cr√≠tica a IA | {report.reasoning_analysis.ai_critiques} | {'‚úÖ Pensamiento cr√≠tico' if report.reasoning_analysis.ai_critiques > 0 else '‚ö†Ô∏è Aceptaci√≥n acr√≠tica'} |
| Coherencia | {report.reasoning_analysis.coherence_score:.2f} | {'‚úÖ Alta coherencia' if report.reasoning_analysis.coherence_score > 0.7 else '‚ö†Ô∏è Baja coherencia'} |
| Calidad de planificaci√≥n | {report.reasoning_analysis.planning_quality:.2f} | {'‚úÖ Buena planificaci√≥n' if report.reasoning_analysis.planning_quality > 0.6 else '‚ö†Ô∏è Planificaci√≥n insuficiente'} |

---

## Evaluaci√≥n por Dimensiones

""")

        for dim in report.dimensions:
            parts.append(f"### {dim.name}\n\n")
            parts.append(f"- **Nivel**: {dim.level.value}\n")
            parts.append(f"- **Score**: {dim.score:.1f}/10\n")
            parts.append(f"- **Evidencias**: {len(dim.evidence)} registradas\n\n")

        parts.append("\n---\n\n")

        # Riesgos detectados
        if report.cognitive_risks:
            parts.append("## ‚ö†Ô∏è Riesgos Cognitivos Detectados\n\n")
            for risk in report.cognitive_risks:
                parts.append(f"- {risk}\n")
            parts.append("\n---\n\n")

        # Recomendaciones para el docente
        if report.recommendations_teacher:
            parts.append("## üéì Recomendaciones Pedag√≥gicas\n\n")
            for rec in report.recommendations_teacher:
                parts.append(f"- {rec}\n")
            parts.append("\n---\n\n")

        # Patrones de uso de IA
        if report.ai_usage_patterns:
            parts.append("## ü§ñ Patrones de Uso de IA\n\n")
            parts.append(f"- Total interacciones con IA: {report.ai_usage_patterns.get('total_ai_interactions', 0)}\n")
            parts.append(f"- Tasa de interacci√≥n: {report.ai_usage_patterns.get('ai_interaction_rate', 0):.0%}\n")
            parts.append("\n---\n\n")

        parts.append("""
## üìù Notas para Evaluaci√≥n

Este reporte analiza el **proceso cognitivo**, no el producto final.
Considerar:

1. **Evaluaci√≥n de proceso (60%) + producto (40%)**
2. Evidencia de trazabilidad N4 disponible para auditor√≠a
3. Decisiones clave documentadas con justificaci√≥n
4. Patrones de colaboraci√≥n humano-IA

---

*Generado autom√°ticamente por E-IA-Proc (Evaluador de Procesos Cognitivos)*
""")

        # ‚úÖ REFACTORED: Join √∫nico en lugar de m√∫ltiples concatenaciones (H1)
        return "".join(parts).strip()

    def _format_cognitive_path(self, path: List[str]) -> str:
        """Formatea el camino cognitivo para visualizaci√≥n"""
        if not path:
            return "No disponible"
        return " ‚Üí ".join(path[:10])  # Primeros 10 estados


# Alias for backwards compatibility with response_generator imports
ProcessEvaluatorAgent = EvaluadorProcesosAgent