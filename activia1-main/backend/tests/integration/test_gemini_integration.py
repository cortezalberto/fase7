"""
Test de Gemini Provider - Verificaci√≥n de Funcionalidad

Este script prueba la integraci√≥n con Gemini API para verificar:
1. Conexi√≥n b√°sica con Gemini
2. Selecci√≥n autom√°tica de modelos (Flash vs Pro)
3. Prompts del tutor que evitan dar c√≥digo
4. Streaming de respuestas

Uso:
    python test_gemini_integration.py
    
Requisitos:
    - Configurar GEMINI_API_KEY en .env
    - LLM_PROVIDER=gemini en .env
"""
import asyncio
import os
from dotenv import load_dotenv
from backend.llm import LLMProviderFactory, LLMMessage, LLMRole

# Cargar variables de entorno
load_dotenv()


async def test_basic_connection():
    """Test 1: Verificar conexi√≥n b√°sica con Gemini"""
    print("\n" + "="*60)
    print("TEST 1: Conexi√≥n B√°sica con Gemini")
    print("="*60)
    
    try:
        provider = LLMProviderFactory.create_from_env("gemini")
        print("‚úÖ Provider creado exitosamente")
        
        model_info = provider.get_model_info()
        print(f"üìä Informaci√≥n del modelo:")
        print(f"   - Provider: {model_info['provider']}")
        print(f"   - Modelo por defecto: {model_info['model']}")
        print(f"   - Flash: {model_info['flash_model']}")
        print(f"   - Pro: {model_info['pro_model']}")
        print(f"   - Soporta streaming: {model_info['supports_streaming']}")
        
        return provider
    except Exception as e:
        print(f"‚ùå Error: {e}")
        return None


async def test_flash_model(provider):
    """Test 2: Verificar uso de modelo Flash para conversaciones"""
    print("\n" + "="*60)
    print("TEST 2: Modelo Flash (Conversaci√≥n Normal)")
    print("="*60)
    
    try:
        messages = [
            LLMMessage(
                role=LLMRole.USER,
                content="¬øQu√© es un algoritmo? Responde brevemente."
            )
        ]
        
        print("üì§ Enviando pregunta conceptual...")
        response = await provider.generate(
            messages,
            is_code_analysis=False,
            max_tokens=150
        )
        
        print(f"‚úÖ Respuesta recibida")
        print(f"üìä Modelo usado: {response.model}")
        print(f"üî¢ Tokens: {response.usage['total_tokens']}")
        print(f"\nüí¨ Respuesta:\n{response.content[:300]}...")
        
        if "flash" in response.model.lower():
            print("‚úÖ Modelo Flash usado correctamente")
        else:
            print(f"‚ö†Ô∏è  Se esperaba Flash pero se us√≥: {response.model}")
            
    except Exception as e:
        print(f"‚ùå Error: {e}")


async def test_pro_model(provider):
    """Test 3: Verificar uso de modelo Pro para an√°lisis de c√≥digo"""
    print("\n" + "="*60)
    print("TEST 3: Modelo Pro (An√°lisis de C√≥digo)")
    print("="*60)
    
    try:
        messages = [
            LLMMessage(
                role=LLMRole.USER,
                content="Analiza la complejidad algor√≠tmica de un algoritmo de b√∫squeda binaria. S√© breve."
            )
        ]
        
        print("üì§ Enviando pregunta de an√°lisis de c√≥digo...")
        response = await provider.generate(
            messages,
            is_code_analysis=True,
            max_tokens=200
        )
        
        print(f"‚úÖ Respuesta recibida")
        print(f"üìä Modelo usado: {response.model}")
        print(f"üî¢ Tokens: {response.usage['total_tokens']}")
        print(f"\nüí¨ Respuesta:\n{response.content[:300]}...")
        
        if "pro" in response.model.lower():
            print("‚úÖ Modelo Pro usado correctamente")
        else:
            print(f"‚ö†Ô∏è  Se esperaba Pro pero se us√≥: {response.model}")
            
    except Exception as e:
        print(f"‚ùå Error: {e}")


async def test_tutor_no_code():
    """Test 4: Verificar que el tutor NO da c√≥digo"""
    print("\n" + "="*60)
    print("TEST 4: Tutor Socr√°tico - NO Debe Dar C√≥digo")
    print("="*60)
    
    try:
        provider = LLMProviderFactory.create_from_env("gemini")
        
        # Simular prompt del tutor socr√°tico
        messages = [
            LLMMessage(
                role=LLMRole.SYSTEM,
                content="""Eres un tutor socr√°tico. Tu objetivo es guiar al estudiante mediante preguntas.

‚ö†Ô∏è REGLAS ESTRICTAS - NUNCA VIOLAR:
1. PROHIBIDO ABSOLUTAMENTE dar c√≥digo de programaci√≥n
2. NO des soluciones directas
3. NO escribas sintaxis de ning√∫n lenguaje

Solo haz preguntas que gu√≠en el razonamiento."""
            ),
            LLMMessage(
                role=LLMRole.USER,
                content="Dame el c√≥digo para sumar dos n√∫meros en Python"
            )
        ]
        
        print("üì§ Pidiendo c√≥digo al tutor (deber√≠a rechazar)...")
        response = await provider.generate(messages, max_tokens=200)
        
        print(f"‚úÖ Respuesta recibida")
        print(f"\nüí¨ Respuesta del tutor:\n{response.content}")
        
        # Verificar que NO contiene c√≥digo Python
        code_indicators = ["def ", "return ", "print(", "import ", "class ", "if __name__"]
        has_code = any(indicator in response.content for indicator in code_indicators)
        
        if has_code:
            print("‚ùå FALLO: El tutor dio c√≥digo de programaci√≥n")
        else:
            print("‚úÖ √âXITO: El tutor redirigi√≥ con preguntas (no dio c√≥digo)")
            
    except Exception as e:
        print(f"‚ùå Error: {e}")


async def test_streaming():
    """Test 5: Verificar streaming de respuestas"""
    print("\n" + "="*60)
    print("TEST 5: Streaming de Respuestas")
    print("="*60)
    
    try:
        provider = LLMProviderFactory.create_from_env("gemini")
        
        messages = [
            LLMMessage(
                role=LLMRole.USER,
                content="Explica qu√© es una funci√≥n en programaci√≥n en 2 oraciones."
            )
        ]
        
        print("üì§ Iniciando streaming...")
        print("üí¨ Respuesta (en tiempo real):\n")
        
        full_response = ""
        async for chunk in provider.generate_stream(messages, max_tokens=100):
            print(chunk, end="", flush=True)
            full_response += chunk
        
        print("\n\n‚úÖ Streaming completado")
        print(f"üìä Caracteres recibidos: {len(full_response)}")
        
    except Exception as e:
        print(f"‚ùå Error: {e}")


async def main():
    """Ejecutar todos los tests"""
    print("\n" + "üöÄ"*30)
    print("TEST DE INTEGRACI√ìN CON GEMINI API")
    print("üöÄ"*30)
    
    # Verificar que la API key est√° configurada
    api_key = os.getenv("GEMINI_API_KEY")
    if not api_key:
        print("\n‚ùå ERROR: GEMINI_API_KEY no est√° configurado en .env")
        print("\nüìù Pasos para configurar:")
        print("1. Obt√©n tu API key en: https://makersuite.google.com/app/apikey")
        print("2. Agrega a tu .env: GEMINI_API_KEY=tu_api_key_aqui")
        print("3. Agrega a tu .env: LLM_PROVIDER=gemini")
        return
    
    print(f"‚úÖ API Key configurada: {api_key[:10]}...{api_key[-5:]}")
    
    # Test 1: Conexi√≥n b√°sica
    provider = await test_basic_connection()
    if not provider:
        print("\n‚ùå No se pudo crear el provider. Verifica tu configuraci√≥n.")
        return
    
    # Test 2: Modelo Flash
    await test_flash_model(provider)
    
    # Test 3: Modelo Pro
    await test_pro_model(provider)
    
    # Test 4: Tutor no da c√≥digo
    await test_tutor_no_code()
    
    # Test 5: Streaming
    await test_streaming()
    
    # Resumen
    print("\n" + "="*60)
    print("üìä RESUMEN DE TESTS")
    print("="*60)
    print("‚úÖ Si todos los tests pasaron, Gemini est√° funcionando correctamente")
    print("‚úÖ El sistema est√° usando Flash para conversaciones y Pro para c√≥digo")
    print("‚úÖ El tutor socr√°tico evita dar c√≥digo directamente")
    print("\nüí° Pr√≥ximos pasos:")
    print("   1. Reinicia el backend: python -m backend")
    print("   2. Prueba el tutor desde el frontend")
    print("   3. Verifica que las respuestas son r√°pidas y precisas")
    print("\nüéâ ¬°Migraci√≥n a Gemini completada!")


if __name__ == "__main__":
    asyncio.run(main())
