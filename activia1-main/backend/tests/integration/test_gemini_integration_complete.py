"""
Prueba de integraci√≥n completa del sistema con Gemini 2.5
"""
import os
import asyncio
import sys
from dotenv import load_dotenv

# Cargar variables de entorno
load_dotenv()

# Agregar backend al path
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'backend'))

from llm.factory import LLMProviderFactory
from llm.base import LLMMessage, LLMRole

async def test_factory_creation():
    """Prueba creaci√≥n de provider desde factory"""
    print("\n1Ô∏è‚É£ Prueba de Factory")
    print("-"*60)
    
    try:
        provider = LLMProviderFactory.create_from_env()
        print(f"‚úÖ Provider creado: {type(provider).__name__}")
        print(f"   Modelo: {provider.model}")
        return True
    except Exception as e:
        print(f"‚ùå Error: {str(e)}")
        return False

async def test_simple_conversation():
    """Prueba conversaci√≥n simple"""
    print("\n2Ô∏è‚É£ Prueba de Conversaci√≥n Simple")
    print("-"*60)
    
    try:
        provider = LLMProviderFactory.create_from_env()
        
        messages = [
            LLMMessage(role=LLMRole.SYSTEM, content="Eres un asistente √∫til."),
            LLMMessage(role=LLMRole.USER, content="Di 'Hola' si me recibes")
        ]
        
        print("üîÑ Generando respuesta...")
        response = await provider.generate(messages, temperature=0.7)
        
        print(f"‚úÖ Respuesta recibida:")
        print(f"   {response.content[:100]}...")
        return True
        
    except Exception as e:
        print(f"‚ùå Error: {str(e)}")
        import traceback
        traceback.print_exc()
        return False

async def test_tutor_scenario():
    """Prueba escenario de tutor socr√°tico"""
    print("\n3Ô∏è‚É£ Prueba de Escenario de Tutor")
    print("-"*60)
    
    try:
        provider = LLMProviderFactory.create_from_env()
        
        messages = [
            LLMMessage(
                role=LLMRole.SYSTEM, 
                content="Eres un tutor de programaci√≥n que usa el m√©todo socr√°tico. Haces preguntas para guiar al estudiante."
            ),
            LLMMessage(
                role=LLMRole.USER, 
                content="No entiendo qu√© es una variable en Python"
            )
        ]
        
        print("üîÑ Generando respuesta de tutor...")
        response = await provider.generate(messages, temperature=0.8)
        
        print(f"‚úÖ Respuesta del tutor:")
        print(f"   {response.content[:200]}...")
        return True
        
    except Exception as e:
        print(f"‚ùå Error: {str(e)}")
        return False

async def test_code_analysis():
    """Prueba an√°lisis de c√≥digo"""
    print("\n4Ô∏è‚É£ Prueba de An√°lisis de C√≥digo")
    print("-"*60)
    
    try:
        provider = LLMProviderFactory.create_from_env()
        
        code = """
def calcular_promedio(numeros):
    suma = 0
    for n in numeros:
        suma = suma + n
    return suma / len(numeros)
"""
        
        messages = [
            LLMMessage(
                role=LLMRole.SYSTEM,
                content="Eres un experto en Python. Analiza el c√≥digo y da feedback conciso."
            ),
            LLMMessage(
                role=LLMRole.USER,
                content=f"Analiza este c√≥digo:\n{code}\n\n¬øEst√° bien? ¬øQu√© mejorar√≠as?"
            )
        ]
        
        print("üîÑ Analizando c√≥digo...")
        response = await provider.generate(messages, temperature=0.5)
        
        print(f"‚úÖ An√°lisis recibido:")
        print(f"   {response.content[:200]}...")
        return True
        
    except Exception as e:
        print(f"‚ùå Error: {str(e)}")
        return False

async def test_streaming():
    """Prueba generaci√≥n streaming"""
    print("\n5Ô∏è‚É£ Prueba de Streaming")
    print("-"*60)
    
    try:
        provider = LLMProviderFactory.create_from_env()
        
        messages = [
            LLMMessage(
                role=LLMRole.USER,
                content="Cuenta del 1 al 5"
            )
        ]
        
        print("üîÑ Generando con streaming...")
        chunks = []
        
        async for chunk in provider.generate_stream(messages):
            chunks.append(chunk.content)
            print(".", end="", flush=True)
        
        full_response = "".join(chunks)
        print(f"\n‚úÖ Streaming completado:")
        print(f"   Recibido: {len(chunks)} chunks")
        print(f"   Respuesta: {full_response[:100]}...")
        return True
        
    except Exception as e:
        print(f"‚ùå Error: {str(e)}")
        import traceback
        traceback.print_exc()
        return False

async def main():
    """Ejecuta todas las pruebas"""
    print("="*60)
    print("PRUEBA DE INTEGRACI√ìN COMPLETA - GEMINI 2.5")
    print("="*60)
    
    # Verificar configuraci√≥n
    print("\nüìã Configuraci√≥n:")
    print(f"   LLM_PROVIDER: {os.getenv('LLM_PROVIDER')}")
    print(f"   GEMINI_MODEL: {os.getenv('GEMINI_MODEL')}")
    print(f"   GEMINI_API_KEY: {'Configurada ‚úì' if os.getenv('GEMINI_API_KEY') else 'NO CONFIGURADA ‚úó'}")
    
    # Ejecutar pruebas
    results = []
    
    results.append(("Factory", await test_factory_creation()))
    results.append(("Conversaci√≥n", await test_simple_conversation()))
    results.append(("Tutor", await test_tutor_scenario()))
    results.append(("An√°lisis", await test_code_analysis()))
    results.append(("Streaming", await test_streaming()))
    
    # Resumen
    print("\n" + "="*60)
    print("RESUMEN DE PRUEBAS")
    print("="*60)
    
    for name, success in results:
        status = "‚úÖ" if success else "‚ùå"
        print(f"{status} {name}")
    
    all_passed = all(result[1] for result in results)
    
    if all_passed:
        print("\nüéâ ¬°TODAS LAS PRUEBAS PASARON!")
        print("   Sistema Gemini 2.5 funcionando correctamente")
    else:
        failed = [name for name, success in results if not success]
        print(f"\n‚ö†Ô∏è Pruebas fallidas: {', '.join(failed)}")
    
    return all_passed

if __name__ == "__main__":
    success = asyncio.run(main())
    exit(0 if success else 1)
