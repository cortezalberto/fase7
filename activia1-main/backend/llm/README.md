# Sistema LLM - Proveedores MÃºltiples

Este directorio contiene la implementaciÃ³n de proveedores de Large Language Models (LLM) para el sistema AI-Native.

## ğŸ¯ Arquitectura

El sistema usa el patrÃ³n **Factory** para soportar mÃºltiples proveedores de LLM de forma intercambiable:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚        LLMProviderFactory               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  + create(type, config)                 â”‚
â”‚  + create_from_env()                    â”‚
â”‚  + register_provider()                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”
        â”‚               â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ GeminiProviderâ”‚ â”‚OllamaProviderâ”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ - Flash      â”‚ â”‚ - Llama2     â”‚
â”‚ - Pro        â”‚ â”‚ - Phi3       â”‚
â”‚ (Auto switch)â”‚ â”‚ - Mistral    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“¦ Proveedores Disponibles

### 1. **Gemini** (Recomendado) ğŸŒŸ

**CaracterÃ­sticas:**
- âœ… API de Google (cloud-based)
- âœ… Dos modelos: Flash (rÃ¡pido) y Pro (avanzado)
- âœ… SelecciÃ³n automÃ¡tica segÃºn tipo de tarea
- âœ… Excelente calidad de respuestas
- âœ… Soporte para streaming
- âœ… Alta velocidad de respuesta

**Modelos:**
- `gemini-1.5-flash`: Conversaciones normales, tutorÃ­as, preguntas
- `gemini-1.5-pro`: AnÃ¡lisis de cÃ³digo, debugging, tareas complejas

**ConfiguraciÃ³n:**
```bash
LLM_PROVIDER=gemini
GEMINI_API_KEY=tu_api_key
GEMINI_MODEL=gemini-1.5-flash  # Modelo por defecto
GEMINI_TEMPERATURE=0.7
```

**Uso:**
```python
from backend.llm import LLMProviderFactory, LLMMessage, LLMRole

# Crear provider desde variables de entorno
provider = LLMProviderFactory.create_from_env()

# ConversaciÃ³n normal (usa Flash automÃ¡ticamente)
messages = [LLMMessage(role=LLMRole.USER, content="Â¿QuÃ© es un bucle?")]
response = await provider.generate(messages, is_code_analysis=False)

# AnÃ¡lisis de cÃ³digo (usa Pro automÃ¡ticamente)
messages = [LLMMessage(role=LLMRole.USER, content="Analiza este algoritmo...")]
response = await provider.generate(messages, is_code_analysis=True)
```

### 2. **Ollama** (Local, Privado)

**CaracterÃ­sticas:**
- âœ… EjecuciÃ³n local (sin enviar datos a la nube)
- âœ… Gratis (sin costos por token)
- âœ… MÃºltiples modelos open-source
- âœ… Privacidad total
- âš ï¸ Requiere hardware potente
- âš ï¸ MÃ¡s lento que Gemini

**Modelos soportados:**
- `phi3`: Microsoft Phi-3 (rÃ¡pido, 3.8B params)
- `llama2`: Meta Llama 2 (buena calidad)
- `mistral`: Mistral AI (equilibrado)
- `codellama`: Especializado en cÃ³digo

**ConfiguraciÃ³n:**
```bash
LLM_PROVIDER=ollama
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=phi3
OLLAMA_TEMPERATURE=0.7
OLLAMA_TIMEOUT=120
```

### 3. **Mock** (Testing)

**CaracterÃ­sticas:**
- âœ… Para pruebas y desarrollo
- âœ… No requiere API keys
- âœ… Respuestas predefinidas
- âš ï¸ Solo para testing, no producciÃ³n

**ConfiguraciÃ³n:**
```bash
LLM_PROVIDER=mock
```

## ğŸ”„ SelecciÃ³n AutomÃ¡tica de Modelos (Gemini)

El sistema **detecta automÃ¡ticamente** cuÃ¡ndo usar cada modelo:

### Palabras Clave que Activan Gemini Pro:

- CÃ³digo: `cÃ³digo`, `code`, `funciÃ³n`, `function`, `class`, `mÃ©todo`
- Algoritmos: `algoritmo`, `algorithm`, `complejidad`, `complexity`
- Debugging: `bug`, `error`, `debug`, `refactor`
- OptimizaciÃ³n: `optimizar`, `optimize`

### Flujo de DecisiÃ³n:

```
User Input
    â”‚
    â”œâ”€ Contiene palabras clave de cÃ³digo?
    â”‚
    â”œâ”€ SÃ â†’ Gemini Pro (anÃ¡lisis profundo)
    â”‚
    â””â”€ NO â†’ Gemini Flash (rÃ¡pido, econÃ³mico)
```

## ğŸ“‹ Interface Base

Todos los proveedores implementan la interface `LLMProvider`:

```python
class LLMProvider(ABC):
    async def generate(
        self,
        messages: List[LLMMessage],
        temperature: float = 0.7,
        max_tokens: Optional[int] = None,
        **kwargs
    ) -> LLMResponse:
        """Generar respuesta"""
        pass

    async def generate_stream(
        self,
        messages: List[LLMMessage],
        temperature: float = 0.7,
        max_tokens: Optional[int] = None,
        **kwargs
    ):
        """Generar respuesta en streaming"""
        pass

    def count_tokens(self, text: str) -> int:
        """Contar tokens en texto"""
        pass
```

## ğŸ”§ Uso Avanzado

### Crear Provider Manualmente:

```python
from backend.llm import LLMProviderFactory

# Gemini con configuraciÃ³n personalizada
provider = LLMProviderFactory.create("gemini", {
    "api_key": "tu_api_key",
    "model": "gemini-1.5-pro",  # Forzar Pro
    "temperature": 0.5,
    "timeout": 30
})

# Ollama con modelo personalizado
provider = LLMProviderFactory.create("ollama", {
    "base_url": "http://localhost:11434",
    "model": "codellama",
    "temperature": 0.3
})
```

### Usar Streaming:

```python
async for chunk in provider.generate_stream(messages):
    print(chunk, end="", flush=True)
```

### ParÃ¡metros Adicionales:

```python
response = await provider.generate(
    messages,
    temperature=0.7,       # Creatividad (0.0-1.0)
    max_tokens=500,        # LÃ­mite de tokens
    is_code_analysis=True  # Gemini: forzar modelo Pro
)
```

## ğŸ“Š MÃ©tricas y Monitoreo

Cada proveedor reporta mÃ©tricas automÃ¡ticamente:

```python
# Las mÃ©tricas estÃ¡n en response.usage
{
    "prompt_tokens": 150,
    "completion_tokens": 200,
    "total_tokens": 350
}

# TambiÃ©n se registran en Prometheus (si estÃ¡ configurado)
llm_requests_total{provider="gemini", model="flash", status="success"}
llm_tokens_total{provider="gemini", model="flash", type="prompt"}
```

## ğŸ” Seguridad

### API Keys:

- **Gemini**: Guardar en `.env`, nunca en cÃ³digo
- **Ollama**: No requiere API key
- Usar variables de entorno para todas las configuraciones sensibles

### Rate Limiting:

- Gemini: Implementa retry automÃ¡tico con backoff exponencial
- Ollama: Sin lÃ­mites (local)

### ValidaciÃ³n:

```python
# El factory valida la configuraciÃ³n automÃ¡ticamente
try:
    provider = LLMProviderFactory.create_from_env("gemini")
except ValueError as e:
    print(f"ConfiguraciÃ³n invÃ¡lida: {e}")
```

## ğŸ§ª Testing

### Test RÃ¡pido:

```python
# tests/test_llm_providers.py
import pytest
from backend.llm import LLMProviderFactory, LLMMessage, LLMRole

@pytest.mark.asyncio
async def test_gemini_provider():
    provider = LLMProviderFactory.create("gemini", {
        "api_key": "test_key",
        "model": "gemini-1.5-flash"
    })
    
    messages = [LLMMessage(role=LLMRole.USER, content="Test")]
    response = await provider.generate(messages)
    
    assert response.content
    assert response.model == "gemini-1.5-flash"
    assert response.usage["total_tokens"] > 0
```

### Test de SelecciÃ³n de Modelo:

```python
@pytest.mark.asyncio
async def test_model_selection():
    provider = LLMProviderFactory.create_from_env("gemini")
    
    # ConversaciÃ³n normal â†’ Flash
    messages = [LLMMessage(role=LLMRole.USER, content="Hola")]
    response = await provider.generate(messages, is_code_analysis=False)
    assert response.model == "gemini-1.5-flash"
    
    # AnÃ¡lisis de cÃ³digo â†’ Pro
    messages = [LLMMessage(role=LLMRole.USER, content="Analiza cÃ³digo")]
    response = await provider.generate(messages, is_code_analysis=True)
    assert response.model == "gemini-1.5-pro"
```

## ğŸ“š Archivos

```
backend/llm/
â”œâ”€â”€ __init__.py           # Exports principales
â”œâ”€â”€ base.py               # Interface LLMProvider
â”œâ”€â”€ factory.py            # Factory para crear providers
â”œâ”€â”€ gemini_provider.py    # ImplementaciÃ³n Gemini â­
â”œâ”€â”€ ollama_provider.py    # ImplementaciÃ³n Ollama
â”œâ”€â”€ mock.py               # Mock provider (testing)
â””â”€â”€ README.md             # Esta documentaciÃ³n
```

## ğŸš€ Mejores PrÃ¡cticas

1. **Usar Factory Pattern:**
   ```python
   # âœ… Correcto
   provider = LLMProviderFactory.create_from_env()
   
   # âŒ Evitar
   provider = GeminiProvider({"api_key": "..."})
   ```

2. **Manejar Errores:**
   ```python
   try:
       response = await provider.generate(messages)
   except httpx.HTTPStatusError as e:
       logger.error(f"LLM error: {e}")
       # Implementar fallback
   ```

3. **Configurar desde ENV:**
   ```python
   # âœ… Correcto
   LLM_PROVIDER=gemini
   GEMINI_API_KEY=...
   
   # âŒ Evitar hardcodear
   provider = create("gemini", {"api_key": "sk-..."})
   ```

4. **Optimizar Costos (Gemini):**
   - Usar `is_code_analysis=False` cuando sea posible (Flash es mÃ¡s barato)
   - Cachear respuestas comunes
   - Limitar `max_tokens` apropiadamente

## ğŸ’¡ Troubleshooting

### "Unknown provider type"
**Causa:** Provider no registrado
**SoluciÃ³n:** Verificar que el provider estÃ¡ en `factory.py`

### "GEMINI_API_KEY is required"
**Causa:** Falta API key en `.env`
**SoluciÃ³n:** Agregar `GEMINI_API_KEY=tu_key` en `.env`

### Respuestas lentas
**Causa:** Timeout muy alto o red lenta
**SoluciÃ³n:** Ajustar `GEMINI_TIMEOUT` o usar Ollama local

### Errores 429 (Rate Limit)
**Causa:** Demasiadas peticiones
**SoluciÃ³n:** Implementar rate limiting local o aumentar cuota

## ğŸ”„ Changelog

### v2.0 (Actual)
- âœ… Soporte para Gemini API
- âœ… SelecciÃ³n automÃ¡tica Flash/Pro
- âœ… Prompts mejorados anti-cÃ³digo
- âœ… DetecciÃ³n inteligente de tareas

### v1.0
- Soporte bÃ¡sico para Ollama
- Mock provider para testing

---

**PrÃ³ximos Pasos:**
1. Configurar tu API key de Gemini
2. Actualizar `.env` con `LLM_PROVIDER=gemini`
3. Probar el sistema con `test_gemini.py`
4. Ver `MIGRACION_GEMINI.md` para guÃ­a completa
