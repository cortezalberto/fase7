"""
AI Gateway - Componente central que coordina todos los submodelos
Implementa la arquitectura C4 del ecosistema AI-Native

REFACTORIZADO (2025-11-19):
- Ahora es completamente STATELESS (no mantiene estado en memoria)
- Dependency Injection completa (repositorios inyectados)
- Escalable (puede funcionar con m√∫ltiples instancias)
- Testeable (f√°cil mockear dependencias)
- Cache LLM integrado (opcional, reduce costos 30-50%)

FUTURE ENHANCEMENT (REC-MED-6 - Audit Cortez):
Background Tasks for Risk Analysis:
- Currently risk analysis runs synchronously in _analyze_risks_async()
- For better UX and scalability, consider implementing:
  1. FastAPI BackgroundTasks for simple async processing
  2. Celery + Redis for distributed task queue
  3. WebSocket notifications when analysis completes
- This would allow immediate response to user while analysis runs in background
- See: https://fastapi.tiangolo.com/tutorial/background-tasks/
"""
from typing import Optional, Dict, Any, List
from datetime import datetime
import uuid
import logging
import time
import asyncio

from .cognitive_engine import CognitiveReasoningEngine, AgentMode
from ..models.trace import CognitiveTrace, TraceLevel, InteractionType, TraceSequence
from ..models.risk import Risk, RiskType, RiskLevel, RiskDimension, RiskReport
from ..models.evaluation import EvaluationReport
from ..llm import LLMProviderFactory, LLMProvider, LLMMessage, LLMRole
from .cache import LLMResponseCache
from ..agents.governance import GobernanzaAgent
# FIX Cortez68 (HIGH-005): Import protocols from gateway module instead of duplicating
from .gateway.protocols import (
    SessionRepositoryProtocol,
    TraceRepositoryProtocol,
    RiskRepositoryProtocol,
    EvaluationRepositoryProtocol,
    SequenceRepositoryProtocol,
)

# Prometheus metrics instrumentation (HIGH-01)
# Lazy import to avoid circular dependency with api.monitoring
# FIXED: Added thread safety with double-checked locking pattern
import threading

_metrics_module = None
_metrics_lock = threading.Lock()

def _get_metrics():
    """
    Lazy load metrics module to avoid circular imports.

    Thread-safe implementation using double-checked locking pattern.
    This prevents race conditions when multiple threads try to initialize
    the metrics module simultaneously.
    """
    global _metrics_module
    if _metrics_module is None:
        with _metrics_lock:
            # Double-check inside lock to avoid race condition
            if _metrics_module is None:
                try:
                    from ..api.monitoring import metrics as m
                    _metrics_module = m
                except ImportError:
                    _metrics_module = False  # Mark as unavailable
    return _metrics_module if _metrics_module else None

logger = logging.getLogger(__name__)


# FIX Cortez68 (HIGH-005): Protocol definitions removed - now imported from gateway.protocols
# See: backend/core/gateway/protocols.py for canonical definitions


class AIGateway:
    """
    AI Gateway - Orquestador central STATELESS del ecosistema AI-Native

    Componentes:
    - C1: Motor LLM (conexi√≥n a OpenAI/Anthropic/etc)
    - C2: Ingesta y Comprensi√≥n de Prompt (IPC)
    - C3: Motor de Razonamiento Cognitivo-Pedag√≥gico (CRPE)
    - C4: Gobernanza, Seguridad y Riesgo (GSR)
    - C5: Orquestaci√≥n de Submodelos (OSM)
    - C6: Trazabilidad Cognitiva N4

    Integra:
    - T-IA-Cog: Tutor IA Cognitivo
    - E-IA-Proc: Evaluador de Procesos
    - S-IA-X: Simuladores Profesionales
    - AR-IA: Analista de Riesgo
    - GOV-IA: Gobernanza
    - TC-N4: Trazabilidad

    IMPORTANTE: Gateway es STATELESS
    - Todo el estado se persiste en BD via repositorios
    - No mantiene sesiones/trazas/riesgos en memoria
    - Puede usarse con m√∫ltiples instancias (load balancer)
    """

    def __init__(
        self,
        llm_provider: Optional[LLMProvider] = None,
        cognitive_engine: Optional[CognitiveReasoningEngine] = None,
        session_repo: Optional[SessionRepositoryProtocol] = None,
        trace_repo: Optional[TraceRepositoryProtocol] = None,
        risk_repo: Optional[RiskRepositoryProtocol] = None,
        evaluation_repo: Optional[EvaluationRepositoryProtocol] = None,
        sequence_repo: Optional[SequenceRepositoryProtocol] = None,
        cache: Optional[LLMResponseCache] = None,
        config: Optional[Dict[str, Any]] = None
    ):
        """
        Inicializa el AI Gateway con Dependency Injection completa

        Args:
            llm_provider: Proveedor de LLM (inyectado)
            cognitive_engine: Motor de razonamiento cognitivo (inyectado)
            session_repo: Repositorio de sesiones (inyectado)
            trace_repo: Repositorio de trazas (inyectado)
            risk_repo: Repositorio de riesgos (inyectado)
            evaluation_repo: Repositorio de evaluaciones (inyectado)
            sequence_repo: Repositorio de secuencias (inyectado)
            cache: Cache de respuestas LLM (inyectado, opcional)
            config: Configuraci√≥n adicional

        Note:
            Si no se inyectan dependencias, se crean con valores por defecto
            (√∫til para backward compatibility con c√≥digo existente)
        """
        self.config = config or {}

        # C1: Motor LLM - Usar proveedor inyectado o crear uno por defecto
        if llm_provider is not None:
            self.llm = llm_provider
        else:
            # Backward compatibility: crear proveedor mock por defecto
            self.llm = LLMProviderFactory.create("mock", self.config.get("llm", {}))

        # C3: Motor de Razonamiento Cognitivo-Pedag√≥gico - Inyectado
        if cognitive_engine is not None:
            self.cognitive_engine = cognitive_engine
        else:
            # Backward compatibility
            self.cognitive_engine = CognitiveReasoningEngine(self.config)
        
        # C4: Agente de Gobernanza - Instanciar para filtrado PII
        self.governance_agent = GobernanzaAgent(llm_provider=None, config=self.config)

        # Repositorios inyectados (opcional para backward compatibility)
        self.session_repo = session_repo
        self.trace_repo = trace_repo
        self.risk_repo = risk_repo
        self.evaluation_repo = evaluation_repo
        self.sequence_repo = sequence_repo

        # Cache LLM (opcional, para reducir costos)
        self.cache = cache

        # FIX Cortez35: Task registry to prevent garbage collection of background tasks
        # This keeps a strong reference to running tasks so they don't get GC'd
        self._background_tasks: set = set()

        # ‚úÖ ELIMINADO: No m√°s estado en memoria
        # ‚ùå self.trace_sequences: Dict[str, TraceSequence] = {}
        # ‚ùå self.traces: List[CognitiveTrace] = []
        # ‚ùå self.risks: Dict[str, RiskReport] = {}
        # ‚ùå self.active_sessions: Dict[str, Dict[str, Any]] = {}

    def create_session(
        self,
        student_id: str,
        activity_id: str,
        mode: str = "TUTOR",
        session_id: Optional[str] = None,
        metadata: Optional[Dict[str, Any]] = None
    ) -> str:
        """
        Crea una nueva sesi√≥n de interacci√≥n (STATELESS)

        Args:
            student_id: ID del estudiante
            activity_id: ID de la actividad
            mode: Modo del agente (TUTOR, EVALUATOR, etc.)
            session_id: ID de sesi√≥n (opcional, se genera si no se proporciona)
            metadata: Metadata adicional

        Returns:
            session_id: ID de la sesi√≥n creada

        Note:
            Si session_repo est√° inyectado, persiste en BD.
            Si no, solo retorna el ID (backward compatibility para CLI).
        """
        if session_id is None:
            session_id = str(uuid.uuid4())

        # ‚úÖ STATELESS: Persistir en BD via repositorio (si est√° inyectado)
        if self.session_repo is not None:
            db_session = self.session_repo.create(
                student_id=student_id,
                activity_id=activity_id,
                mode=mode
            )
            session_id = db_session.id

        # ‚úÖ STATELESS: Crear secuencia de trazas en BD (si est√° inyectado)
        if self.sequence_repo is not None:
            trace_sequence = TraceSequence(
                id=f"seq_{session_id}",
                session_id=session_id,
                student_id=student_id,
                activity_id=activity_id
            )
            self.sequence_repo.create(trace_sequence)

        # ‚ùå ELIMINADO: No m√°s guardado en memoria
        # self.active_sessions[session_id] = session
        # self.trace_sequences[session_id] = trace_sequence

        return session_id

    async def process_interaction(
        self,
        session_id: str,
        prompt: str,
        context: Optional[Dict[str, Any]] = None,
        flow_id: Optional[str] = None,
    ) -> Dict[str, Any]:
        """
        Procesa una interacci√≥n del estudiante a trav√©s del gateway (STATELESS)

        Este es el flujo principal que:
        1. Valida entrada
        2. Obtiene sesi√≥n desde BD (no de memoria)
        3. Clasifica el prompt (IPC)
        4. Verifica gobernanza (GSR)
        5. Genera estrategia pedag√≥gica (CRPE)
        6. Detecta riesgos (AR-IA)
        7. Registra en trazabilidad (N4) v√≠a repositorio
        8. Genera respuesta seg√∫n el agente activo

        Args:
            session_id: ID de la sesi√≥n
            prompt: Prompt del estudiante
            context: Contexto adicional

        Returns:
            Diccionario con la respuesta y metadata

        Raises:
            ValueError: Si la entrada no cumple con los requisitos de validaci√≥n
        """
        flow_id = flow_id or f"flow_{uuid.uuid4()}"
        flow_started_at = time.perf_counter()
        logger.info(
            "Interaction flow started",
            extra={
                "flow_id": flow_id,
                "session_id": session_id,
                "prompt_preview": prompt[:160],
                "context_keys": sorted((context or {}).keys()) if context else [],
                # FIX Cortez68 (MEDIUM): Use timezone-aware datetime
                "timestamp": datetime.now(tz=__import__('datetime').timezone.utc).isoformat()
            }
        )

        # ‚úÖ VALIDACI√ìN: Validar entrada antes de procesar
        self._validate_interaction_input(session_id, prompt, context)

        # ‚úÖ GOBERNANZA: Filtrar PII del prompt antes de procesarlo
        sanitized_prompt, pii_detected = self.governance_agent.sanitize_prompt(prompt)
        if pii_detected:
            logger.warning(
                "PII detectado y removido del prompt",
                extra={
                    "session_id": session_id,
                    "original_length": len(prompt),
                    "flow_id": flow_id
                }
            )
            prompt = sanitized_prompt
            logger.info(
                "Prompt sanitized after PII detection",
                extra={
                    "flow_id": flow_id,
                    "session_id": session_id,
                    "pii_detected": True,
                    "sanitized_length": len(prompt)
                }
            )

        # ‚úÖ STATELESS: Obtener sesi√≥n desde BD (no desde self.active_sessions)
        # TODO Cortez68 (CRIT-006): Consider adding optimistic locking with version field
        # to prevent race conditions on concurrent session modifications. This requires
        # adding a `version` column to SessionDB and incrementing it on each update.
        if self.session_repo is not None:
            db_session = self.session_repo.get_by_id(session_id)
            if not db_session:
                raise ValueError(f"Sesi√≥n {session_id} no encontrada en BD")

            student_id = db_session.student_id
            activity_id = db_session.activity_id
            current_mode = AgentMode(db_session.mode.upper())
            logger.info(
                "Session context loaded",
                extra={
                    "flow_id": flow_id,
                    "session_id": session_id,
                    "student_id": student_id,
                    "activity_id": activity_id,
                    "agent_mode": current_mode.value
                }
            )
        else:
            raise ValueError("Session repo no disponible - no se puede procesar interacci√≥n")

        # C2: Ingesta y Comprensi√≥n de Prompt (IPC)
        classification = self.cognitive_engine.classify_prompt(
            prompt,
            context or {}
        )
        logger.info(
            "Prompt classified",
            extra={
                "session_id": session_id,
                "student_id": student_id,
                "classification": classification,
                "flow_id": flow_id
            }
        )

        # C4: Gobernanza - verificar si debe bloquearse
        should_block, block_reason = self.cognitive_engine.should_block_response(classification)
        logger.info(
            "Governance decision evaluated",
            extra={
                "flow_id": flow_id,
                "session_id": session_id,
                "should_block": should_block,
                "block_reason": block_reason
            }
        )

        # C6: Registrar traza de entrada (N3/N4)
        input_trace = self._create_trace(
            session_id=session_id,
            student_id=student_id,
            activity_id=activity_id,
            interaction_type=InteractionType.STUDENT_PROMPT,
            content=prompt,
            level=TraceLevel.N4_COGNITIVO,
            cognitive_intent=classification.get("cognitive_state", "").value if classification.get("cognitive_state") else None,
            context={"classification": classification}
        )
        self._persist_trace(input_trace)

        # Si debe bloquearse, retornar mensaje pedag√≥gico
        if should_block:
            response = self._generate_blocked_response(block_reason, classification)

            # Registrar la intervenci√≥n
            intervention_trace = self._create_trace(
                session_id=session_id,
                student_id=student_id,
                activity_id=activity_id,
                interaction_type=InteractionType.TUTOR_INTERVENTION,
                content=response.get("response", response.get("message", "")),  # Support both keys
                level=TraceLevel.N4_COGNITIVO,
                agent_id="GOV-IA"
            )
            self._persist_trace(intervention_trace)

            # Registrar riesgo detectado
            self._persist_risk(
                session_id=session_id,
                student_id=student_id,
                activity_id=activity_id,
                risk_type=RiskType.COGNITIVE_DELEGATION,
                risk_level=RiskLevel.HIGH,
                dimension=RiskDimension.COGNITIVE,
                description="Delegaci√≥n total detectada en el prompt",
                evidence=[prompt],
                trace_ids=[input_trace.id],
                flow_id=flow_id
            )

            # ‚úÖ HIGH-01: Record Prometheus metrics for governance block
            metrics = _get_metrics()
            if metrics:
                metrics.record_interaction(
                    session_id=session_id,
                    student_id=student_id,
                    agent_used="GOV-IA",
                    status="blocked"
                )
                metrics.record_governance_block(
                    reason="total_delegation",
                    session_id=session_id
                )

            logger.info(
                "Interaction flow completed",
                extra={
                    "flow_id": flow_id,
                    "session_id": session_id,
                    "agent_mode": current_mode.value,
                    "outcome": "blocked",
                    "duration_ms": round((time.perf_counter() - flow_started_at) * 1000, 2)
                }
            )
            return response

        # C3: Generar estrategia pedag√≥gica
        student_history = self._get_student_history(student_id, activity_id)
        strategy = self.cognitive_engine.generate_pedagogical_response_strategy(
            prompt,
            classification,
            student_history
        )
        # Log strategy generation
        logger.info(
            "Pedagogical strategy generated",
            extra={
                "session_id": session_id,
                "student_id": student_id,
                "strategy": strategy,
                "flow_id": flow_id
            }
        )

        # C5: Orquestaci√≥n - delegar al submodelo apropiado
        logger.info(
            "Orchestrating to agent",
            extra={
                "session_id": session_id,
                "agent": current_mode.value,
                "prompt_preview": prompt[:200],
                "flow_id": flow_id
            }
        )

        agent_started_at = time.perf_counter()
        if current_mode == AgentMode.TUTOR:
            response = await self._process_tutor_mode(
                session_id, prompt, strategy, classification, flow_id=flow_id
            )
        elif current_mode == AgentMode.SIMULATOR:
            # FIX Cortez22 DEFECTO 1.1: Add await for async method
            response = await self._process_simulator_mode(
                session_id, prompt, strategy, classification, flow_id=flow_id
            )
        elif current_mode == AgentMode.EVALUATOR:
            # FIX Cortez22 DEFECTO 1.1: Add await for async method
            response = await self._process_evaluator_mode(
                session_id, prompt, strategy, classification, flow_id=flow_id
            )
        else:
            # FIX Cortez22 DEFECTO 1.8: Use "response" not "message"
            response = {"response": "Modo no implementado", "metadata": {}}
        agent_duration_ms = round((time.perf_counter() - agent_started_at) * 1000, 2)
        logger.info(
            "Agent response generated",
            extra={
                "flow_id": flow_id,
                "session_id": session_id,
                "agent_mode": current_mode.value,
                "from_cache": response.get("metadata", {}).get("from_cache"),
                "duration_ms": agent_duration_ms
            }
        )

        # Registrar respuesta en trazas
        response_trace = self._create_trace(
            session_id=session_id,
            student_id=student_id,
            activity_id=activity_id,
            interaction_type=InteractionType.AI_RESPONSE,
            content=response.get("response", response.get("message", "")),  # Support both keys
            level=TraceLevel.N4_COGNITIVO,
            agent_id=current_mode.value,
            context={"strategy": strategy}
        )
        self._persist_trace(response_trace)

        # An√°lisis de riesgo en paralelo (AR-IA)
        self._run_risk_analysis_background(
            session_id,
            input_trace,
            response_trace,
            classification,
            flow_id
        )

        # ‚úÖ HIGH-01: Record Prometheus metrics for successful interaction
        metrics = _get_metrics()
        if metrics:
            metrics.record_interaction(
                session_id=session_id,
                student_id=student_id,
                agent_used=current_mode.value,
                status="success"
            )
            # Record cognitive state if available
            cognitive_state = classification.get("cognitive_state")
            if cognitive_state:
                metrics.record_cognitive_state(cognitive_state.value if hasattr(cognitive_state, 'value') else str(cognitive_state))

        logger.info(
            "Interaction flow completed",
            extra={
                "flow_id": flow_id,
                "session_id": session_id,
                "agent_mode": current_mode.value,
                "outcome": "success",
                "duration_ms": round((time.perf_counter() - flow_started_at) * 1000, 2)
            }
        )
        return response

    def _run_risk_analysis_background(
        self,
        session_id: str,
        input_trace: CognitiveTrace,
        response_trace: CognitiveTrace,
        classification: Dict[str, Any],
        flow_id: Optional[str]
    ) -> None:
        """Ejecuta el an√°lisis de riesgo en background para liberar la respuesta principal."""
        if self.risk_repo is None:
            # Sin repositorio no podemos persistir riesgos (compatibilidad/backward)
            self._analyze_risks_async(session_id, input_trace, response_trace, classification, flow_id)
            return

        logger.info(
            "Scheduling risk analysis task",
            extra={
                "flow_id": flow_id,
                "session_id": session_id,
                "input_trace_id": input_trace.id,
                "response_trace_id": response_trace.id
            }
        )

        try:
            loop = asyncio.get_running_loop()
        except RuntimeError:
            # Entorno s√≠ncrono (por ejemplo, tests) => ejecutar inline para conservar comportamiento
            self._analyze_risks_async(session_id, input_trace, response_trace, classification, flow_id)
            return

        def _analyze_with_fresh_db_session() -> None:
            """Run risk analysis using a brand-new DB session (safe for background threads)."""
            try:
                from ..database import get_db_session
                from ..database.repositories import RiskRepository
            except Exception:
                logger.error(
                    "Failed to import database helpers for background risk analysis",
                    exc_info=True,
                    extra={"flow_id": flow_id, "session_id": session_id}
                )
                return

            with get_db_session() as db_session:
                risk_repo = RiskRepository(db_session)
                self._analyze_risks_async(
                    session_id,
                    input_trace,
                    response_trace,
                    classification,
                    flow_id,
                    risk_repo_override=risk_repo,
                )

        async def _async_task():
            # FIX Cortez68 (CRIT-005): Add retry mechanism for failed risk analysis
            max_retries = 3
            retry_delay = 1.0  # seconds
            started_at = time.perf_counter()
            last_exception = None

            for attempt in range(max_retries):
                try:
                    await asyncio.to_thread(
                        _analyze_with_fresh_db_session
                    )
                    logger.info(
                        "Risk analysis completed asynchronously",
                        extra={
                            "flow_id": flow_id,
                            "session_id": session_id,
                            "duration_ms": round((time.perf_counter() - started_at) * 1000, 2),
                            "attempt": attempt + 1
                        }
                    )
                    return  # Success, exit
                except Exception as exc:
                    last_exception = exc
                    if attempt < max_retries - 1:
                        logger.warning(
                            "Risk analysis task failed, retrying",
                            extra={
                                "flow_id": flow_id,
                                "session_id": session_id,
                                "attempt": attempt + 1,
                                "max_retries": max_retries,
                                "retry_delay": retry_delay
                            }
                        )
                        await asyncio.sleep(retry_delay * (attempt + 1))  # Exponential backoff
                    else:
                        logger.error(
                            "Risk analysis task failed after all retries",
                            exc_info=True,
                            extra={
                                "flow_id": flow_id,
                                "session_id": session_id,
                                "input_trace_id": input_trace.id,
                                "response_trace_id": response_trace.id,
                                "total_attempts": max_retries,
                                "error": str(last_exception)
                            }
                        )

        # FIX Cortez34: Add done callback to track task completion and log errors
        # FIX Cortez35: Also remove task from registry when done
        def _task_done_callback(task: asyncio.Task) -> None:
            """Callback to log unhandled exceptions and cleanup task registry."""
            # FIX Cortez35: Remove from registry to allow GC after completion
            self._background_tasks.discard(task)

            try:
                exc = task.exception()
                if exc is not None:
                    logger.error(
                        "Background risk analysis task raised unhandled exception",
                        exc_info=exc,
                        extra={
                            "flow_id": flow_id,
                            "session_id": session_id,
                            "task_name": task.get_name()
                        }
                    )
            except asyncio.CancelledError:
                logger.warning(
                    "Background risk analysis task was cancelled",
                    extra={"flow_id": flow_id, "session_id": session_id}
                )
            except asyncio.InvalidStateError:
                pass  # Task not done yet, ignore

        task = loop.create_task(_async_task(), name=f"risk_analysis_{session_id}")
        # FIX Cortez35: Add to registry to prevent GC while running
        self._background_tasks.add(task)
        task.add_done_callback(_task_done_callback)

    def _generate_blocked_response(
        self,
        reason: str,
        classification: Dict[str, Any]
    ) -> Dict[str, Any]:
        """Genera una respuesta pedag√≥gica cuando se bloquea la solicitud"""
        cognitive_state = classification.get("cognitive_state")

        message = f"""
He detectado que tu solicitud implica una delegaci√≥n total del problema a la IA.

{reason}

Para poder ayudarte efectivamente, necesito que:

1. **Expliques tu comprensi√≥n del problema**: ¬øQu√© te piden resolver?
2. **Descompongas el problema**: ¬øQu√© partes identificas?
3. **Compartas tu plan inicial**: ¬øC√≥mo pens√°s abordarlo?
4. **Identifiques tus dudas espec√≠ficas**: ¬øQu√© parte espec√≠fica te genera dificultad?

Esto no es una limitaci√≥n arbitraria: el objetivo es que desarrolles tu capacidad de razonamiento y resoluci√≥n de problemas, que son competencias fundamentales.

¬øPod√©s reformular tu consulta siguiendo estas pautas?
"""

        return {
            "response": message.strip(),  # Changed from "message" to "response"
            "blocked": True,
            "block_reason": reason,  # Changed from "reason" to "block_reason"
            "requires_reformulation": True,
            "metadata": {
                "classification": classification,
                "pedagogical_intent": "promote_autonomy"
            }
        }

    async def _process_tutor_mode(
        self,
        session_id: str,
        prompt: str,
        strategy: Dict[str, Any],
        classification: Dict[str, Any],
        flow_id: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        Procesa la interacci√≥n en modo T-IA-Cog (Tutor)

        En el MVP, genera respuestas basadas en la estrategia pedag√≥gica.
        En producci√≥n, integrar√≠a con LLM real.

        Integraci√≥n de cache:
        - Verifica cache antes de generar respuesta
        - Guarda respuesta en cache despu√©s de generarla
        - Ahorra costos de LLM (30-50% en prompts repetidos)
        """
        response_type = strategy.get("response_type", "unknown")
        
        logger.info(
            "Processing tutor mode",
            extra={
                "session_id": session_id,
                "flow_id": flow_id,
                "response_type": response_type,
                "strategy_keys": sorted(strategy.keys()) if isinstance(strategy, dict) else []
            }
        )

        # Preparar contexto para cache key
        cache_context = {
            "response_type": response_type,
            "cognitive_state": classification.get("cognitive_state", "").value if classification.get("cognitive_state") else None
        }

        # Intentar obtener respuesta del cache
        cached_response = None
        if self.cache is not None:
            cached_response = self.cache.get(
                prompt=prompt,
                context=cache_context,
                mode="TUTOR"
            )

        if cached_response is not None:
            # Cache HIT - usar respuesta cacheada
            logger.info(
                f"Using cached response for prompt (saved LLM call)",
                extra={
                    "session_id": session_id,
                    "response_type": response_type,
                    "prompt_preview": prompt[:50],
                    "flow_id": flow_id
                }
            )
            message = cached_response
        else:
            # Cache MISS - generar respuesta nueva
            if response_type == "socratic_questioning":
                message = await self._generate_socratic_response(prompt, strategy, session_id, flow_id=flow_id)
            elif response_type == "conceptual_explanation":
                message = await self._generate_conceptual_explanation(prompt, strategy, session_id, flow_id=flow_id)
            elif response_type == "guided_hints":
                message = await self._generate_guided_hints(prompt, strategy, session_id, flow_id=flow_id)
            # ============================================================
            # NUEVOS TIPOS DE RESPUESTA (FIX Cortez64)
            # ============================================================
            elif response_type == "empathetic_support":
                message = await self._generate_empathetic_support(prompt, strategy, session_id, flow_id=flow_id)
            elif response_type == "metacognitive_guidance":
                message = await self._generate_metacognitive_guidance(prompt, strategy, session_id, flow_id=flow_id)
            elif response_type == "example_based":
                message = await self._generate_example_based(prompt, strategy, session_id, flow_id=flow_id)
            elif response_type == "clarification_request":
                message = await self._generate_clarification_request(prompt, strategy, session_id, flow_id=flow_id)
            else:
                # Fallback: usar explicaci√≥n conceptual para casos no clasificados
                logger.warning(
                    "Unknown response_type received, using conceptual_explanation",
                    extra={
                        "session_id": session_id,
                        "flow_id": flow_id,
                        "response_type": response_type
                    }
                )
                message = await self._generate_conceptual_explanation(prompt, strategy, session_id, flow_id=flow_id)

            # Guardar en cache para futuras solicitudes id√©nticas
            if self.cache is not None:
                self.cache.set(
                    prompt=prompt,
                    response=message,
                    context=cache_context,
                    mode="TUTOR"
                )

        return {
            "response": message,  # Changed from "message" to "response"
            "strategy": strategy,
            "mode": "tutor",
            "metadata": {
                "response_type": response_type,
                "cognitive_state": classification.get("cognitive_state", "").value if classification.get("cognitive_state") else None,
                "from_cache": cached_response is not None
            }
        }

    async def _generate_socratic_response(
        self,
        prompt: str,
        strategy: Dict[str, Any],
        session_id: str = None,
        flow_id: Optional[str] = None
    ) -> str:
        """‚úÖ Genera respuesta socr√°tica con memoria de conversaci√≥n"""
        # Recuperar historial de conversaci√≥n si hay session_id
        conversation_history = []
        if session_id and self.trace_repo:
            conversation_history = self._load_conversation_history(session_id)
        
        # Construir mensajes con historial + system prompt + prompt actual
        messages = [
            LLMMessage(
                role=LLMRole.SYSTEM,
                content="""Eres un tutor socr√°tico. Tu objetivo es guiar al estudiante a descubrir la respuesta por s√≠ mismo mediante preguntas orientadoras.

‚ö†Ô∏è REGLAS ESTRICTAS - NUNCA VIOLAR:
1. **PROHIBIDO ABSOLUTAMENTE dar c√≥digo de programaci√≥n** (ni completo, ni fragmentos, ni pseudoc√≥digo detallado)
2. **NO des soluciones directas** - Solo haz preguntas que gu√≠en el razonamiento
3. **NO escribas sintaxis de ning√∫n lenguaje** - Solo conceptos y estrategias de alto nivel

Lo que S√ç puedes hacer:
- Hacer preguntas que exploren su comprensi√≥n actual
- Identificar sus suposiciones y ayudarlo a cuestionarlas
- Guiarlo a descomponer el problema conceptualmente
- Sugerir qu√© conceptos te√≥ricos revisar
- Ayudarlo a encontrar la soluci√≥n por s√≠ mismo mediante razonamiento

Si te piden c√≥digo directamente, RECHAZA cort√©smente y redirige con preguntas:
- "En vez de darte el c√≥digo, ay√∫dame a entender: ¬øqu√© intentaste hasta ahora?"
- "¬øQu√© conceptos crees que necesitas aplicar para resolver esto?"
- "Expl√≠came tu plan en lenguaje natural antes de codificar"

S√© breve y preciso. M√°ximo 4-5 preguntas por respuesta."""
            )
        ]
        
        # Agregar historial de conversaci√≥n
        messages.extend(conversation_history)
        
        # Agregar prompt actual
        messages.append(
            LLMMessage(
                role=LLMRole.USER,
                content=f"Pregunta: {prompt}"
            )
        )
        
        try:
            # Log messages sent to LLM (preview)
            logger.info(
                "Sending messages to LLM (socratic)",
                extra={
                    "session_id": session_id,
                    "messages_count": len(messages),
                    "messages_preview": [m.content[:200] for m in messages],
                    "flow_id": flow_id
                }
            )

            llm_started_at = time.perf_counter()
            # Decisi√≥n inteligente de modelo (h√≠brido: keywords + Flash decide)
            model_decision = await self._decide_model_for_prompt(prompt)
            use_pro = (model_decision == "pro")
            
            # Use Flash model for conversational tutoring (unless Pro is needed)
            # FIX Cortez68 (HIGH-004): Add timeout to prevent indefinite hangs
            LLM_TIMEOUT_SECONDS = 30.0
            response = await asyncio.wait_for(
                self.llm.generate(
                    messages,
                    max_tokens=300,
                    temperature=0.7,
                    is_code_analysis=use_pro  # Pro si Flash decidi√≥ que es necesario
                ),
                timeout=LLM_TIMEOUT_SECONDS
            )
            llm_duration_ms = round((time.perf_counter() - llm_started_at) * 1000, 2)

            # Log LLM response metadata and preview
            try:
                usage = response.usage if hasattr(response, 'usage') else None
                logger.info(
                    "LLM response received (socratic)",
                    extra={
                        "session_id": session_id,
                        "model": getattr(response, 'model', None),
                        "usage": usage,
                        "response_preview": response.content[:300],
                        "flow_id": flow_id,
                        "duration_ms": llm_duration_ms
                    }
                )
            except Exception:
                logger.debug("Could not log full LLM response metadata", exc_info=True)

            return response.content.strip()
        except Exception as e:
            # FIX Cortez36: Use lazy logging formatting
            logger.error("LLM generation failed: %s", e, exc_info=True)
            # Circuit Breaker: Fallback cuando Ollama est√° inaccesible
            return self._get_fallback_socratic_response(prompt, flow_id=flow_id)

    async def _decide_model_for_prompt(self, prompt: str) -> str:
        """
        Decide inteligentemente qu√© modelo usar (Flash o Pro)
        
        Enfoque h√≠brido:
        1. Primero: Check r√°pido con keywords (instant√°neo)
        2. Si es obvio que necesita Pro -> usar Pro
        3. Si es obvio que NO necesita Pro -> usar Flash
        4. Si es ambiguo -> preguntarle a Flash que decida
        
        Args:
            prompt: La consulta del usuario
            
        Returns:
            "pro" o "flash" (nombre del modelo a usar)
        """
        # Keywords obvios que requieren Pro (an√°lisis profundo)
        PRO_KEYWORDS = [
            'complejidad', 'complexity', 'big o', 'algoritmo complejo',
            'optimizar algoritmo', 'optimize algorithm', 'refactor',
            'arquitectura', 'architecture', 'dise√±o de sistema',
            'patrones de dise√±o', 'design patterns', 'solid principles',
            'analizar c√≥digo', 'analyze code', 'revisar implementaci√≥n',
            'debugging avanzado', 'advanced debug'
        ]
        
        # Keywords obvios que NO requieren Pro (conversaci√≥n simple)
        FLASH_KEYWORDS = [
            '¬øqu√© es', 'what is', 'expl√≠came', 'explain',
            'hola', 'hello', 'ayuda', 'help',
            'gracias', 'thanks', 'entiendo', 'understand'
        ]
        
        prompt_lower = prompt.lower()
        
        # 1. Check r√°pido: ¬øObviamente necesita Pro?
        if any(keyword in prompt_lower for keyword in PRO_KEYWORDS):
            # FIX Cortez36: Use lazy logging formatting
            logger.info("Quick decision: Using Pro (matched keyword)")
            return "pro"

        # 2. Check r√°pido: ¬øObviamente NO necesita Pro?
        if any(keyword in prompt_lower for keyword in FLASH_KEYWORDS):
            # FIX Cortez36: Use lazy logging formatting
            logger.info("Quick decision: Using Flash (matched simple keyword)")
            return "flash"
        
        # 3. Caso ambiguo: Preguntarle a Flash que analice
        # Solo si es Gemini provider (que soporta analyze_complexity)
        if hasattr(self.llm, 'analyze_complexity'):
            try:
                analysis = await self.llm.analyze_complexity(prompt)
                decision = "pro" if analysis["needs_pro"] else "flash"
                # FIX Cortez68 (HIGH-009): Convert f-string logging to lazy format
                logger.info(
                    "Smart decision by Flash: Using %s",
                    decision,
                    extra={
                        "reason": analysis["reason"],
                        "confidence": analysis["confidence"]
                    }
                )
                return decision
            except Exception as e:
                # FIX Cortez36: Use lazy logging formatting
                logger.warning("Flash analysis failed: %s, defaulting to Flash", e)
                return "flash"
        
        # 4. Fallback: usar Flash por defecto (m√°s econ√≥mico)
        logger.info("Default decision: Using Flash")
        return "flash"

    async def _generate_conceptual_explanation(
        self,
        prompt: str,
        strategy: Dict[str, Any],
        session_id: str = None,
        flow_id: Optional[str] = None
    ) -> str:
        """‚úÖ Genera explicaci√≥n conceptual con memoria de conversaci√≥n"""
        # Recuperar historial de conversaci√≥n si hay session_id
        conversation_history = []
        if session_id and self.trace_repo:
            conversation_history = self._load_conversation_history(session_id)
        
        messages = [
            LLMMessage(
                role=LLMRole.SYSTEM,
                content="""Eres un tutor pedag√≥gico. Explica conceptos fundamentales de manera clara y did√°ctica.

‚ö†Ô∏è REGLA CR√çTICA:
**NUNCA proporciones c√≥digo de programaci√≥n.** Solo explica conceptos, estrategias y razonamientos.

Estructura tu explicaci√≥n:
1. Concepto clave (definici√≥n simple en lenguaje natural)
2. Principio fundamental (por qu√© es importante)
3. Ejemplo conceptual (SIN c√≥digo, solo la idea)
4. Aplicaci√≥n pr√°ctica (c√≥mo pensar el problema, no c√≥mo implementarlo)

Si te preguntan por c√≥digo espec√≠fico:
- Explica el CONCEPTO detr√°s de lo que necesitan
- Describe la ESTRATEGIA de alto nivel
- Sugiere QU√â buscar en la documentaci√≥n
- NO escribas sintaxis de ning√∫n lenguaje

Usa markdown para formato. S√© claro y conciso (m√°ximo 200 palabras)."""
            )
        ]
        
        # Agregar historial
        messages.extend(conversation_history)
        
        # Agregar prompt actual
        messages.append(
            LLMMessage(
                role=LLMRole.USER,
                content=f"Pregunta: {prompt}"
            )
        )
        
        try:
            logger.info(
                "Sending messages to LLM (conceptual)",
                extra={
                    "session_id": session_id,
                    "messages_count": len(messages),
                    "messages_preview": [m.content[:200] for m in messages],
                    "flow_id": flow_id
                }
            )

            llm_started_at = time.perf_counter()
            # Decisi√≥n inteligente: keywords r√°pidos + Flash analiza si es ambiguo
            model_decision = await self._decide_model_for_prompt(prompt)
            use_pro = (model_decision == "pro")

            # FIX Cortez68 (HIGH-004): Add timeout to prevent indefinite hangs
            LLM_TIMEOUT_SECONDS = 30.0
            response = await asyncio.wait_for(
                self.llm.generate(
                    messages,
                    max_tokens=400,
                    temperature=0.7,
                    is_code_analysis=use_pro
                ),
                timeout=LLM_TIMEOUT_SECONDS
            )
            llm_duration_ms = round((time.perf_counter() - llm_started_at) * 1000, 2)

            try:
                usage = response.usage if hasattr(response, 'usage') else None
                logger.info(
                    "LLM response received (conceptual)",
                    extra={
                        "session_id": session_id,
                        "model": getattr(response, 'model', None),
                        "usage": usage,
                        "response_preview": response.content[:300],
                        "flow_id": flow_id,
                        "duration_ms": llm_duration_ms
                    }
                )
            except Exception:
                logger.debug("Could not log full LLM response metadata", exc_info=True)

            return response.content.strip()
        except Exception as e:
            # FIX Cortez36: Use lazy logging formatting
            logger.error("LLM generation failed: %s", e, exc_info=True)
            # Circuit Breaker: Fallback cuando Ollama est√° inaccesible
            return self._get_fallback_conceptual_explanation(prompt, flow_id=flow_id)

    async def _generate_guided_hints(
        self,
        prompt: str,
        strategy: Dict[str, Any],
        session_id: str = None,
        flow_id: Optional[str] = None
    ) -> str:
        """‚úÖ Genera pistas guiadas con memoria de conversaci√≥n"""
        # Recuperar historial de conversaci√≥n si hay session_id
        conversation_history = []
        if session_id and self.trace_repo:
            conversation_history = self._load_conversation_history(session_id)
        
        messages = [
            LLMMessage(
                role=LLMRole.SYSTEM,
                content="""Eres un tutor que da pistas graduadas. NO des la soluci√≥n completa ni c√≥digo.

‚ö†Ô∏è PROHIBIDO ESTRICTAMENTE:
- Escribir c√≥digo funcional (ni completo, ni parcial)
- Dar pseudoc√≥digo detallado con sintaxis
- Proporcionar implementaciones directas

Proporciona 3-4 pistas CONCEPTUALES que:
1. Sugieran c√≥mo descomponer el problema (en ideas, no en c√≥digo)
2. Mencionen conceptos/estructuras relevantes (nombres, no sintaxis)
3. Indiquen casos a considerar (situaciones, no implementaci√≥n)
4. Sugieran un pr√≥ximo paso de RAZONAMIENTO (pensar, no codificar)

Ejemplo de pista BUENA:
‚úÖ "Pens√° en qu√© estructura de datos te permite acceso r√°pido por clave"

Ejemplo de pista MALA (NO hacer):
‚ùå "Us√° un HashMap y hac√©: map.put(key, value)"

Cada pista debe acercar al estudiante a la soluci√≥n conceptual sin d√°rsela directamente."""
            )
        ]
        
        # Agregar historial
        messages.extend(conversation_history)
        
        # Agregar prompt actual
        messages.append(
            LLMMessage(
                role=LLMRole.USER,
                content=f"Pregunta: {prompt}"
            )
        )
        
        try:
            logger.info(
                "Sending messages to LLM (guided_hints)",
                extra={
                    "session_id": session_id,
                    "messages_count": len(messages),
                    "messages_preview": [m.content[:200] for m in messages],
                    "flow_id": flow_id
                }
            )

            llm_started_at = time.perf_counter()
            # Decisi√≥n inteligente de modelo
            model_decision = await self._decide_model_for_prompt(prompt)
            use_pro = (model_decision == "pro")

            # FIX Cortez68 (HIGH-004): Add timeout to prevent indefinite hangs
            LLM_TIMEOUT_SECONDS = 30.0
            response = await asyncio.wait_for(
                self.llm.generate(
                    messages,
                    max_tokens=350,
                    temperature=0.7,
                    is_code_analysis=use_pro
                ),
                timeout=LLM_TIMEOUT_SECONDS
            )
            llm_duration_ms = round((time.perf_counter() - llm_started_at) * 1000, 2)

            try:
                usage = response.usage if hasattr(response, 'usage') else None
                logger.info(
                    "LLM response received (guided_hints)",
                    extra={
                        "session_id": session_id,
                        "model": getattr(response, 'model', None),
                        "usage": usage,
                        "response_preview": response.content[:300],
                        "flow_id": flow_id,
                        "duration_ms": llm_duration_ms
                    }
                )
            except Exception:
                logger.debug("Could not log full LLM response metadata", exc_info=True)

            return response.content.strip()
        except Exception as e:
            # FIX Cortez36: Use lazy logging formatting
            logger.error("LLM generation failed: %s", e, exc_info=True)
            # Circuit Breaker: Fallback cuando Ollama est√° inaccesible
            return self._get_fallback_guided_hints(prompt, flow_id=flow_id)

    # ============================================================
    # NUEVOS M√âTODOS DE GENERACI√ìN DE RESPUESTA (FIX Cortez64)
    # ============================================================

    async def _generate_empathetic_support(
        self,
        prompt: str,
        strategy: Dict[str, Any],
        session_id: str = None,
        flow_id: Optional[str] = None
    ) -> str:
        """
        FIX Cortez64: Genera respuesta emp√°tica para estudiantes frustrados.

        Reconoce la frustraci√≥n, normaliza la dificultad, y ofrece un enfoque
        fresco con pistas m√°s directas para desbloquear.
        """
        conversation_history = []
        if session_id and self.trace_repo:
            conversation_history = self._load_conversation_history(session_id)

        messages = [
            LLMMessage(
                role=LLMRole.SYSTEM,
                content="""Eres un tutor emp√°tico y comprensivo. El estudiante est√° frustrado o atascado.

üéØ TU OBJETIVO: Reconocer su frustraci√≥n, normalizarla, y ayudarlo a desbloquear.

‚úÖ LO QUE DEBES HACER:
1. **Reconocer la frustraci√≥n** - "Entiendo que esto es frustrante..." / "Es normal sentirse trabado..."
2. **Normalizar la dificultad** - "Este es un problema que toma tiempo..." / "Todos nos trabamos con esto al principio..."
3. **Ofrecer perspectiva fresca** - Sugerir mirar el problema desde otro √°ngulo
4. **Dar pistas m√°s directas** - Ser m√°s espec√≠fico de lo usual para ayudar a desbloquear
5. **Sugerir pasos peque√±os** - Dividir en partes manejables
6. **Mantener tono alentador** - "Ya avanzaste en X, ahora veamos Y..."

‚ö†Ô∏è PROHIBIDO:
- Dar c√≥digo completo (pero s√≠ puedes ser m√°s espec√≠fico con conceptos)
- Minimizar su frustraci√≥n ("es f√°cil", "no es para tanto")
- Dar la soluci√≥n directa

üí° Puedes ser m√°s generoso con las pistas que en modo normal,
pero sin dar la soluci√≥n completa.

S√© c√°lido, paciente y constructivo. M√°ximo 200 palabras."""
            )
        ]

        messages.extend(conversation_history)
        messages.append(
            LLMMessage(
                role=LLMRole.USER,
                content=f"Estudiante frustrado dice: {prompt}"
            )
        )

        try:
            logger.info(
                "Sending messages to LLM (empathetic_support)",
                extra={
                    "session_id": session_id,
                    "messages_count": len(messages),
                    "flow_id": flow_id
                }
            )

            llm_started_at = time.perf_counter()
            # FIX Cortez68 (HIGH-004): Add timeout to prevent indefinite hangs
            LLM_TIMEOUT_SECONDS = 30.0
            response = await asyncio.wait_for(
                self.llm.generate(
                    messages,
                    max_tokens=400,
                    temperature=0.8  # Un poco m√°s de variedad para empat√≠a
                ),
                timeout=LLM_TIMEOUT_SECONDS
            )
            llm_duration_ms = round((time.perf_counter() - llm_started_at) * 1000, 2)

            logger.info(
                "LLM response received (empathetic_support)",
                extra={
                    "session_id": session_id,
                    "flow_id": flow_id,
                    "duration_ms": llm_duration_ms
                }
            )

            return response.content.strip()
        except Exception as e:
            logger.error("LLM generation failed (empathetic_support): %s", e, exc_info=True)
            return self._get_fallback_empathetic_support(prompt, flow_id=flow_id)

    async def _generate_metacognitive_guidance(
        self,
        prompt: str,
        strategy: Dict[str, Any],
        session_id: str = None,
        flow_id: Optional[str] = None
    ) -> str:
        """
        FIX Cortez64: Genera gu√≠a metacognitiva para estudiantes que preguntan
        sobre c√≥mo pensar o encarar un problema.
        """
        conversation_history = []
        if session_id and self.trace_repo:
            conversation_history = self._load_conversation_history(session_id)

        messages = [
            LLMMessage(
                role=LLMRole.SYSTEM,
                content="""Eres un tutor especializado en metacognici√≥n y estrategias de aprendizaje.
El estudiante pregunta sobre C√ìMO PENSAR o C√ìMO ENCARAR un problema.

üéØ TU OBJETIVO: Ense√±ar el PROCESO de pensamiento, no el contenido.

‚úÖ LO QUE DEBES HACER:
1. **Estrategia de descomposici√≥n** - "Primero, identifiquemos qu√© sabemos y qu√© nos piden..."
2. **Plan de acci√≥n** - "Un buen enfoque ser√≠a: 1) entender el input, 2) definir el output..."
3. **T√©cnicas de resoluci√≥n** - Mencionar patrones como "dividir y conquistar", "casos base"
4. **Preguntas de autoevaluaci√≥n** - "Preguntate: ¬øqu√© pasa si el input est√° vac√≠o?"
5. **Orden de pasos** - Dar una secuencia l√≥gica para abordar problemas similares

‚ö†Ô∏è PROHIBIDO:
- Dar c√≥digo o pseudoc√≥digo detallado
- Resolver el problema espec√≠fico directamente

üí° Ense√±ale a PESCAR, no le des el pescado.

Estructura tu respuesta como un plan de acci√≥n numerado.
M√°ximo 250 palabras."""
            )
        ]

        messages.extend(conversation_history)
        messages.append(
            LLMMessage(
                role=LLMRole.USER,
                content=f"Estudiante pregunta sobre estrategia: {prompt}"
            )
        )

        try:
            logger.info(
                "Sending messages to LLM (metacognitive_guidance)",
                extra={
                    "session_id": session_id,
                    "messages_count": len(messages),
                    "flow_id": flow_id
                }
            )

            llm_started_at = time.perf_counter()
            # FIX Cortez68 (HIGH-004): Add timeout to prevent indefinite hangs
            LLM_TIMEOUT_SECONDS = 30.0
            response = await asyncio.wait_for(
                self.llm.generate(
                    messages,
                    max_tokens=450,
                    temperature=0.6  # M√°s estructurado
                ),
                timeout=LLM_TIMEOUT_SECONDS
            )
            llm_duration_ms = round((time.perf_counter() - llm_started_at) * 1000, 2)

            logger.info(
                "LLM response received (metacognitive_guidance)",
                extra={
                    "session_id": session_id,
                    "flow_id": flow_id,
                    "duration_ms": llm_duration_ms
                }
            )

            return response.content.strip()
        except Exception as e:
            logger.error("LLM generation failed (metacognitive_guidance): %s", e, exc_info=True)
            return self._get_fallback_metacognitive_guidance(prompt, flow_id=flow_id)

    async def _generate_example_based(
        self,
        prompt: str,
        strategy: Dict[str, Any],
        session_id: str = None,
        flow_id: Optional[str] = None
    ) -> str:
        """
        FIX Cortez64: Genera respuesta basada en ejemplos an√°logos.

        Proporciona un ejemplo similar pero diferente al problema actual,
        guiando la transferencia del conocimiento.
        """
        conversation_history = []
        if session_id and self.trace_repo:
            conversation_history = self._load_conversation_history(session_id)

        messages = [
            LLMMessage(
                role=LLMRole.SYSTEM,
                content="""Eres un tutor que ense√±a mediante ejemplos an√°logos.
El estudiante pidi√≥ un ejemplo para entender mejor.

üéØ TU OBJETIVO: Dar un ejemplo SIMILAR pero NO ID√âNTICO al problema.

‚úÖ LO QUE DEBES HACER:
1. **Ejemplo an√°logo** - Usar un caso diferente pero con la misma estructura
   - Si pregunta sobre ordenar n√∫meros, dar ejemplo de ordenar palabras alfab√©ticamente
   - Si pregunta sobre listas, dar ejemplo con otra colecci√≥n

2. **Explicar el ejemplo** - Paso a paso, conceptualmente
3. **Preguntas de transferencia** - "¬øVes c√≥mo esto se aplica a tu problema?"
4. **Conexi√≥n expl√≠cita** - Ayudar a ver el paralelo con su problema

‚ö†Ô∏è PROHIBIDO:
- Dar la soluci√≥n directa a SU problema
- Dar c√≥digo copiable para su ejercicio espec√≠fico

üí° El ejemplo debe iluminar el PATR√ìN, no resolver el problema.

Ejemplo de estructura:
"Imagin√° que en lugar de [su problema] ten√©s [problema an√°logo]...
En ese caso, pensar√≠as en... [explicaci√≥n]
Ahora, ¬øc√≥mo aplicar√≠as esta misma idea a tu problema?"

M√°ximo 300 palabras."""
            )
        ]

        messages.extend(conversation_history)
        messages.append(
            LLMMessage(
                role=LLMRole.USER,
                content=f"Estudiante pide ejemplo: {prompt}"
            )
        )

        try:
            logger.info(
                "Sending messages to LLM (example_based)",
                extra={
                    "session_id": session_id,
                    "messages_count": len(messages),
                    "flow_id": flow_id
                }
            )

            llm_started_at = time.perf_counter()
            # FIX Cortez68 (HIGH-004): Add timeout to prevent indefinite hangs
            LLM_TIMEOUT_SECONDS = 30.0
            response = await asyncio.wait_for(
                self.llm.generate(
                    messages,
                    max_tokens=500,
                    temperature=0.7
                ),
                timeout=LLM_TIMEOUT_SECONDS
            )
            llm_duration_ms = round((time.perf_counter() - llm_started_at) * 1000, 2)

            logger.info(
                "LLM response received (example_based)",
                extra={
                    "session_id": session_id,
                    "flow_id": flow_id,
                    "duration_ms": llm_duration_ms
                }
            )

            return response.content.strip()
        except Exception as e:
            logger.error("LLM generation failed (example_based): %s", e, exc_info=True)
            return self._get_fallback_example_based(prompt, flow_id=flow_id)

    async def _generate_clarification_request(
        self,
        prompt: str,
        strategy: Dict[str, Any],
        session_id: str = None,
        flow_id: Optional[str] = None
    ) -> str:
        """
        FIX Cortez64: Solicita clarificaci√≥n cuando el prompt es ambiguo.
        Actualizado a async con LLM para respuestas m√°s naturales.
        """
        conversation_history = []
        if session_id and self.trace_repo:
            conversation_history = self._load_conversation_history(session_id)

        messages = [
            LLMMessage(
                role=LLMRole.SYSTEM,
                content="""Eres un tutor que necesita m√°s informaci√≥n para ayudar.
El mensaje del estudiante es ambiguo o le falta contexto.

üéØ TU OBJETIVO: Pedir clarificaci√≥n de manera amable y espec√≠fica.

‚úÖ LO QUE DEBES HACER:
1. Reconocer que quer√©s ayudar
2. Explicar qu√© informaci√≥n te falta
3. Hacer preguntas espec√≠ficas:
   - ¬øQu√© parte exacta no entend√©s?
   - ¬øQu√© intentaste hasta ahora?
   - ¬øPod√©s mostrar tu c√≥digo actual?
   - ¬øQu√© resultado esperabas vs. qu√© obtuviste?

S√© amable y espec√≠fico. M√°ximo 100 palabras."""
            )
        ]

        messages.extend(conversation_history)
        messages.append(
            LLMMessage(
                role=LLMRole.USER,
                content=f"Mensaje ambiguo: {prompt}"
            )
        )

        try:
            logger.info(
                "Sending messages to LLM (clarification_request)",
                extra={
                    "session_id": session_id,
                    "messages_count": len(messages),
                    "flow_id": flow_id
                }
            )

            # FIX Cortez68 (HIGH-004): Add timeout to prevent indefinite hangs
            LLM_TIMEOUT_SECONDS = 30.0
            response = await asyncio.wait_for(
                self.llm.generate(
                    messages,
                    max_tokens=200,
                    temperature=0.5
                ),
                timeout=LLM_TIMEOUT_SECONDS
            )

            return response.content.strip()
        except Exception as e:
            logger.error("LLM generation failed (clarification_request): %s", e, exc_info=True)
            return self._get_fallback_clarification_request(prompt, flow_id=flow_id)

    # ============================================================
    # FALLBACKS PARA NUEVOS TIPOS (FIX Cortez64)
    # ============================================================

    def _get_fallback_empathetic_support(self, prompt: str, flow_id: Optional[str] = None) -> str:
        """Fallback para respuesta emp√°tica cuando LLM no est√° disponible."""
        return """Entiendo que te sent√≠s frustrado/a, y es completamente normal.
Todos nos trabamos en alg√∫n momento cuando aprendemos a programar.

Tomemos un respiro y miremos esto desde otro √°ngulo:

1. **¬øQu√© parte S√ç entend√©s del problema?** Empez√° por ah√≠.
2. **¬øPod√©s dividirlo en partes m√°s peque√±as?** A veces ayuda resolver pedacitos.
3. **¬øQu√© fue lo √∫ltimo que funcion√≥?** Volvamos a ese punto.

No te rindas. Cada error es un paso hacia la soluci√≥n.
¬øPor d√≥nde te gustar√≠a empezar de nuevo?"""

    def _get_fallback_metacognitive_guidance(self, prompt: str, flow_id: Optional[str] = None) -> str:
        """Fallback para gu√≠a metacognitiva cuando LLM no est√° disponible."""
        return """Para encarar este tipo de problemas, te sugiero seguir estos pasos:

**Plan de Acci√≥n:**

1. **Entender el problema**
   - ¬øQu√© datos de entrada ten√©s?
   - ¬øQu√© resultado necesit√°s obtener?

2. **Identificar lo que sab√©s**
   - ¬øQu√© conceptos ya conoc√©s que podr√≠an servir?
   - ¬øResolviste algo similar antes?

3. **Descomponer en pasos**
   - ¬øCu√°l ser√≠a el primer paso m√°s peque√±o?
   - ¬øQu√© necesit√°s resolver primero?

4. **Probar con ejemplos simples**
   - ¬øQu√© pasar√≠a con un caso muy sencillo?
   - ¬øY con un caso borde (vac√≠o, un solo elemento)?

¬øPod√©s empezar describiendo qu√© entend√©s del problema en tus palabras?"""

    def _get_fallback_example_based(self, prompt: str, flow_id: Optional[str] = None) -> str:
        """Fallback para respuesta basada en ejemplos cuando LLM no est√° disponible."""
        return """Te doy un ejemplo an√°logo para que veas el patr√≥n:

**Problema similar:** Imagin√° que ten√©s que ordenar cartas de un mazo.

- Primero, necesit√°s un criterio de orden (¬øpor n√∫mero? ¬øpor palo?)
- Luego, compar√°s de a pares para decidir cu√°l va primero
- Repet√≠s hasta que todo est√© ordenado

**La pregunta clave:** ¬øC√≥mo decid√≠s si un elemento va antes que otro?

Ahora pens√° en tu problema:
- ¬øCu√°l es tu "criterio de orden"?
- ¬øC√≥mo comparar√≠as dos elementos?

¬øVes la conexi√≥n entre este ejemplo y tu problema?"""

    def _get_fallback_clarification_request(self, prompt: str, flow_id: Optional[str] = None) -> str:
        """Fallback para solicitud de clarificaci√≥n cuando LLM no est√° disponible."""
        return """Para poder ayudarte mejor, necesito que seas m√°s espec√≠fico:

- ¬øQu√© parte exacta del problema te genera dificultad?
- ¬øQu√© intentaste hasta ahora?
- ¬øQu√© resultado esperabas vs. qu√© obtuviste?

Si ten√©s c√≥digo, compartilo para poder ver d√≥nde est√° el problema.
Por favor, reformul√° tu pregunta con m√°s detalles."""

    # FIX Cortez22 DEFECTO 1.1: Make async for future LLM integration
    async def _process_simulator_mode(
        self,
        session_id: str,
        prompt: str,
        strategy: Dict[str, Any],
        classification: Dict[str, Any],
        flow_id: Optional[str] = None
    ) -> Dict[str, Any]:
        """Procesa interacci√≥n en modo S-IA-X (Simulador)"""
        # Placeholder para simuladores
        # FIX Cortez22 DEFECTO 1.8: Use "response" not "message"
        logger.info(
            "Processing simulator mode",
            extra={
                "session_id": session_id,
                "flow_id": flow_id,
                "strategy_keys": sorted(strategy.keys()) if isinstance(strategy, dict) else []
            }
        )
        return {
            "response": "[Modo Simulador - En desarrollo]",
            "mode": "simulator",
            "metadata": {}
        }

    # FIX Cortez22 DEFECTO 1.1: Make async for future LLM integration
    async def _process_evaluator_mode(
        self,
        session_id: str,
        prompt: str,
        strategy: Dict[str, Any],
        classification: Dict[str, Any],
        flow_id: Optional[str] = None
    ) -> Dict[str, Any]:
        """Procesa interacci√≥n en modo E-IA-Proc (Evaluador)"""
        # Placeholder para evaluador
        # FIX Cortez22 DEFECTO 1.8: Use "response" not "message"
        logger.info(
            "Processing evaluator mode",
            extra={
                "session_id": session_id,
                "flow_id": flow_id,
                "strategy_keys": sorted(strategy.keys()) if isinstance(strategy, dict) else []
            }
        )
        return {
            "response": "[Modo Evaluador - En desarrollo]",
            "mode": "evaluator",
            "metadata": {}
        }

    def _create_trace(
        self,
        session_id: str,
        student_id: str,
        activity_id: str,
        interaction_type: InteractionType,
        content: str,
        level: TraceLevel,
        **kwargs
    ) -> CognitiveTrace:
        """Crea una traza cognitiva (no la persiste a√∫n)"""
        trace_id = str(uuid.uuid4())

        return CognitiveTrace(
            id=trace_id,
            session_id=session_id,
            student_id=student_id,
            activity_id=activity_id,
            trace_level=level,
            interaction_type=interaction_type,
            content=content,
            **kwargs
        )

    def _persist_trace(self, trace: CognitiveTrace) -> None:
        """Persiste una traza en BD (STATELESS)"""
        if self.trace_repo is not None:
            try:
                db_trace = self.trace_repo.create(trace)
                logger.debug(
                    "Trace persisted successfully",
                    extra={
                        "trace_id": db_trace.id,
                        "interaction_type": trace.interaction_type.value,
                        "session_id": trace.session_id,
                        "cognitive_state": trace.cognitive_state.value if trace.cognitive_state else None
                    }
                )
                # ‚úÖ HIGH-01: Record Prometheus metric for trace creation
                metrics = _get_metrics()
                if metrics:
                    metrics.record_trace_creation(
                        trace_level=trace.trace_level.value if hasattr(trace.trace_level, 'value') else str(trace.trace_level),
                        interaction_type=trace.interaction_type.value if hasattr(trace.interaction_type, 'value') else str(trace.interaction_type)
                    )
            except Exception as e:
                logger.error(
                    "Failed to persist trace",
                    extra={
                        "error": str(e),
                        "session_id": trace.session_id,
                        "interaction_type": trace.interaction_type.value
                    },
                    exc_info=True
                )
        else:
            logger.warning(
                "Trace repository is None, cannot persist trace",
                extra={
                    "session_id": trace.session_id,
                    "interaction_type": trace.interaction_type.value
                }
            )
        # Si no hay repo, no hacer nada (backward compatibility)

    def _load_conversation_history(
        self,
        session_id: str,
        max_messages: int = 50  # FIX Cortez22 DEFECTO 1.7: Add limit parameter
    ) -> List[LLMMessage]:
        """
        ‚úÖ NUEVO: Carga el historial de conversaci√≥n de esta sesi√≥n como mensajes LLM.

        Recupera las trazas m√°s recientes de la sesi√≥n y las convierte al formato de mensajes
        que espera el LLM provider, manteniendo el contexto de la conversaci√≥n.

        FIX Cortez22 DEFECTO 1.7: Added limit to prevent OOM in long sessions

        Args:
            session_id: ID de la sesi√≥n actual
            max_messages: L√≠mite m√°ximo de mensajes a retornar (default: 50)

        Returns:
            Lista de LLMMessage con el historial formateado (√∫ltimos max_messages)
        """
        if self.trace_repo is None:
            logger.warning("No trace repository available for conversation history")
            return []

        try:
            # FIX Cortez22 DEFECTO 1.7: Add limit=100 to prevent loading ALL traces
            # This prevents OOM in sessions with thousands of interactions
            db_traces = self.trace_repo.get_by_session(session_id, limit=100)

            messages = []
            for trace in db_traces:
                # Agregar mensaje del usuario (STUDENT_PROMPT)
                if trace.interaction_type == InteractionType.STUDENT_PROMPT.value and trace.content:
                    messages.append(
                        LLMMessage(
                            role=LLMRole.USER,
                            content=trace.content
                        )
                    )

                # Agregar respuesta del asistente (AI_RESPONSE o TUTOR_INTERVENTION)
                elif trace.interaction_type in [
                    InteractionType.AI_RESPONSE.value,
                    InteractionType.TUTOR_INTERVENTION.value
                ] and trace.content:
                    messages.append(
                        LLMMessage(
                            role=LLMRole.ASSISTANT,
                            content=trace.content
                        )
                    )

            # FIX Cortez22 DEFECTO 1.7: Limit to last N messages to prevent LLM token explosion
            if len(messages) > max_messages:
                messages = messages[-max_messages:]
                logger.info(
                    f"Truncated conversation history to last {max_messages} messages",
                    extra={"session_id": session_id, "original_count": len(messages) + (len(messages) - max_messages)}
                )

            logger.info(
                f"Loaded conversation history: {len(messages)} messages",
                extra={"session_id": session_id}
            )
            return messages
            
        except Exception as e:
            logger.error(
                f"Error loading conversation history: {e}",
                exc_info=True,
                extra={"session_id": session_id}
            )
            return []
    
    def _get_student_history(
        self,
        student_id: str,
        activity_id: Optional[str] = None
    ) -> List[CognitiveTrace]:
        """Obtiene el historial de trazas del estudiante desde BD (STATELESS)"""
        if self.trace_repo is None:
            return []  # Backward compatibility

        # ‚úÖ STATELESS: Leer desde BD
        db_traces = self.trace_repo.get_by_student(student_id, limit=100)

        # Convertir de ORM a Pydantic
        traces = []
        for db_trace in db_traces:
            try:
                trace = CognitiveTrace(
                    id=db_trace.id,
                    session_id=db_trace.session_id,
                    student_id=db_trace.student_id,
                    activity_id=db_trace.activity_id,
                    trace_level=TraceLevel(db_trace.trace_level),
                    interaction_type=InteractionType(db_trace.interaction_type),
                    content=db_trace.content or "",
                    context=db_trace.context or {},
                    metadata=db_trace.trace_metadata or {},
                    cognitive_state=db_trace.cognitive_state,
                    ai_involvement=db_trace.ai_involvement or 0.5,
                )
                traces.append(trace)
            except Exception as e:
                # Skip invalid traces pero log the error
                logger.warning(
                    f"Failed to convert database trace to Pydantic model: {type(e).__name__}: {str(e)}",
                    exc_info=True,
                    extra={
                        "trace_id": db_trace.id if hasattr(db_trace, 'id') else 'unknown',
                        "session_id": db_trace.session_id if hasattr(db_trace, 'session_id') else 'unknown',
                        "student_id": student_id
                    }
                )
                continue

        # Filtrar por activity_id si se especific√≥
        if activity_id:
            traces = [t for t in traces if t.activity_id == activity_id]

        return traces

    def _create_risk(
        self,
        session_id: str,
        student_id: str,
        activity_id: str,
        risk_type: RiskType,
        risk_level: RiskLevel,
        dimension: RiskDimension,
        description: str,
        evidence: List[str],
        trace_ids: List[str],
        **kwargs
    ) -> Risk:
        """Crea un objeto Risk (sin persistirlo a√∫n)"""
        return Risk(
            id=str(uuid.uuid4()),
            session_id=session_id,
            student_id=student_id,
            activity_id=activity_id,
            risk_type=risk_type,
            risk_level=risk_level,
            dimension=dimension,
            description=description,
            evidence=evidence,
            trace_ids=trace_ids,
            **kwargs
        )

    def _persist_risk(
        self,
        session_id: str,
        student_id: str,
        activity_id: str,
        risk_type: RiskType,
        risk_level: RiskLevel,
        dimension: RiskDimension,
        description: str,
        evidence: List[str],
        trace_ids: List[str],
        flow_id: Optional[str] = None,
        risk_repo_override: Optional[RiskRepositoryProtocol] = None,
        **kwargs
    ) -> Optional[Risk]:
        """Registra un riesgo detectado en BD (STATELESS)"""
        risk = Risk(
            id=str(uuid.uuid4()),
            session_id=session_id,
            student_id=student_id,
            activity_id=activity_id,
            risk_type=risk_type,
            risk_level=risk_level,
            dimension=dimension,
            description=description,
            evidence=evidence,
            trace_ids=trace_ids,
            **kwargs
        )

        repo = risk_repo_override or self.risk_repo

        # ‚úÖ STATELESS: Persistir en BD
        if repo is not None:
            try:
                repo.create(risk)
                # ‚úÖ Structured logging: Risk persisted successfully
                logger.info(
                    "Risk persisted to database",
                    extra={
                        "risk_id": risk.id,
                        "risk_type": risk_type.value,
                        "risk_level": risk_level.value,
                        "dimension": dimension.value,
                        "student_id": student_id,
                        "activity_id": activity_id,
                        "trace_count": len(trace_ids),
                        "flow_id": flow_id
                    }
                )
                # ‚úÖ HIGH-01: Record Prometheus metric for risk detection
                metrics = _get_metrics()
                if metrics:
                    metrics.record_risk_detection(
                        risk_type=risk_type.value,
                        risk_level=risk_level.value,
                        dimension=dimension.value
                    )
            except Exception as e:
                # ‚úÖ Structured logging: Risk persistence failed
                logger.error(
                    f"Failed to persist risk to database: {type(e).__name__}: {str(e)}",
                    exc_info=True,
                    extra={
                        "risk_id": risk.id,
                        "risk_type": risk_type.value,
                        "risk_level": risk_level.value,
                        "student_id": student_id,
                        "activity_id": activity_id,
                        "flow_id": flow_id
                    }
                )
                # Re-raise to maintain error propagation
                raise
        else:
            # ‚úÖ Structured logging: No repository available
            logger.warning(
                "Risk repository is None, cannot persist risk",
                extra={
                    "risk_id": risk.id,
                    "risk_type": risk_type.value,
                    "risk_level": risk_level.value,
                    "student_id": student_id,
                    "flow_id": flow_id
                }
            )

        return risk

    def _persist_risk_object(
        self,
        risk: Risk,
        flow_id: Optional[str] = None,
        risk_repo_override: Optional[RiskRepositoryProtocol] = None,
    ) -> None:
        """
        Persiste un objeto Risk existente en BD (STATELESS)

        Args:
            risk: Objeto Risk ya creado que se debe persistir
        """
        repo = risk_repo_override or self.risk_repo
        if repo is not None:
            try:
                repo.create(risk)
                logger.info(
                    "Risk persisted to database",
                    extra={
                        "risk_id": risk.id,
                        "risk_type": risk.risk_type.value,
                        "risk_level": risk.risk_level.value,
                        "dimension": risk.dimension.value,
                        "session_id": risk.session_id,
                        "flow_id": flow_id
                    }
                )
                # Record Prometheus metric for risk detection
                metrics = _get_metrics()
                if metrics:
                    metrics.record_risk_detection(
                        risk_type=risk.risk_type.value,
                        risk_level=risk.risk_level.value,
                        dimension=risk.dimension.value
                    )
            except Exception as e:
                logger.error(
                    f"Failed to persist risk to database: {type(e).__name__}: {str(e)}",
                    exc_info=True,
                    extra={
                        "risk_id": risk.id,
                        "risk_type": risk.risk_type.value,
                        "session_id": risk.session_id,
                        "flow_id": flow_id
                    }
                )
        else:
            logger.warning(
                "Risk repository is None, cannot persist risk",
                extra={"risk_id": risk.id, "risk_type": risk.risk_type.value, "flow_id": flow_id}
            )

    def _analyze_risks_async(
        self,
        session_id: str,
        input_trace: CognitiveTrace,
        response_trace: CognitiveTrace,
        classification: Dict[str, Any],
        flow_id: Optional[str] = None,
        risk_repo_override: Optional[RiskRepositoryProtocol] = None,
    ) -> None:
        """
        An√°lisis de riesgos as√≠ncrono (AR-IA)

        Realiza an√°lisis b√°sico de riesgos a partir de las trazas de entrada y salida.
        En el MVP ejecuta sincr√≥nicamente, en producci√≥n ser√≠a async (celery/rq).

        Args:
            session_id: ID de la sesi√≥n
            input_trace: Traza de entrada (prompt del estudiante)
            response_trace: Traza de salida (respuesta del agente)
            classification: Clasificaci√≥n del prompt (tipo, delegaci√≥n, etc.)

        Detecta:
        - RC1: Delegaci√≥n total (solicitudes de c√≥digo completo)
        - RC2: Dependencia excesiva de IA (alto ai_involvement)
        - RC3: Falta de justificaci√≥n (decisiones sin explicaci√≥n)
        - RE1: Integridad acad√©mica (uso no divulgado de IA)
        - REp1: Aceptaci√≥n acr√≠tica (no cuestiona respuestas de IA)
        """
        logger.info(
            "Starting risk analysis",
            extra={
                "session_id": session_id,
                "input_trace_id": input_trace.id,
                "response_trace_id": response_trace.id,
                "classification_type": classification.get("type", "unknown"),
                "is_total_delegation": classification.get("is_total_delegation", False),
                "ai_involvement": input_trace.ai_involvement,
                "flow_id": flow_id
            }
        )

        # Skip if no repositories available (backward compatibility)
        if self.risk_repo is None and risk_repo_override is None:
            logger.debug(
                "Risk repository not available, skipping risk analysis",
                extra={"flow_id": flow_id, "session_id": session_id}
            )
            return

        detected_risks = []

        # === RC1: Delegaci√≥n Total ===
        # NOTE: cognitive_engine returns "is_total_delegation", not "is_delegation"
        if classification.get("is_total_delegation", False):
            delegation_signals = classification.get("delegation_signals", [])
            risk = self._create_risk(
                session_id=session_id,
                student_id=input_trace.student_id,
                activity_id=input_trace.activity_id,
                risk_type=RiskType.COGNITIVE_DELEGATION,
                risk_level=RiskLevel.HIGH,
                dimension=RiskDimension.COGNITIVE,
                description=(
                    "Intento de delegaci√≥n total detectado. El estudiante solicita "
                    "soluciones completas sin descomposici√≥n del problema."
                ),
                evidence=[
                    input_trace.content[:200],  # First 200 chars of prompt
                    f"Se√±ales: {', '.join(delegation_signals[:3])}"
                ],
                trace_ids=[input_trace.id, response_trace.id],
                root_cause="Tendencia a delegar la resoluci√≥n completa a la IA sin esfuerzo propio",
                recommendations=[
                    "Solicitar descomposici√≥n expl√≠cita del problema en subtareas",
                    "Exigir justificaci√≥n de cada paso antes de implementar",
                    "Reducir nivel de ayuda del tutor temporalmente",
                    "Asignar ejercicios similares sin acceso a IA"
                ],
                pedagogical_intervention=(
                    "Activar modo socr√°tico estricto: solo preguntas gu√≠a, sin pistas directas"
                )
            )
            detected_risks.append(risk)
            self._persist_risk_object(
                risk,
                flow_id=flow_id,
                risk_repo_override=risk_repo_override,
            )

        # === RC2: Dependencia Excesiva de IA ===
        # Import constant to avoid hardcoded value
        from .constants import AI_DEPENDENCY_MEDIUM_THRESHOLD
        if input_trace.ai_involvement > AI_DEPENDENCY_MEDIUM_THRESHOLD:
            risk = self._create_risk(
                session_id=session_id,
                student_id=input_trace.student_id,
                activity_id=input_trace.activity_id,
                risk_type=RiskType.AI_DEPENDENCY,
                risk_level=RiskLevel.MEDIUM,
                dimension=RiskDimension.COGNITIVE,
                description=(
                    f"Nivel de dependencia de IA elevado: {input_trace.ai_involvement:.1%}. "
                    "El estudiante depende excesivamente de la asistencia de IA."
                ),
                evidence=[f"AI involvement: {input_trace.ai_involvement:.2%}"],
                trace_ids=[input_trace.id],
                recommendations=[
                    "Fomentar resoluci√≥n aut√≥noma con menos asistencia de IA",
                    "Asignar ejercicios sin acceso a IA para desarrollar autonom√≠a",
                    "Revisar interacciones previas para identificar patr√≥n de dependencia"
                ]
            )
            detected_risks.append(risk)
            self._persist_risk_object(
                risk,
                flow_id=flow_id,
                risk_repo_override=risk_repo_override,
            )

        # === RC3: Falta de Justificaci√≥n ===
        has_justification = (
            input_trace.decision_justification is not None and
            len(input_trace.decision_justification.strip()) > 20
        )
        if not has_justification and not classification.get("is_question", False):
            # Solo detectar si NO es una pregunta conceptual (donde no se espera justificaci√≥n)
            risk = self._create_risk(
                session_id=session_id,
                student_id=input_trace.student_id,
                activity_id=input_trace.activity_id,
                risk_type=RiskType.LACK_JUSTIFICATION,
                risk_level=RiskLevel.LOW,
                dimension=RiskDimension.COGNITIVE,
                description=(
                    "El estudiante no proporciona justificaci√≥n de sus decisiones o razonamiento. "
                    "Esto dificulta evaluar su proceso cognitivo."
                ),
                evidence=["No se detect√≥ campo decision_justification"],
                trace_ids=[input_trace.id],
                recommendations=[
                    "Exigir explicitaci√≥n del razonamiento en cada interacci√≥n",
                    "Solicitar que explique 'por qu√©' eligi√≥ cierto enfoque",
                    "Usar prompts estructurados que incluyan secci√≥n de justificaci√≥n"
                ]
            )
            detected_risks.append(risk)
            self._persist_risk_object(
                risk,
                flow_id=flow_id,
                risk_repo_override=risk_repo_override,
            )

        # === REp1: Aceptaci√≥n Acr√≠tica ===
        # Detectar si el estudiante acepta respuestas sin cuestionarlas
        # (Simplificado: si ai_critiques es 0 y hay m√∫ltiples interacciones)
        if hasattr(response_trace, 'alternatives_considered'):
            alternatives = response_trace.alternatives_considered or []
            if len(alternatives) == 0 and input_trace.ai_involvement > 0.5:
                risk = self._create_risk(
                    session_id=session_id,
                    student_id=input_trace.student_id,
                    activity_id=input_trace.activity_id,
                    risk_type=RiskType.UNCRITICAL_ACCEPTANCE,
                    risk_level=RiskLevel.MEDIUM,
                    dimension=RiskDimension.EPISTEMIC,
                    description=(
                        "El estudiante no considera alternativas ni cuestiona las respuestas de la IA. "
                        "Esto indica posible aceptaci√≥n acr√≠tica."
                    ),
                    evidence=["No se registraron alternativas consideradas"],
                    trace_ids=[input_trace.id, response_trace.id],
                    recommendations=[
                        "Fomentar pensamiento cr√≠tico: '¬øQu√© otras opciones existen?'",
                        "Solicitar comparaci√≥n entre diferentes enfoques",
                        "Pedir que identifique limitaciones de la soluci√≥n propuesta"
                    ]
                )
                detected_risks.append(risk)
                self._persist_risk_object(risk, flow_id=flow_id)  # FIX: Persistir riesgo inmediatamente

        # Log summary
        if detected_risks:
            logger.warning(
                f"Risk analysis completed: {len(detected_risks)} risks detected",
                extra={
                    "session_id": session_id,
                    "risk_count": len(detected_risks),
                    "risk_types": [r.risk_type.value for r in detected_risks],
                    "risk_levels": [r.risk_level.value for r in detected_risks],
                    "flow_id": flow_id
                }
            )
        else:
            logger.info(
                "Risk analysis completed: no risks detected",
                extra={"session_id": session_id, "flow_id": flow_id}
            )

        # FIX APPLIED: Risks are now persisted immediately via _persist_risk_object()
        # after each _create_risk() call above

    def get_trace_sequence(self, session_id: str) -> Optional[TraceSequence]:
        """Obtiene la secuencia de trazas de una sesi√≥n desde BD (STATELESS)"""
        if self.sequence_repo is None:
            return None  # Backward compatibility

        # ‚úÖ STATELESS: Leer desde BD
        return self.sequence_repo.get_by_session(session_id)

    def get_risk_report(self, student_id: str, activity_id: str) -> Optional[RiskReport]:
        """Obtiene el reporte de riesgos desde BD (STATELESS)"""
        if self.risk_repo is None:
            return None  # Backward compatibility

        # ‚úÖ STATELESS: Construir reporte desde BD
        risks = self.risk_repo.get_by_student(student_id)

        # Filtrar por activity_id
        risks = [r for r in risks if r.activity_id == activity_id]

        if not risks:
            return None

        # Construir RiskReport
        report = RiskReport(
            id=f"report_{student_id}_{activity_id}",
            student_id=student_id,
            activity_id=activity_id
        )

        for db_risk in risks:
            risk = Risk(
                id=db_risk.id,
                student_id=db_risk.student_id,
                activity_id=db_risk.activity_id,
                risk_type=RiskType(db_risk.risk_type),
                risk_level=RiskLevel(db_risk.risk_level),
                dimension=RiskDimension(db_risk.dimension),
                description=db_risk.description or "",
                evidence=db_risk.evidence or [],
                trace_ids=db_risk.trace_ids or [],
            )
            report.add_risk(risk)

        return report

    def _validate_interaction_input(
        self,
        session_id: str,
        prompt: str,
        context: Optional[Dict[str, Any]]
    ) -> None:
        """
        Valida la entrada de una interacci√≥n.

        Args:
            session_id: ID de la sesi√≥n
            prompt: Prompt del estudiante
            context: Contexto adicional

        Raises:
            ValueError: Si la validaci√≥n falla
        """
        from .constants import (
            PROMPT_MIN_LENGTH,
            PROMPT_MAX_LENGTH,
            CONTEXT_MAX_SIZE_BYTES,
            SESSION_ID_MAX_LENGTH
        )
        import json

        # Validar session_id
        if not session_id or not isinstance(session_id, str):
            raise ValueError("session_id debe ser un string no vac√≠o")

        if len(session_id) > SESSION_ID_MAX_LENGTH:
            raise ValueError(
                f"session_id excede longitud m√°xima ({SESSION_ID_MAX_LENGTH} caracteres)"
            )

        # Validar prompt
        if not prompt or not isinstance(prompt, str):
            raise ValueError("prompt debe ser un string no vac√≠o")

        prompt_length = len(prompt.strip())
        if prompt_length < PROMPT_MIN_LENGTH:
            raise ValueError(
                f"Prompt demasiado corto (m√≠nimo {PROMPT_MIN_LENGTH} caracteres)"
            )

        if prompt_length > PROMPT_MAX_LENGTH:
            raise ValueError(
                f"Prompt demasiado largo (m√°ximo {PROMPT_MAX_LENGTH} caracteres)"
            )

        # Validar context
        if context is not None:
            if not isinstance(context, dict):
                raise ValueError("context debe ser un diccionario")

            # Validar tama√±o del contexto
            try:
                context_size = len(json.dumps(context).encode('utf-8'))
                if context_size > CONTEXT_MAX_SIZE_BYTES:
                    raise ValueError(
                        f"Context demasiado grande (m√°ximo {CONTEXT_MAX_SIZE_BYTES} bytes)"
                    )
            except (TypeError, ValueError) as e:
                raise ValueError(f"Context no es serializable a JSON: {str(e)}")

        logger.debug(
            "Input validation passed",
            extra={
                "session_id": session_id,
                "prompt_length": prompt_length,
                "has_context": context is not None
            }
        )

    def set_mode(self, session_id: str, mode: AgentMode) -> None:
        """Cambia el modo operativo de una sesi√≥n en BD (STATELESS)"""
        if self.session_repo is not None:
            self.session_repo.update_mode(session_id, mode.value.upper())
        # Si no hay repo, no hacer nada (backward compatibility)

    # =========================================================================
    # Circuit Breaker: Fallback Methods (cuando LLM falla o est√° inaccesible)
    # =========================================================================
    
    def _get_fallback_socratic_response(self, prompt: str, flow_id: Optional[str] = None) -> str:
        """
        Fallback cuando Ollama est√° inaccesible - Respuesta Socr√°tica
        
        Usa un banco de preguntas gen√©ricas pero pedag√≥gicamente v√°lidas
        """
        logger.warning(
            "Using fallback Socratic response (LLM unavailable)",
            extra={"flow_id": flow_id} if flow_id else None
        )
        return """‚ö†Ô∏è El sistema de IA est√° experimentando dificultades temporales, pero puedo ayudarte con estas preguntas gu√≠a:

**Para ayudarte mejor, necesito entender tu proceso de pensamiento:**

1. ¬øQu√© entend√©s que te est√°n pidiendo resolver?
2. ¬øQu√© conceptos cre√©s que son relevantes para este problema?
3. ¬øC√≥mo funcionar√≠a una soluci√≥n ideal?
4. ¬øQu√© has intentado hasta ahora y qu√© resultados obtuviste?

üí° **Tip**: Intenta descomponer el problema en partes m√°s peque√±as y manejables.

_Responde estas preguntas y podremos continuar cuando el sistema se recupere._"""

    def _get_fallback_conceptual_explanation(self, prompt: str, flow_id: Optional[str] = None) -> str:
        """
        Fallback cuando Ollama est√° inaccesible - Explicaci√≥n Conceptual
        """
        logger.warning(
            "Using fallback conceptual explanation (LLM unavailable)",
            extra={"flow_id": flow_id} if flow_id else None
        )
        return """‚ö†Ô∏è El sistema de IA est√° temporalmente fuera de servicio.

**Mientras tanto, aqu√≠ tienes una estructura para explorar el concepto:**

**Concepto clave**: [Identifica el concepto central de tu pregunta]

**Principio fundamental**: 
- ¬øPor qu√© es importante este concepto en programaci√≥n?
- ¬øQu√© problema resuelve?

**Ejemplo simple**: 
- Busca en tu material de estudio un ejemplo concreto
- Intenta relacionarlo con situaciones de la vida real

**Aplicaci√≥n pr√°ctica**: 
- ¬øC√≥mo lo usar√≠as en un proyecto real?
- ¬øQu√© ventajas te dar√≠a?

üìö **Recomendaci√≥n**: Consulta la documentaci√≥n oficial del lenguaje o framework que est√°s usando.

_El sistema estar√° disponible nuevamente en breve._"""

    def _get_fallback_guided_hints(self, prompt: str, flow_id: Optional[str] = None) -> str:
        """
        Fallback cuando Ollama est√° inaccesible - Pistas Guiadas
        """
        logger.warning(
            "Using fallback guided hints (LLM unavailable)",
            extra={"flow_id": flow_id} if flow_id else None
        )
        return """‚ö†Ô∏è El asistente de IA est√° temporalmente inaccesible.

**Aqu√≠ tienes una estrategia general de resoluci√≥n de problemas:**

**Pista 1 - Descomponer**: 
- Divide el problema en subproblemas m√°s peque√±os
- Resuelve cada parte por separado

**Pista 2 - Estructuras de datos**: 
- ¬øQu√© estructura (lista, diccionario, conjunto) facilitar√≠a la soluci√≥n?
- ¬øNecesitas acceso r√°pido, orden, o valores √∫nicos?

**Pista 3 - Casos especiales**: 
- No olvides casos l√≠mite (vac√≠o, un solo elemento, valores extremos)
- Prueba tu l√≥gica con ejemplos simples primero

**Pista 4 - Algoritmo paso a paso**: 
- Escribe en pseudoc√≥digo antes de programar
- Verifica cada paso con un ejemplo concreto

**Pr√≥ximo paso**: Intenta escribir el esqueleto de la soluci√≥n primero, sin preocuparte por los detalles.

üîß **Herramientas**: Usa print() o debugger para entender qu√© est√° haciendo tu c√≥digo en cada paso.

_El sistema de IA volver√° pronto. Mientras tanto, estos pasos pueden ayudarte a avanzar._"""