# ============================================================================
# Reusable Performance Regression Testing Workflow
# ============================================================================
# Cortez59: Added to detect performance regressions in critical paths
#
# This workflow runs performance benchmarks against critical API endpoints
# and cognitive processing paths. It uses k6 for load testing and Python
# benchmarks for algorithm verification.
#
# The workflow validates:
# - API response times under load
# - Cognitive trace processing performance (O(n log n) algorithms)
# - Risk analysis batch processing
# - Database query performance
#
# Usage:
#   jobs:
#     performance:
#       uses: ./.github/workflows/reusable-performance.yml
#       with:
#         python-version: '3.11'
#         concurrent-users: 10
# ============================================================================

name: Reusable Performance Testing

on:
  workflow_call:
    inputs:
      python-version:
        description: 'Python version to use'
        required: false
        default: '3.11'
        type: string
      concurrent-users:
        description: 'Number of concurrent virtual users for load test'
        required: false
        default: 10
        type: number
      test-duration:
        description: 'Duration of load test in seconds'
        required: false
        default: 30
        type: number
      baseline-p95-ms:
        description: 'P95 response time baseline in milliseconds'
        required: false
        default: 500
        type: number

permissions:
  contents: read

jobs:
  performance-benchmarks:
    name: Performance Benchmarks
    runs-on: ubuntu-latest
    timeout-minutes: 20

    services:
      postgres:
        image: postgres:15-alpine
        env:
          POSTGRES_DB: perf_test
          POSTGRES_USER: ai_native
          POSTGRES_PASSWORD: test_password
        ports:
          - 5432:5432
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5

      redis:
        image: redis:7.2-alpine
        ports:
          - 6379:6379
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ inputs.python-version }}
          cache: 'pip'

      - name: Install dependencies
        run: |
          pip install --upgrade pip
          pip install -r requirements.txt
          pip install pytest-benchmark locust httpx

      - name: Install k6
        run: |
          sudo gpg -k
          sudo gpg --no-default-keyring --keyring /usr/share/keyrings/k6-archive-keyring.gpg --keyserver hkp://keyserver.ubuntu.com:80 --recv-keys C5AD17C747E3415A3642D57D77C6C491D6AC1D69
          echo "deb [signed-by=/usr/share/keyrings/k6-archive-keyring.gpg] https://dl.k6.io/deb stable main" | sudo tee /etc/apt/sources.list.d/k6.list
          sudo apt-get update
          sudo apt-get install -y k6

      - name: Initialize database
        env:
          DATABASE_URL: postgresql://ai_native:test_password@localhost:5432/perf_test
          REDIS_URL: redis://localhost:6379/0
          JWT_SECRET_KEY: perf_test_secret_key
          SECRET_KEY: perf_test_secret_key
          LLM_PROVIDER: mock
          ENVIRONMENT: testing
        run: |
          echo "Initializing database..."
          python -m backend.scripts.init_db || echo "Database may already exist"

      - name: Create k6 test script
        run: |
          cat > k6-api-test.js << 'EOF'
          import http from 'k6/http';
          import { check, sleep } from 'k6';
          import { Rate, Trend } from 'k6/metrics';

          const errorRate = new Rate('errors');
          const healthTrend = new Trend('health_endpoint_duration');
          const sessionsTrend = new Trend('sessions_endpoint_duration');

          export const options = {
            vus: ${{ inputs.concurrent-users }},
            duration: '${{ inputs.test-duration }}s',
            thresholds: {
              http_req_duration: ['p(95)<${{ inputs.baseline-p95-ms }}'],
              errors: ['rate<0.1'],
            },
          };

          const BASE_URL = 'http://localhost:8000/api/v1';

          export default function () {
            // Health endpoint
            let healthRes = http.get(`${BASE_URL}/health`);
            healthTrend.add(healthRes.timings.duration);
            check(healthRes, {
              'health status is 200': (r) => r.status === 200,
            }) || errorRate.add(1);

            sleep(0.1);

            // Simulators list (public endpoint)
            let simulatorsRes = http.get(`${BASE_URL}/simulators`);
            check(simulatorsRes, {
              'simulators status is 200 or 401': (r) => r.status === 200 || r.status === 401,
            }) || errorRate.add(1);

            sleep(0.1);

            // Training languages (public endpoint)
            let lenguajesRes = http.get(`${BASE_URL}/training/lenguajes`);
            check(lenguajesRes, {
              'lenguajes returns data': (r) => r.status === 200 || r.status === 401,
            }) || errorRate.add(1);

            sleep(0.5);
          }

          export function handleSummary(data) {
            return {
              'k6-summary.json': JSON.stringify(data, null, 2),
            };
          }
          EOF

      - name: Start API server
        env:
          DATABASE_URL: postgresql://ai_native:test_password@localhost:5432/perf_test
          REDIS_URL: redis://localhost:6379/0
          JWT_SECRET_KEY: perf_test_secret_key
          SECRET_KEY: perf_test_secret_key
          LLM_PROVIDER: mock
          ENVIRONMENT: testing
        run: |
          echo "Starting API server..."
          python -m backend &
          sleep 10

          # Verify server is running
          for i in {1..30}; do
            if curl -s http://localhost:8000/api/v1/health > /dev/null; then
              echo "Server is ready"
              break
            fi
            echo "Waiting for server... ($i)"
            sleep 1
          done

      - name: Run k6 load test
        run: |
          echo "Running k6 load test..."
          k6 run k6-api-test.js || true

      - name: Analyze k6 results
        run: |
          python -c "
          import json
          import sys

          try:
              with open('k6-summary.json', 'r') as f:
                  data = json.load(f)
          except FileNotFoundError:
              print('k6 summary not found - test may have failed to start')
              sys.exit(0)

          metrics = data.get('metrics', {})

          print('K6 Load Test Results')
          print('=' * 50)

          # HTTP request duration
          http_dur = metrics.get('http_req_duration', {})
          if http_dur:
              values = http_dur.get('values', {})
              print(f\"HTTP Request Duration:\")
              print(f\"  Average: {values.get('avg', 0):.2f}ms\")
              print(f\"  P50: {values.get('med', 0):.2f}ms\")
              print(f\"  P90: {values.get('p(90)', 0):.2f}ms\")
              print(f\"  P95: {values.get('p(95)', 0):.2f}ms\")
              print(f\"  P99: {values.get('p(99)', 0):.2f}ms\")
              print(f\"  Max: {values.get('max', 0):.2f}ms\")

              p95 = values.get('p(95)', 0)
              baseline = ${{ inputs.baseline-p95-ms }}
              if p95 > baseline:
                  print(f'')
                  print(f'WARNING: P95 ({p95:.2f}ms) exceeds baseline ({baseline}ms)')
              else:
                  print(f'')
                  print(f'P95 within baseline: {p95:.2f}ms < {baseline}ms')

          # Error rate
          errors = metrics.get('errors', {})
          if errors:
              rate = errors.get('values', {}).get('rate', 0)
              print(f'')
              print(f'Error Rate: {rate*100:.2f}%')

          # Request count
          http_reqs = metrics.get('http_reqs', {})
          if http_reqs:
              count = http_reqs.get('values', {}).get('count', 0)
              rate = http_reqs.get('values', {}).get('rate', 0)
              print(f'')
              print(f'Total Requests: {count}')
              print(f'Requests/sec: {rate:.2f}')
          "

      - name: Run algorithm benchmarks
        env:
          DATABASE_URL: postgresql://ai_native:test_password@localhost:5432/perf_test
          REDIS_URL: redis://localhost:6379/0
          JWT_SECRET_KEY: perf_test_secret_key
          SECRET_KEY: perf_test_secret_key
          LLM_PROVIDER: mock
          ENVIRONMENT: testing
        run: |
          python -c "
          import time
          import random
          from datetime import datetime, timedelta

          print('Algorithm Performance Benchmarks')
          print('=' * 50)

          # Simulate cognitive trace data
          def generate_traces(n):
              traces = []
              base_time = datetime.now()
              for i in range(n):
                  traces.append({
                      'id': f'trace_{i}',
                      'timestamp': base_time + timedelta(seconds=i),
                      'cognitive_state': random.choice(['exploracion', 'implementacion', 'depuracion']),
                      'ai_involvement': random.uniform(0, 1),
                      'content': f'Sample content {i}' * 10,
                  })
              return traces

          # Test O(n log n) algorithm performance
          sizes = [100, 500, 1000, 5000]

          print('')
          print('Trace Processing Performance:')
          print('-' * 40)
          print(f'{'Size':>8} | {'Time (ms)':>10} | {'Ops/sec':>10}')
          print('-' * 40)

          for size in sizes:
              traces = generate_traces(size)

              start = time.perf_counter()

              # Simulate bisect-based range query (O(n log n))
              from bisect import bisect_left, bisect_right
              timestamps = sorted([t['timestamp'] for t in traces])

              # Perform 100 range queries
              for _ in range(100):
                  target = timestamps[len(timestamps)//2]
                  idx = bisect_right(timestamps, target)

              end = time.perf_counter()
              elapsed_ms = (end - start) * 1000
              ops_per_sec = 100 / (end - start)

              print(f'{size:>8} | {elapsed_ms:>10.2f} | {ops_per_sec:>10.0f}')

          print('')
          print('Performance benchmarks completed')
          "

      - name: Stop API server
        if: always()
        run: |
          pkill -f "python -m backend" || true

      - name: Upload k6 results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: k6-results
          path: k6-summary.json
          retention-days: 30

      - name: Generate performance summary
        if: always()
        run: |
          echo "## Performance Test Results" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### Load Test Configuration" >> $GITHUB_STEP_SUMMARY
          echo "| Parameter | Value |" >> $GITHUB_STEP_SUMMARY
          echo "|-----------|-------|" >> $GITHUB_STEP_SUMMARY
          echo "| Virtual Users | ${{ inputs.concurrent-users }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Duration | ${{ inputs.test-duration }}s |" >> $GITHUB_STEP_SUMMARY
          echo "| P95 Baseline | ${{ inputs.baseline-p95-ms }}ms |" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### Results" >> $GITHUB_STEP_SUMMARY
          echo "| Check | Status |" >> $GITHUB_STEP_SUMMARY
          echo "|-------|--------|" >> $GITHUB_STEP_SUMMARY
          echo "| k6 Load Test | ${{ job.status == 'success' && 'Passed' || 'Check Required' }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Algorithm Benchmarks | ${{ job.status == 'success' && 'Passed' || 'Check Required' }} |" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Artifact**: \`k6-results\`" >> $GITHUB_STEP_SUMMARY
