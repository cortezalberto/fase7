# üöÄ Gu√≠a R√°pida de Deploy - Mejoras AI-Native

## ‚ö° Inicio R√°pido (Todo en Uno)

```powershell
# Ejecuta el script de deploy autom√°tico
.\deploy_mejoras.ps1
```

Este script hace todo autom√°ticamente:
1. ‚úì Verifica Docker
2. ‚úì Reconstruye contenedores con nuevas configs
3. ‚úì Descarga modelo llama3.2:3b
4. ‚úì Instala dependencias del frontend

---

## üìã Paso a Paso Manual (Si prefieres control total)

### Backend

```powershell
# 1. Reconstruir stack
docker-compose down
docker-compose up -d --build

# 2. Descargar modelo nuevo (primera vez)
docker exec -it ai-native-ollama ollama pull llama3.2:3b

# 3. Verificar que est√° activo
docker exec -it ai-native-ollama ollama list
# Debe aparecer: llama3.2:3b

# 4. Ver logs
docker-compose logs -f api
docker-compose logs -f ollama
```

### Frontend

```powershell
# 1. Instalar dependencias (primera vez)
cd frontEnd
npm install

# 2. Iniciar dev server
npm run dev

# 3. Abrir navegador
# http://localhost:5173
```

---

## ‚úÖ Verificaci√≥n

### 1. Backend funcionando
```powershell
curl http://localhost:8000/api/v1/health
```

Debe retornar:
```json
{
  "status": "healthy",
  "version": "1.0.0"
}
```

### 2. Ollama funcionando
```powershell
docker exec -it ai-native-ollama ollama list
```

Debe mostrar:
```
NAME              ID              SIZE      MODIFIED
llama3.2:3b      abc123def456    2.0 GB    X minutes ago
```

### 3. Frontend funcionando
Abre http://localhost:5173 - Debe aparecer la aplicaci√≥n

---

## üéØ Qu√© Cambi√≥

### Backend
- ‚úÖ Modelo: `llama3.2:3b` (2.3x m√°s r√°pido)
- ‚úÖ Keep-Alive: Permanente (sin latencia inicial)
- ‚úÖ Reintentos: 3 intentos autom√°ticos con backoff
- ‚úÖ Fallback: Respuestas educativas si LLM falla

### Frontend
- ‚úÖ Monaco Editor (VS Code)
- ‚úÖ Layout resizable de 3 paneles
- ‚úÖ AI Companion (Tutor/Juez/Simulador)
- ‚úÖ Dashboard Docente con m√©tricas
- ‚úÖ Skeleton loading (percepci√≥n de velocidad)
- ‚úÖ Sistema de toasts (feedback no-intrusivo)

---

## üêõ Troubleshooting

### "Docker no est√° corriendo"
```powershell
# Inicia Docker Desktop desde el men√∫ de Windows
# Espera a que el √≠cono de Docker sea verde
# Vuelve a ejecutar el script
```

### "Ollama no responde"
```powershell
# Ver logs de Ollama
docker-compose logs ollama

# Reiniciar solo Ollama
docker-compose restart ollama

# Espera 10-15 segundos y reintenta
```

### "npm install falla"
```powershell
# Aseg√∫rate de tener Node.js 18+
node --version

# Limpia cache
npm cache clean --force
npm install
```

### "Puerto 8000 ocupado"
```powershell
# Ver qu√© usa el puerto
netstat -ano | findstr :8000

# Matar proceso (reemplaza PID)
taskkill /PID <PID> /F

# O cambia el puerto en docker-compose.yml
```

---

## üìä Testing de Performance

### Medir latencia del LLM
```powershell
# Prueba simple (debe responder <1s despu√©s del primer uso)
curl -X POST http://localhost:8000/api/v1/chat \
  -H "Content-Type: application/json" \
  -d '{"message": "Hola", "session_id": "test"}'
```

### Ver m√©tricas de reintentos
```powershell
# Busca en logs cu√°ntos reintentos hubo
docker-compose logs api | findstr "attempt"
```

---

## üìö Documentaci√≥n Completa

Ver `MEJORAS_IMPLEMENTADAS.md` para detalles t√©cnicos completos.

---

## üÜò Soporte

Si algo no funciona:
1. Lee los logs: `docker-compose logs -f`
2. Verifica que todos los servicios est√©n "healthy": `docker-compose ps`
3. Revisa el troubleshooting arriba

---

**√öltima actualizaci√≥n**: Diciembre 2025
