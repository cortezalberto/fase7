# ğŸ“ Sistema de EvaluaciÃ³n de CÃ³digo con Mentor "Alex"

## ğŸ“‹ VisiÃ³n General

Sistema completo de evaluaciÃ³n inteligente de cÃ³digo que usa un LLM (Claude, GPT-4, etc.) actuando como **"Alex"**, un Arquitecto de Software Senior que realiza code reviews pedagÃ³gicos.

**CaracterÃ­sticas:**
- âœ… EvaluaciÃ³n en 3 dimensiones (Functionality, Code Quality, Robustness)
- âœ… Feedback narrativo personalizado en Markdown
- âœ… Anotaciones lÃ­nea por lÃ­nea en el cÃ³digo
- âœ… Sugerencias de refactoring (versiÃ³n Senior)
- âœ… Sistema de gamificaciÃ³n (XP, logros)
- âœ… Type-safe con TypeScript

---

## ğŸ“ Archivos Creados

```
activia1-main/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ prompts/
â”‚   â”‚   â””â”€â”€ code_evaluator_prompt.md      # â­ Prompt template para LLM
â”‚   â””â”€â”€ services/
â”‚       â””â”€â”€ code_evaluator.py             # â­ Servicio de evaluaciÃ³n
â”‚
â”œâ”€â”€ frontEnd/
â”‚   â””â”€â”€ src/
â”‚       â””â”€â”€ types/
â”‚           â”œâ”€â”€ evaluation.d.ts           # â­ Interfaces TypeScript
â”‚           â””â”€â”€ index.ts                  # âœ… ACTUALIZADO con exports
â”‚
â””â”€â”€ examples/
    â””â”€â”€ ejemplo_evaluacion_ui.tsx         # â­ Componentes React
```

---

## ğŸ”§ IntegraciÃ³n Backend

### 1. InstalaciÃ³n del Servicio

```python
# backend/services/code_evaluator.py
from backend.services.code_evaluator import CodeEvaluator

# Con cliente LLM (Claude, OpenAI, etc.)
from anthropic import AsyncAnthropic

client = AsyncAnthropic(api_key="tu-api-key")
evaluator = CodeEvaluator(llm_client=client)

# Modo mock (para testing sin LLM)
evaluator = CodeEvaluator()  # Sin cliente
```

### 2. Endpoint de EvaluaciÃ³n

```python
# backend/api/routers/exercises.py
from backend.services.code_evaluator import evaluate_code
from backend.data.exercises.loader import get_exercise

@router.post("/exercises/{exercise_id}/evaluate")
async def evaluate_exercise(
    exercise_id: str,
    request: EvaluationRequest,
    current_user: User = Depends(get_current_user)
):
    """EvalÃºa el cÃ³digo del estudiante con mentor Alex"""
    
    # 1. Cargar ejercicio
    exercise = get_exercise(exercise_id)
    if not exercise:
        raise HTTPException(404, "Ejercicio no encontrado")
    
    # 2. Ejecutar cÃ³digo en sandbox
    sandbox_result = await execute_in_sandbox(
        code=request.student_code,
        language=exercise['ui_config']['editor_language'],
        tests=exercise['hidden_tests']
    )
    
    # 3. Evaluar con Alex
    evaluation = await evaluate_code(
        exercise=exercise,
        student_code=request.student_code,
        sandbox_result=sandbox_result,
        llm_client=llm_client  # Tu cliente LLM
    )
    
    # 4. Guardar en BD
    save_evaluation(
        user_id=current_user.id,
        exercise_id=exercise_id,
        evaluation=evaluation
    )
    
    return evaluation
```

### 3. Schemas Pydantic

```python
# backend/api/schemas/evaluation.py
from pydantic import BaseModel
from typing import List, Optional

class EvaluationRequest(BaseModel):
    student_code: str

class DimensionScore(BaseModel):
    score: float  # 0-10
    comment: str

class CodeAnnotation(BaseModel):
    line_number: int
    severity: str  # 'info' | 'warning' | 'error'
    message: str

class EvaluationResponse(BaseModel):
    evaluation: dict
    dimensions: dict
    code_review: dict
    gamification: dict
    metadata: Optional[dict] = None
```

---

## ğŸ¨ IntegraciÃ³n Frontend

### 1. Tipos TypeScript

```typescript
import { 
  IEvaluationResult, 
  IEvaluationRequest,
  ACHIEVEMENTS_CATALOG 
} from '@/types';

// Todos los tipos estÃ¡n disponibles globalmente
```

### 2. Servicio API

```typescript
// frontEnd/src/services/api/evaluation.service.ts
import { BaseApiService } from './base.service';
import { IEvaluationResult, IEvaluationRequest } from '@/types';

class EvaluationService extends BaseApiService {
  constructor() {
    super('/exercises');
  }

  async evaluate(
    exerciseId: string,
    request: IEvaluationRequest
  ): Promise<IEvaluationResult> {
    return this.post<IEvaluationResult>(
      `/${exerciseId}/evaluate`,
      request
    );
  }

  async getHistory(userId: string) {
    return this.get(`/users/${userId}/evaluations`);
  }

  async getProgress(userId: string) {
    return this.get(`/users/${userId}/progress`);
  }
}

export const evaluationService = new EvaluationService();
```

### 3. Componente React

Ver archivo completo: `examples/ejemplo_evaluacion_ui.tsx`

```tsx
import { EvaluationResultView } from '@/components/evaluation';
import { evaluationService } from '@/services/api';

// En tu componente de ejercicio
const handleSubmit = async () => {
  const result = await evaluationService.evaluate('U1-VAR-01', {
    student_code: code
  });
  
  setEvaluationResult(result);
};

// Renderizar
{evaluationResult && (
  <EvaluationResultView
    result={evaluationResult}
    studentCode={code}
    onRetry={() => setEvaluationResult(null)}
  />
)}
```

---

## ğŸ“Š Estructura de la Respuesta

### Ejemplo de JSON Completo

```json
{
  "evaluation": {
    "score": 85.5,
    "status": "PASS",
    "title": "LÃ³gica correcta, pero frÃ¡gil",
    "summary_markdown": "Tu soluciÃ³n **cumple la misiÃ³n**, pero presenta **vulnerabilidades** en el manejo de errores.",
    "toast_type": "success",
    "toast_message": "Â¡Bien! FuncionÃ³, pero podrÃ­a ser mÃ¡s robusto."
  },
  "dimensions": {
    "functionality": {
      "score": 9,
      "comment": "La lÃ³gica cumple con todos los requisitos."
    },
    "code_quality": {
      "score": 8,
      "comment": "Buen uso de nombres descriptivos."
    },
    "robustness": {
      "score": 6,
      "comment": "Falta validaciÃ³n de inputs."
    }
  },
  "code_review": {
    "highlighted_lines": [
      {
        "line_number": 12,
        "severity": "warning",
        "message": "DivisiÃ³n sin validar denominador."
      }
    ],
    "refactoring_suggestion": "def calcular_promedio(valores):\n    if not valores:\n        return 0.0\n    ..."
  },
  "gamification": {
    "xp_earned": 85,
    "achievements_unlocked": ["Clean Code Ninja", "First Blood"]
  },
  "metadata": {
    "exercise_id": "U1-VAR-01",
    "evaluated_at": "2025-12-17T10:30:00Z",
    "llm_model": "claude-sonnet-4.5"
  }
}
```

---

## ğŸ¯ Criterios de EvaluaciÃ³n

### Functionality (0-10)
| Score | DescripciÃ³n |
|-------|-------------|
| 10 | Cumple 100% la misiÃ³n. Output exacto. |
| 7-9 | Funciona pero con pequeÃ±as desviaciones. |
| 4-6 | LÃ³gica parcial, algunos tests fallan. |
| 0-3 | No funciona o hardcoded. |

### Code Quality (0-10)
| Score | DescripciÃ³n |
|-------|-------------|
| 10 | CÃ³digo limpio, naming perfecto, bien estructurado. |
| 7-9 | Buen cÃ³digo, pequeÃ±os detalles de naming. |
| 4-6 | CÃ³digo funcional pero desorganizado. |
| 0-3 | Spaghetti code, variables `x`, `y`, `z`. |

### Robustness (0-10)
| Score | DescripciÃ³n |
|-------|-------------|
| 10 | Maneja todos los edge cases, excepciones bien tratadas. |
| 7-9 | Maneja casos bÃ¡sicos, falta validaciÃ³n exhaustiva. |
| 4-6 | CÃ³digo frÃ¡gil, crashea con inputs inesperados. |
| 0-3 | Sin manejo de errores, bare excepts. |

---

## ğŸ† Sistema de Logros

### Logros Disponibles

| Logro | CondiciÃ³n | XP Bonus | Rareza |
|-------|-----------|----------|--------|
| ğŸ¯ First Blood | Primer ejercicio completado | 50 | Common |
| ğŸ¥‹ Clean Code Ninja | code_quality >= 9 | 100 | Rare |
| ğŸ›¡ï¸ Error Handler | Try-except correcto | 75 | Common |
| ğŸ Pythonista | Features pythonic | 150 | Epic |
| ğŸ›¡ï¸ Defensive Programmer | Valida inputs, edge cases | 200 | Epic |
| â™»ï¸ DRY Master | CÃ³digo modular, sin repeticiÃ³n | 100 | Rare |
| âš¡ Speed Demon | < 50% tiempo estimado | 150 | Rare |
| ğŸ’ Perfectionist | Score 100/100 | 500 | Legendary |

### IntegraciÃ³n de Logros

```typescript
import { ACHIEVEMENTS_CATALOG } from '@/types';

// Buscar logro
const achievement = ACHIEVEMENTS_CATALOG.find(
  a => a.id === 'clean_code_ninja'
);

// Mostrar detalles
<div>
  <span>{achievement.icon}</span>
  <span>{achievement.name}</span>
  <span>+{achievement.xp_bonus} XP</span>
</div>
```

---

## ğŸ”¥ Features Clave

### 1. **DetecciÃ³n de Hardcoding**
El evaluador detecta cuando el estudiante imprime directamente la respuesta:

```python
# Hardcoding detectado (Score 0)
print("Total: $42600")
print("Promedio: $14200.00")
```

### 2. **Anotaciones Contextuales**
Comentarios especÃ­ficos en lÃ­neas problemÃ¡ticas:

```typescript
{
  line_number: 12,
  severity: "warning",
  message: "DivisiÃ³n sin validar denominador. Si 'total' es 0, esto crashea."
}
```

### 3. **Refactoring Senior**
Muestra cÃ³mo un Senior escribirÃ­a el cÃ³digo:

```python
# VersiÃ³n Senior (mÃ¡s robusta)
def calcular_promedio(valores):
    if not valores:  # Validar lista vacÃ­a
        return 0.0
    try:
        total = sum(valores)
        promedio = total / len(valores)
        return round(promedio, 2)
    except (TypeError, ValueError) as e:
        print(f'Error: {e}')
        return None
```

### 4. **Feedback PedagÃ³gico**
Tono constructivo que empieza validando, seÃ±ala el gap y cierra con consejo:

> Tu soluciÃ³n **funciona correctamente** y el uso de **f-strings** es excelente. Sin embargo, no validas si la lista estÃ¡ **vacÃ­a**, lo que causarÃ­a un error. Un Senior siempre pregunta: *"Â¿QuÃ© puede romper esto?"*. Agrega `if not valores: return 0.0` al inicio.

---

## ğŸ§ª Testing

### Modo Mock (Sin LLM)

```python
# Ãštil para desarrollo y testing
evaluator = CodeEvaluator()  # Sin cliente LLM
result = await evaluator.evaluate(exercise, code, sandbox_result)

# Retorna evaluaciÃ³n bÃ¡sica basada en tests
```

### Prueba del Prompt

```python
# backend/services/code_evaluator.py
python code_evaluator.py

# Output:
# {
#   "evaluation": {
#     "score": 100,
#     "status": "PASS",
#     ...
#   }
# }
```

---

## ğŸ“ Ejemplo de Uso Completo

### Backend

```python
from backend.services.code_evaluator import CodeEvaluator
from backend.data.exercises.loader import get_exercise

async def evaluate_student_code(exercise_id: str, code: str):
    # 1. Cargar ejercicio
    exercise = get_exercise(exercise_id)
    
    # 2. Ejecutar en sandbox
    sandbox_result = {
        "exit_code": 0,
        "stdout": "Total: $42600\nPromedio: $14200.00\n",
        "stderr": "",
        "tests_passed": 2,
        "tests_total": 2
    }
    
    # 3. Evaluar
    evaluator = CodeEvaluator(llm_client)
    result = await evaluator.evaluate(
        exercise=exercise,
        student_code=code,
        sandbox_result=sandbox_result
    )
    
    return result
```

### Frontend

```tsx
const ExerciseWithEvaluation = () => {
  const [code, setCode] = useState('');
  const [result, setResult] = useState<IEvaluationResult | null>(null);
  
  const handleSubmit = async () => {
    const evaluation = await evaluationService.evaluate('U1-VAR-01', {
      student_code: code
    });
    setResult(evaluation);
  };
  
  return (
    <div>
      <Editor value={code} onChange={setCode} />
      <button onClick={handleSubmit}>Evaluar</button>
      
      {result && (
        <EvaluationResultView
          result={result}
          studentCode={code}
          onRetry={() => setResult(null)}
        />
      )}
    </div>
  );
};
```

---

## ğŸš€ PrÃ³ximos Pasos

### ImplementaciÃ³n Inmediata
1. âœ… Integrar cliente LLM (Claude/OpenAI)
2. âœ… Crear endpoint `/exercises/{id}/evaluate`
3. âœ… Implementar sandbox de ejecuciÃ³n
4. âœ… Guardar evaluaciones en BD

### Mejoras Futuras
- [ ] EvaluaciÃ³n comparativa (vs otros estudiantes)
- [ ] DetecciÃ³n de plagio
- [ ] Hints progresivos antes de evaluar
- [ ] EvaluaciÃ³n de complejidad algorÃ­tmica (Big O)
- [ ] AnÃ¡lisis de seguridad de cÃ³digo

---

## ğŸ“ Referencias

- **Prompt Template:** `backend/prompts/code_evaluator_prompt.md`
- **Servicio Python:** `backend/services/code_evaluator.py`
- **Tipos TS:** `frontEnd/src/types/evaluation.d.ts`
- **Componente UI:** `examples/ejemplo_evaluacion_ui.tsx`

---

**Generado:** 17 de Diciembre, 2025  
**VersiÃ³n:** 1.0  
**Stack:** Python + FastAPI + React + TypeScript + LLM
