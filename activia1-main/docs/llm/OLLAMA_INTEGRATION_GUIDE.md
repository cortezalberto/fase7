# Gu√≠a de Integraci√≥n Ollama - Phoenix MVP

## üìã √çndice
1. [¬øQu√© es Ollama?](#qu√©-es-ollama)
2. [Instalaci√≥n](#instalaci√≥n)
3. [Configuraci√≥n](#configuraci√≥n)
4. [Uso](#uso)
5. [Modelos Disponibles](#modelos-disponibles)
6. [Docker Deployment](#docker-deployment)
7. [Troubleshooting](#troubleshooting)
8. [Comparaci√≥n con otros Proveedores](#comparaci√≥n)

---

## ü§ñ ¬øQu√© es Ollama?

**Ollama** es un framework ligero para ejecutar modelos de lenguaje (LLMs) **localmente** en tu infraestructura. 

### Ventajas
- ‚úÖ **100% Local**: Sin env√≠o de datos a servicios externos
- ‚úÖ **Sin API Keys**: No necesita claves de acceso
- ‚úÖ **Privacidad Total**: Datos sensibles nunca salen de tu servidor
- ‚úÖ **Sin Costos por Token**: No hay cargos por uso
- ‚úÖ **Offline**: Funciona sin conexi√≥n a Internet
- ‚úÖ **Modelos Open Source**: Llama 2, Mistral, Gemma, Code Llama, etc.

### Casos de Uso Ideales
- üè• **Sector Salud**: Datos m√©dicos sensibles (HIPAA compliance)
- üè¶ **Sector Financiero**: Informaci√≥n confidencial de clientes
- üè≠ **Empresas**: C√≥digo propietario, documentaci√≥n interna
- üéì **Educaci√≥n**: Privacidad de estudiantes (FERPA)
- üß™ **Desarrollo/Testing**: Entorno local sin costos

---

## üì¶ Instalaci√≥n

### Windows

```powershell
# Descargar instalador oficial
# https://ollama.ai/download/windows

# Ejecutar instalador
# El servicio se inicia autom√°ticamente
```

### Linux

```bash
# Instalaci√≥n con un comando
curl -fsSL https://ollama.ai/install.sh | sh

# Verificar instalaci√≥n
ollama --version
```

### macOS

```bash
# Descargar desde https://ollama.ai/download/mac
# O instalar con Homebrew
brew install ollama
```

### Docker (Recomendado para Producci√≥n)

Ya incluido en `docker-compose.yml`:

```bash
# Iniciar Ollama con el stack completo
docker-compose --profile ollama up -d

# O solo Ollama
docker-compose up -d ollama
```

---

## üéØ Configuraci√≥n

### 1. Instalar y Descargar Modelos

```bash
# Descargar modelo Llama 2 (4GB)
ollama pull llama2

# Descargar Mistral 7B (m√°s r√°pido, similar calidad)
ollama pull mistral

# Descargar Code Llama (optimizado para c√≥digo)
ollama pull codellama

# Descargar Gemma (modelo de Google)
ollama pull gemma:7b

# Ver modelos instalados
ollama list
```

### 2. Configurar Variables de Entorno

Editar archivo `.env` en la ra√≠z del proyecto:

```bash
# Seleccionar Ollama como proveedor
LLM_PROVIDER=ollama

# Configuraci√≥n de Ollama (opcional, estos son los defaults)
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=llama2
```

### 3. Configuraci√≥n Program√°tica

```python
from src.ai_native_mvp.llm import LLMProviderFactory

# M√©todo 1: Desde variables de entorno
provider = LLMProviderFactory.create_from_env("ollama")

# M√©todo 2: Configuraci√≥n manual
provider = LLMProviderFactory.create("ollama", {
    "base_url": "http://localhost:11434",
    "model": "mistral",
    "temperature": 0.7,
    "timeout": 120.0
})
```

---

## üöÄ Uso

### Generaci√≥n de Respuestas

```python
from src.ai_native_mvp.llm import LLMProviderFactory
from src.ai_native_mvp.llm.base import LLMMessage, LLMRole

# Crear provider
provider = LLMProviderFactory.create_from_env("ollama")

# Preparar mensajes
messages = [
    LLMMessage(
        role=LLMRole.SYSTEM,
        content="Sos un profesor de matem√°ticas experto."
    ),
    LLMMessage(
        role=LLMRole.USER,
        content="¬øCu√°l es la f√≥rmula del √°rea de un c√≠rculo?"
    )
]

# Generar respuesta
response = await provider.generate(
    messages=messages,
    temperature=0.5,  # M√°s determin√≠stico
    max_tokens=500
)

print(response.content)
print(f"Tokens usados: {response.usage['total_tokens']}")
```

### Streaming (Respuestas en Tiempo Real)

```python
# Streaming para UX interactiva
async for chunk in provider.generate_stream(messages):
    print(chunk, end="", flush=True)
```

### Verificar Modelos Disponibles

```python
# Listar modelos instalados en Ollama
models = await provider.list_available_models()
print(f"Modelos disponibles: {models}")

# Verificar si un modelo espec√≠fico est√° disponible
is_available = await provider.is_model_available()
if not is_available:
    print(f"‚ö†Ô∏è  Modelo {provider.model} no encontrado. Instalar con:")
    print(f"   ollama pull {provider.model}")
```

---

## üé≠ Modelos Disponibles

### Modelos Recomendados

| Modelo | Tama√±o | Velocidad | Calidad | Uso Ideal | Comando |
|--------|--------|-----------|---------|-----------|---------|
| **Llama 2 7B** | 4GB | ‚ö°‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê | General purpose | `ollama pull llama2` |
| **Mistral 7B** | 4GB | ‚ö°‚ö°‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê‚≠ê | Conversaci√≥n, an√°lisis | `ollama pull mistral` |
| **Code Llama 7B** | 4GB | ‚ö°‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê‚≠ê | Programaci√≥n | `ollama pull codellama` |
| **Gemma 7B** | 5GB | ‚ö°‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê‚≠ê | Google, multiling√ºe | `ollama pull gemma:7b` |
| **Llama 2 13B** | 7GB | ‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê‚≠ê | Mejor calidad | `ollama pull llama2:13b` |
| **Mixtral 8x7B** | 26GB | ‚ö° | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | M√°xima calidad | `ollama pull mixtral` |

### Variantes por Idioma

```bash
# Modelos optimizados para espa√±ol
ollama pull gemma:7b        # Google, excelente espa√±ol
ollama pull llama2          # Multiling√ºe con buen espa√±ol

# Modelos especializados
ollama pull codellama       # C√≥digo (Python, JS, etc.)
ollama pull llama2:70b      # M√°xima calidad (requiere GPU potente)
```

### Comparaci√≥n de Rendimiento

```bash
# Test r√°pido de un modelo
ollama run mistral "Explica qu√© es FastAPI en una frase"

# Benchmarks (aproximados en CPU moderna)
# - Llama 2 7B:    ~20 tokens/segundo
# - Mistral 7B:    ~25 tokens/segundo  
# - Code Llama 7B: ~20 tokens/segundo
# - Gemma 7B:      ~18 tokens/segundo

# Con GPU (NVIDIA RTX 3090):
# - Llama 2 7B:    ~100 tokens/segundo
# - Mistral 7B:    ~120 tokens/segundo
```

---

## üê≥ Docker Deployment

### Opci√≥n 1: Servicio Independiente (Producci√≥n)

Ollama ya est√° configurado en `docker-compose.yml`:

```yaml
services:
  ollama:
    image: ollama/ollama:latest
    container_name: ai-native-ollama
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama  # Persistencia de modelos
    networks:
      - ai-native-network
    restart: unless-stopped
    profiles:
      - ollama  # Activar con --profile ollama
```

### Iniciar Stack con Ollama

```bash
# Iniciar API + PostgreSQL + Redis + Ollama
docker-compose --profile ollama up -d

# Ver logs
docker-compose logs -f ollama

# Descargar modelos en container
docker-compose exec ollama ollama pull llama2
docker-compose exec ollama ollama pull mistral

# Listar modelos descargados
docker-compose exec ollama ollama list
```

### Opci√≥n 2: Con GPU (NVIDIA)

Descomentar secci√≥n en `docker-compose.yml`:

```yaml
ollama:
  # ... (config anterior)
  deploy:
    resources:
      reservations:
        devices:
          - driver: nvidia
            count: 1
            capabilities: [gpu]
```

**Prerequisitos:**
- Instalar [NVIDIA Container Toolkit](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html)
- Drivers NVIDIA actualizados

```bash
# Verificar GPU disponible
docker run --rm --gpus all nvidia/cuda:11.0-base nvidia-smi

# Iniciar con GPU
docker-compose --profile ollama up -d
```

---

## üîß Troubleshooting

### Problema: "Cannot connect to Ollama server"

```python
‚ùå ValueError: Cannot connect to Ollama server at http://localhost:11434
```

**Soluciones:**

1. **Verificar que Ollama est√° corriendo:**
   ```bash
   # Linux/macOS
   ps aux | grep ollama
   systemctl status ollama  # Si se instal√≥ como servicio
   
   # Windows
   Get-Process ollama
   
   # Docker
   docker-compose ps ollama
   ```

2. **Iniciar Ollama:**
   ```bash
   # Local
   ollama serve
   
   # Docker
   docker-compose up -d ollama
   ```

3. **Verificar puerto:**
   ```bash
   # Verificar que puerto 11434 est√° escuchando
   netstat -an | grep 11434
   curl http://localhost:11434/api/tags
   ```

### Problema: "Model not found"

```python
‚ùå Ollama API error (404): Model 'mistral' not found
```

**Soluci√≥n:**

```bash
# Descargar modelo
ollama pull mistral

# Verificar modelos instalados
ollama list

# Docker
docker-compose exec ollama ollama pull mistral
```

### Problema: Respuestas muy lentas

**Causas comunes:**

1. **Modelo muy grande para tu hardware**
   ```bash
   # Usar modelo m√°s peque√±o
   ollama pull mistral      # En lugar de llama2:70b
   ```

2. **Sin GPU** (esperado, es normal que sea m√°s lento)
   - Modelos 7B: ~20 tokens/seg en CPU moderna
   - Modelos 13B: ~10 tokens/seg en CPU moderna
   - Soluci√≥n: Habilitar GPU o usar modelo m√°s peque√±o

3. **Aumentar timeout:**
   ```python
   provider = OllamaProvider({
       "timeout": 300.0  # 5 minutos
   })
   ```

### Problema: "Out of Memory"

```bash
# Usar modelo m√°s peque√±o
ollama pull llama2:7b      # En lugar de llama2:70b

# O configurar l√≠mite de memoria en Docker
docker-compose.yml:
  ollama:
    mem_limit: 8g
```

---

## üìä Comparaci√≥n con Otros Proveedores

### Ollama vs OpenAI vs Gemini

| Caracter√≠stica | Ollama | OpenAI | Gemini |
|----------------|--------|--------|--------|
| **Privacidad** | ‚úÖ 100% local | ‚ùå Datos en cloud | ‚ùå Datos en cloud |
| **Costo** | ‚úÖ Gratis | ‚ùå $0.002-$0.06/1K tokens | ‚úÖ Gratis (con l√≠mites) |
| **API Key** | ‚úÖ No requiere | ‚ùå Requiere | ‚ùå Requiere |
| **Velocidad** | ‚ö°‚ö°‚ö° (depende HW) | ‚ö°‚ö°‚ö°‚ö°‚ö° | ‚ö°‚ö°‚ö°‚ö° |
| **Calidad** | ‚≠ê‚≠ê‚≠ê (7B), ‚≠ê‚≠ê‚≠ê‚≠ê (70B) | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê |
| **Offline** | ‚úÖ S√≠ | ‚ùå No | ‚ùå No |
| **Setup** | ‚ö° Muy simple | ‚ö°‚ö° API Key | ‚ö°‚ö° API Key |
| **Hardware** | üñ•Ô∏è CPU/GPU local | ‚òÅÔ∏è Cloud | ‚òÅÔ∏è Cloud |

### Cu√°ndo Usar Cada Proveedor

**Usar Ollama cuando:**
- ‚úÖ Privacidad es cr√≠tica (datos m√©dicos, financieros)
- ‚úÖ Budget limitado (proyectos educativos, startups)
- ‚úÖ Offline/air-gapped environments
- ‚úÖ Control total sobre infraestructura
- ‚úÖ Compliance (HIPAA, GDPR, etc.)

**Usar OpenAI cuando:**
- ‚úÖ M√°xima calidad de respuestas (GPT-4)
- ‚úÖ Latencia ultra-baja
- ‚úÖ Escalabilidad ilimitada
- ‚úÖ Budget disponible

**Usar Gemini cuando:**
- ‚úÖ Contextos ultra-largos (2M tokens)
- ‚úÖ Capacidades multimodales (im√°genes)
- ‚úÖ Free tier generoso
- ‚úÖ Integraci√≥n con Google Cloud

---

## üéØ Roadmap y Mejoras Futuras

### ‚úÖ Implementado
- [x] Integraci√≥n b√°sica con Ollama API
- [x] Soporte para streaming
- [x] Manejo de errores robusto
- [x] Docker deployment
- [x] Tests unitarios completos
- [x] M√©tricas de Prometheus

### üöß En Desarrollo
- [ ] Fine-tuning de modelos locales
- [ ] Cuantizaci√≥n autom√°tica (GGUF)
- [ ] Cache inteligente de embeddings
- [ ] Load balancing entre m√∫ltiples instancias

### üí° Planeado
- [ ] Soporte para modelos multimodales (LLaVA)
- [ ] Auto-scaling basado en demanda
- [ ] Benchmarking autom√°tico de modelos
- [ ] Integraci√≥n con Ray para inferencia distribuida

---

## üìö Recursos Adicionales

### Documentaci√≥n Oficial
- [Ollama Website](https://ollama.ai)
- [Ollama GitHub](https://github.com/ollama/ollama)
- [Ollama Model Library](https://ollama.ai/library)
- [Ollama API Reference](https://github.com/ollama/ollama/blob/main/docs/api.md)

### Modelos Open Source
- [Llama 2 (Meta)](https://ai.meta.com/llama/)
- [Mistral AI](https://mistral.ai)
- [Google Gemma](https://ai.google.dev/gemma)
- [Code Llama](https://ai.meta.com/blog/code-llama-large-language-model-coding/)

### Comunidad
- [Ollama Discord](https://discord.gg/ollama)
- [Awesome Ollama](https://github.com/jmorganca/awesome-ollama)

---

## ü§ù Soporte

¬øPreguntas o problemas? 

1. **Revisar logs:**
   ```bash
   docker-compose logs -f ollama
   docker-compose logs -f api
   ```

2. **Verificar health check:**
   ```bash
   curl http://localhost:11434/api/tags
   ```

3. **Reportar issue:** [GitHub Issues](https://github.com/tu-repo/issues)

---

**√öltima actualizaci√≥n:** Diciembre 2025  
**Versi√≥n:** 1.0.0  
**Autor:** Phoenix Development Team
